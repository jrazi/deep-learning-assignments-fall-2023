{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_zBh5XkFS2x"
   },
   "source": [
    "# In the name of God\n",
    "### HW6\n",
    "### Deep Q-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le8_aVDDFwXN"
   },
   "source": [
    "**Name:** ...\n",
    "\n",
    "**Std. No.:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43stiMNMF00N"
   },
   "source": [
    "\n",
    "### Deep Q-Learning (DQN)\n",
    "\n",
    "Deep Q-Learning is a popular algorithm in reinforcement learning that combines the ideas of Q-learning, a traditional reinforcement learning method, with deep neural networks. The goal is to train an agent to make decisions by estimating the optimal action-value function Q, which represents the expected cumulative future rewards for taking a particular action in a given state.\n",
    "\n",
    "Key components of DQN:\n",
    "\n",
    "- **Experience Replay:** To break the temporal correlation in sequential data and improve sample efficiency, we use an experience replay buffer to store and sample past experiences.\n",
    "- **Target Networks:** The use of two separate networks, the main network and a target network, helps stabilize training by decoupling the update targets from the online network's constantly changing values.\n",
    "\n",
    "### The Lunar Lander Problem\n",
    "\n",
    "The task is to control a lunar lander and guide it to land safely on the moon's surface. The agent needs to learn a policy that takes into account the lunar lander's state (position, velocity, angle, angular velocity, etc.) and chooses appropriate actions (thrust left, thrust right, thrust up, or do nothing) to achieve a safe landing.\n",
    "\n",
    "### Overview\n",
    "\n",
    "- **Environment:** LunarLander-v2 from OpenAI Gym.\n",
    "- **Objective:** Train an agent to learn a policy for landing the lunar lander safely.\n",
    "- **Techniques:** Deep Q-Learning, Experience Replay, Target Networks.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Follow the instructions and comments in the code cells to implement and understand each component.\n",
    "2. Replace the `#####TO DO#####` placeholders with your code.\n",
    "3. Experiment with hyperparameters and observe how they affect the training process.\n",
    "4. Run the notebook to train the agent and play the game with the trained model.\n",
    "5. Answer any provided questions or tasks to reinforce your understanding.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Make sure you have the following libraries installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_SIKDiCCBfC"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DllO2OwTCjh3"
   },
   "outputs": [],
   "source": [
    "!pip install swig\n",
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWLFFqKfGcip"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJG1tV3p-8EM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_k--sktuIFlO"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_features, n_actions):\n",
    "        \"\"\"\n",
    "        Initialize the Deep Q-Network (DQN).\n",
    "\n",
    "        Parameters:\n",
    "        - in_features (int): Number of input features (dimension of the state).\n",
    "        - n_actions (int): Number of possible actions in the environment.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # TODO: Implement the neural network architecture\n",
    "        # Use Linear layers with ReLU\n",
    "        # Number of hidden units in each layer:\n",
    "        # - Layer 1: 256 units\n",
    "        # - Layer 2: 128 units\n",
    "        # - Layer 3: 64 units\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor representing the state.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor representing Q-values for each action.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "B6zGowe__ac8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ExperienceBuffer():\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initialize the Experience Replay Buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - capacity (int): Maximum capacity of the buffer.\n",
    "        \"\"\"\n",
    "        self.exp_buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, exp):\n",
    "        \"\"\"\n",
    "        Append a new experience to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - exp (tuple): Tuple representing a single experience (state, action, reward, done, next_state).\n",
    "        \"\"\"\n",
    "        self.exp_buffer.append(exp)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the current size of the buffer.\n",
    "\n",
    "        Returns:\n",
    "        - int: Number of experiences currently stored in the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.exp_buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clear all experiences from the buffer.\"\"\"\n",
    "        self.exp_buffer.clear()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        TODO: Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size (int): Size of the batch to be sampled.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Batch of experiences (states, actions, rewards, dones, next_states).\n",
    "        \"\"\"\n",
    "        # TODO: Implement the sampling logic\n",
    "\n",
    "\n",
    "        # TODO: Convert to NumPy arrays with appropriate data types\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "G8ZC4Jbc_oo6"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, buffer):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "\n",
    "        Parameters:\n",
    "        - env: The environment the agent interacts with.\n",
    "        - buffer: Experience replay buffer to store agent experiences.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.buffer = buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent's state and total rewards to the initial state.\n",
    "        \"\"\"\n",
    "        self.state = env.reset()\n",
    "        self.total_rewards = 0.0\n",
    "\n",
    "    def step(self, net, eps, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        TODO: Implement the exploration-exploitation strategy (epsilon-greedy) here.\n",
    "\n",
    "        Take a step in the environment using the provided neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - net: The neural network representing the agent's policy.\n",
    "        - eps (float): Epsilon value for epsilon-greedy exploration.\n",
    "        - device (str): Device for neural network computations.\n",
    "\n",
    "        Returns:\n",
    "        - done_reward: Total rewards obtained in the episode if it is finished, otherwise None.\n",
    "        \"\"\"\n",
    "        done_reward = None\n",
    "\n",
    "        # TODO: Implement exploration-exploitation strategy here\n",
    "\n",
    "        # TODO: Take the selected action for 4 time steps (adjustable)\n",
    "\n",
    "        # TODO: Append the experience to the buffer\n",
    "\n",
    "        return done_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "U5sWRHaU_fD7"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99  # Discount factor for future rewards\n",
    "EPSILON_START = 1.0  # Initial exploration probability (epsilon-greedy)\n",
    "EPSILON_FINAL = 0.01  # Final exploration probability (epsilon-greedy)\n",
    "EPSILON_DECAY_OBS = 10**5  # Number of observations for epsilon decay\n",
    "BATCH_SIZE = 32  # Size of the experience replay batch\n",
    "MEAN_GOAL_REWARD = 250  # Mean reward goal for solving the environment\n",
    "REPLAY_BUFFER_SIZE = 10000  # Maximum capacity of the experience replay buffer\n",
    "REPLAY_MIN_SIZE = 10000  # Minimum size of the experience replay buffer before training begins\n",
    "LEARNING_RATE = 1e-4  # Learning rate for the neural network optimizer\n",
    "SYNC_TARGET_OBS = 1000  # Number of observations before synchronizing target and online networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FItvw2-BD6gm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def cal_loss(batch, net, tgt_net, device='cpu'):\n",
    "    \"\"\"\n",
    "    TODO: Implement the loss calculation for Deep Q-Learning.\n",
    "\n",
    "    Calculate the loss for Deep Q-Learning.\n",
    "\n",
    "    Parameters:\n",
    "    - batch (tuple): Batch of experiences (states, actions, rewards, dones, next_states).\n",
    "    - net: The neural network representing the online Q-network.\n",
    "    - tgt_net: The neural network representing the target Q-network.\n",
    "    - device (str): Device for neural network computations (default is \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Loss value calculated using Mean Squared Error (MSE) loss.\n",
    "    \"\"\"\n",
    "\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    dones_v = torch.BoolTensor(dones).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "\n",
    "    # TODO: Calculate Q-values for the current states and selected actions\n",
    "\n",
    "    # TODO: Calculate the maximum Q-value for the next states using the target network\n",
    "\n",
    "    # TODO: Zero out Q-values for terminal states\n",
    "\n",
    "    # TODO: Detach Q-values for the next states to avoid gradient flow\n",
    "\n",
    "    # TODO: Calculate the expected return for the current states\n",
    "\n",
    "    # TODO: Implement the Mean Squared Error (MSE) loss calculation\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZadHztDQVs0"
   },
   "source": [
    "# Learning Curves\n",
    " Plot learning curves showing key metrics (e.g., total rewards, loss) over the course of training. Analyze the trends and identify key points in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdXuc3WBAjbi"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "tgt_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Lists to track total rewards and losses over training\n",
    "total_rewards = []\n",
    "losses = []\n",
    "\n",
    "# Initialize time variables for tracking training time\n",
    "ts = time.time()\n",
    "best_mean_reward = None\n",
    "obs_id = 0\n",
    "\n",
    "while True:\n",
    "    obs_id += 1\n",
    "\n",
    "    # Update exploration rate based on epsilon decay schedule\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - obs_id/EPSILON_DECAY_OBS)\n",
    "\n",
    "    # Agent takes a step in the environment, receives a reward\n",
    "    reward = agent.step(net, epsilon, device=device)\n",
    "\n",
    "    if reward is not None:\n",
    "        # Store total rewards and update game time\n",
    "        total_rewards.append(reward)\n",
    "        game_time = time.time() - ts\n",
    "        ts = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "\n",
    "        losses.append(loss_t.item())\n",
    "\n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), './lunar_lander-best.dat')\n",
    "\n",
    "            if best_mean_reward is None:\n",
    "                last = mean_reward\n",
    "                best_mean_reward = mean_reward\n",
    "\n",
    "            if best_mean_reward is not None and best_mean_reward - last > 10:\n",
    "                last = best_mean_reward\n",
    "                print(\"GAME : {}, TIME ECLAPSED : {}, EPSILON : {}, MEAN_REWARD : {}\"\n",
    "                      .format(obs_id, game_time, epsilon, mean_reward))\n",
    "                print(\"Reward {} -> {} Model Saved\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "            best_mean_reward = mean_reward\n",
    "\n",
    "        if mean_reward > MEAN_GOAL_REWARD:\n",
    "            print(\"SOLVED in {} obs\".format(obs_id))\n",
    "            break\n",
    "\n",
    "    # Continue training if the replay buffer size is below the minimum required\n",
    "    if len(buffer) < REPLAY_MIN_SIZE:\n",
    "        continue\n",
    "\n",
    "    # Synchronize target network with the Q-network at regular intervals\n",
    "    if obs_id % SYNC_TARGET_OBS == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    # TODO: Implement the training process (calculating loss, backpropagation, and optimizer step)\n",
    "\n",
    "\n",
    "    # TODO: Plot learning curves every few episodes or steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwZygyvBOzLe"
   },
   "source": [
    "# Visual Comparison:\n",
    "\n",
    "write a function to render and display the environment before and after training. What visual differences do you observe in the agent's behavior? Discuss it. Also, Upload the Videos with your notebook. You can use the following library for rendering and saving videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_ClI7VqMHgh"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "# Helper function for rendering and saving a video\n",
    "def render_and_save_video(env, net, episodes=10, save_path=\"./render_video.mp4\", device=\"cpu\"):\n",
    "    # TODO: Render and display the environment\n",
    "\n",
    "\n",
    "# Render and save a video before training\n",
    "print(\"### BEFORE TRAINING ###\")\n",
    "render_and_save_video(env, net, device=device,save_path = './before.mp4')\n",
    "\n",
    "# Render and save a video after training\n",
    "print(\"### AFTER TRAINING ###\")\n",
    "render_and_save_video(env, net, device=device,save_path = './after.mp4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZsGQY4DQ9Z_"
   },
   "source": [
    "# Question:\n",
    "\n",
    "Exploration (Epsilon-Greedy):\n",
    "\n",
    "Discuss the significance of the exploration strategy, specifically the Epsilon-Greedy approach, in balancing exploration and exploitation during training."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
