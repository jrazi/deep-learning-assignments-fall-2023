{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_zBh5XkFS2x"
      },
      "source": [
        "# In the name of God\n",
        "## HW6\n",
        "### Practical Section: TRPO Algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le8_aVDDFwXN"
      },
      "source": [
        "**Name:** Javad Razi\n",
        "\n",
        "**Std. No.:** 401204354"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43stiMNMF00N"
      },
      "source": [
        "### PPO Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Required Libraries\n",
        "\n",
        "First, we need to import the necessary libraries. We will be using OpenAI's `gym` for the Lunar Lander environment, `numpy` for numerical operations, and `torch` for implementing the neural network and optimization.\n"
      ],
      "metadata": {
        "id": "l0FByoE9S89D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade setuptools wheel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "Qlab_4fdTRcW",
        "outputId": "49e72e41-960e-407e-ea11-e15c1b9508a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Installing collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-69.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMqRGxwaTf08",
        "outputId": "450bf0a7-92ad-4cdd-9b76-bfb87fef83ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Using cached swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "Installing collected packages: swig\n",
            "Successfully installed swig-4.1.1.post1\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Using cached pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.1.1.post1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373128 sha256=3b4b9332556f839483b763ee10224573920c89defdd92488dcfed82b6664066c\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "RKyEIJBqTFWG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Environment\n",
        "\n",
        "We will create the Lunar Lander environment using the `gym.make()` function. We will also set the `enable_wind` parameter to `True` as mentioned.\n"
      ],
      "metadata": {
        "id": "sdVkSi9kTHvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env.enable_wind = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNNNNsviTJF0",
        "outputId": "4885dfe6-7c6c-468e-fd08-cf848c9199ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Policy Network\n",
        "\n",
        "We will define a simple policy network using PyTorch. This network will take the state of the environment as input and output the action probabilities and state value.\n"
      ],
      "metadata": {
        "id": "_0cTEOeNTMoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.tanh(self.linear1(state))\n",
        "        x = self.linear2(x)\n",
        "        action_probs = torch.softmax(x, dim=1)\n",
        "        return action_probs\n"
      ],
      "metadata": {
        "id": "dvokncfWTNU6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Value Network\n",
        "\n",
        "Next, we define a value network that estimates the value of a state. This network is separate from the policy network and has its own parameters.\n"
      ],
      "metadata": {
        "id": "8b-Z3S4-TuEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_size, learning_rate=3e-4):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.tanh(self.linear1(state))\n",
        "        x = self.linear2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "gCE_Zm2RTu3t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the TRPO Algorithm\n",
        "\n",
        "Now, we will implement the TRPO algorithm. We will use the PyTorch's automatic differentiation feature to compute the gradients. The objective function and the constraint are implemented as mentioned in the task description.\n"
      ],
      "metadata": {
        "id": "fKvCiqPqT13a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trpo_step(policy_net, value_net, states, actions, rewards, masks, epsilon=0.2):\n",
        "    # Compute the old action probabilities\n",
        "    old_action_probs = policy_net(states).gather(1, actions)\n",
        "\n",
        "    # Compute the value function\n",
        "    values = value_net(states)\n",
        "\n",
        "    # Compute the advantages\n",
        "    advantages = rewards + masks * values - values.detach()\n",
        "\n",
        "    # Compute the new action probabilities\n",
        "    new_action_probs = policy_net(states).gather(1, actions)\n",
        "\n",
        "    # Compute the surrogate function\n",
        "    ratio = new_action_probs / old_action_probs\n",
        "    surrogate = ratio * advantages\n",
        "\n",
        "    # Compute the KL divergence\n",
        "    kl_divergence = old_action_probs * torch.log(old_action_probs / new_action_probs)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = -surrogate + epsilon * kl_divergence\n",
        "\n",
        "    # Update the policy network\n",
        "    policy_net.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    policy_net.optimizer.step()\n",
        "\n",
        "    # Update the value network\n",
        "    value_net.optimizer.zero_grad()\n",
        "    values.backward()\n",
        "    value_net.optimizer.step()"
      ],
      "metadata": {
        "id": "8ToB-r_ET2lu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the PPO Algorithm\n",
        "\n",
        "Next, we will implement the PPO algorithm. The PPO algorithm is similar to the TRPO algorithm, but it uses a clipped surrogate objective instead of the original surrogate objective.\n"
      ],
      "metadata": {
        "id": "R9ccHYfOUBTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: This is a simplified version of the PPO algorithm and may need adjustments based on the specific requirements of your task.\n",
        "\n",
        "def ppo_step(policy_net, value_net, states, actions, rewards, masks, epsilon=0.2, beta=3.0):\n",
        "    # Compute the old action probabilities\n",
        "    old_action_probs = policy_net(states).gather(1, actions)\n",
        "\n",
        "    # Compute the value function\n",
        "    values = value_net(states)\n",
        "\n",
        "    # Compute the advantages\n",
        "    advantages = rewards + masks * values - values.detach()\n",
        "\n",
        "    # Compute the new action probabilities\n",
        "    new_action_probs = policy_net(states).gather(1, actions)\n",
        "\n",
        "    # Compute the surrogate function\n",
        "    ratio = new_action_probs / old_action_probs\n",
        "    surrogate = ratio * advantages\n",
        "\n",
        "    # Compute the KL divergence\n",
        "    kl_divergence = old_action_probs * torch.log(old_action_probs / new_action_probs)\n",
        "\n",
        "    # Compute the clipped surrogate function\n",
        "    clipped_surrogate = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = -torch.min(surrogate, clipped_surrogate) + beta * kl_divergence\n",
        "\n",
        "    # Update the policy network\n",
        "    policy_net.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    policy_net.optimizer.step()\n",
        "\n",
        "    # Update the value network\n",
        "    value_net.optimizer.zero_grad()\n",
        "    values.backward()\n",
        "    value_net.optimizer.step()"
      ],
      "metadata": {
        "id": "7KzI9dr2UCM5"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}