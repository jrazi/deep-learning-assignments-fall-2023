{"cells":[{"cell_type":"markdown","metadata":{"id":"DHvhhpBU-QtI"},"source":["# Homework 3 Part 2\n","\n","## Course Name: Deep Learning\n","#### Lecturers: Dr. Beigy\n","\n","---\n","\n","#### Notebooks Supervised By: Zeinab Sadat Taghavi\n","#### Notebooks Prepared By: Zahra Khoramnejad, Mehran Sarmadi, Zahra Rahimi\n","\n","**Contact**: Ask your questions in Quera\n","\n","---\n","\n","### Instructions:\n","- Complete all exercises presented in this notebook.\n","- Ensure you run each cell after you've entered your solution.\n","- After completing the exercises, save the notebook and <font color='red'>follow the submission guidelines provided in the PDF.</font>\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"LG6yNYAmreI_"},"source":["#Text Generation\n","\n","<p align='justify'>Text generation task involves generating new text based on a given input or a prompt. It is a natural language processing (NLP) task that aims to generate coherent and contextually relevant text.\n","\n","In text generation, a model is trained on a large corpus of text data and learns the patterns and structures of the language. This model can then be used to generate new text by sampling from the learned distribution of words or characters.\n","\n","Text generation has various applications, including chatbots, language translation, poetry generation, and content creation. It can be implemented using different techniques such as `recurrent neural networks (RNNs)`, `transformers`, and `Markov chains`.\n","\n","The goal of text generation is to produce text that is fluent, coherent, and contextually relevant. It requires a deep understanding of the language and the ability to generate text that follows grammatical rules and maintains semantic coherence.</p>"]},{"cell_type":"markdown","metadata":{"id":"-u6fGIqx0LIz"},"source":["## Charachter-leve l text generation"]},{"cell_type":"markdown","metadata":{"id":"cKhboP-Y0TrV"},"source":["One stage of the task of text generation is mapping, which can be at the word or character level. At this stage, a number is assigned to each word or character.\n","\n","In this exercise, we generate text at the character level. Because generating text at the word level, even though it leads to more meaningful outputs, requires a rich dataset with a high number of word repetitions."]},{"cell_type":"markdown","metadata":{"id":"00sNsfmXtW6K"},"source":["We will implement models based on `recurrent networks` for text generation and compare the performance of different models. In the following, we will check the performance of the best models on different datasets and compare the results"]},{"cell_type":"markdown","metadata":{"id":"jw8uJvD8vTyM"},"source":["The steps of this exercise are as follows:\n","1. Train RNN and LSTM\n","2. FineTuning\n","3. Experiment on different datasets"]},{"cell_type":"markdown","metadata":{"id":"nb9egMrmAhXK"},"source":["---\n","---"]},{"cell_type":"markdown","metadata":{"id":"SBqL42w1vt1Q"},"source":["#1. Train RNN and LSTM"]},{"cell_type":"markdown","metadata":{"id":"dP0JQJIj2rL0"},"source":["## Imports"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-07T02:07:31.668336Z","iopub.status.busy":"2023-12-07T02:07:31.667432Z","iopub.status.idle":"2023-12-07T02:07:31.676209Z","shell.execute_reply":"2023-12-07T02:07:31.675097Z","shell.execute_reply.started":"2023-12-07T02:07:31.668292Z"},"id":"x7cTJANKwpck","outputId":"94e67019-c1e8-4570-8f6b-a40394484c86","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","\n","import numpy as np\n","import pandas as pd\n","import random\n","import re\n","import string\n","\n","import matplotlib.pyplot as plt\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"hJSsyWUyv8KC"},"source":["## Load data"]},{"cell_type":"markdown","metadata":{"id":"oHGryhW7wbsZ"},"source":["- We use the dataset of `Shakespeare's plays` as the main dataset for this exercise"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:31.678629Z","iopub.status.busy":"2023-12-07T02:07:31.678283Z","iopub.status.idle":"2023-12-07T02:07:32.854044Z","shell.execute_reply":"2023-12-07T02:07:32.852913Z","shell.execute_reply.started":"2023-12-07T02:07:31.678598Z"},"id":"eUNd0YrFwW2E","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-12-07 02:07:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 416 Range Not Satisfiable\n","\n","    The file is already fully retrieved; nothing to do.\n","\n"]}],"source":["!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" -c -P {'data/'}"]},{"cell_type":"markdown","metadata":{"id":"hYntrpS_Arse"},"source":["- Load data in amout of 30kb for training models"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.855799Z","iopub.status.busy":"2023-12-07T02:07:32.855481Z","iopub.status.idle":"2023-12-07T02:07:32.861286Z","shell.execute_reply":"2023-12-07T02:07:32.860342Z","shell.execute_reply.started":"2023-12-07T02:07:32.855771Z"},"id":"xYsndtIoxd0Z","trusted":true},"outputs":[],"source":["sh_data_file = \"./data/input.txt\"\n","sh_data = open(sh_data_file, 'r').read(30000)"]},{"cell_type":"markdown","metadata":{"id":"Pj-xesSRwI9S"},"source":["##Charachter mapping"]},{"cell_type":"markdown","metadata":{"id":"Jl-LIH_Syl0M"},"source":["- For better performance of the model, we limit the set of allowed characters"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.863080Z","iopub.status.busy":"2023-12-07T02:07:32.862720Z","iopub.status.idle":"2023-12-07T02:07:32.871256Z","shell.execute_reply":"2023-12-07T02:07:32.870450Z","shell.execute_reply.started":"2023-12-07T02:07:32.863047Z"},"id":"KLiOHN8sCSe2","trusted":true},"outputs":[],"source":["chars = list(string.ascii_lowercase + '\\n' + ' ' + ':' + '.')\n","vocab_size = len(chars)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.874029Z","iopub.status.busy":"2023-12-07T02:07:32.873743Z","iopub.status.idle":"2023-12-07T02:07:32.880895Z","shell.execute_reply":"2023-12-07T02:07:32.880043Z","shell.execute_reply.started":"2023-12-07T02:07:32.873993Z"},"id":"xvP9iW3FyT6g","trusted":true},"outputs":[],"source":["# Mapping of char-index\n","char_to_ix = { ch:i for i,ch in enumerate(chars) }\n","ix_to_char = { i:ch for i,ch in enumerate(chars) }"]},{"cell_type":"markdown","metadata":{"id":"nh8Fo1WnwC9T"},"source":["##Preprocessing"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.882259Z","iopub.status.busy":"2023-12-07T02:07:32.881978Z","iopub.status.idle":"2023-12-07T02:07:32.891018Z","shell.execute_reply":"2023-12-07T02:07:32.890107Z","shell.execute_reply.started":"2023-12-07T02:07:32.882235Z"},"id":"vxGtjsgay0x9","trusted":true},"outputs":[],"source":["def remove_extraneous_characters(data, valid_char_list):\n","    pattern = f\"[^{re.escape(''.join(valid_char_list))}]\"\n","    return re.sub(pattern, '', data)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.892441Z","iopub.status.busy":"2023-12-07T02:07:32.892137Z","iopub.status.idle":"2023-12-07T02:07:32.919332Z","shell.execute_reply":"2023-12-07T02:07:32.918470Z","shell.execute_reply.started":"2023-12-07T02:07:32.892415Z"},"id":"KscUSBpbyRNZ","trusted":true},"outputs":[],"source":["sh_data = remove_extraneous_characters(sh_data.lower(), chars)\n","sh_data_size = len(sh_data)\n","\n","# Extract indexes of data characters\n","sh_data = list(sh_data)\n","for i, ch in enumerate(sh_data):\n","    sh_data[i] = char_to_ix[ch]\n","\n","sh_data = torch.tensor(sh_data).to(device)\n","sh_data = torch.unsqueeze(sh_data, dim=1)"]},{"cell_type":"markdown","metadata":{"id":"Df7TxCxOzueM"},"source":["## Modeling"]},{"cell_type":"markdown","metadata":{"id":"Rsj67WalA4fY"},"source":["- In this part define RNN and LSTM model, according to the mentioned characteristics and function inputs.\n"]},{"cell_type":"markdown","metadata":{"id":"kFj9nkzoz2Hs"},"source":["### RNN"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.921471Z","iopub.status.busy":"2023-12-07T02:07:32.920517Z","iopub.status.idle":"2023-12-07T02:07:32.930386Z","shell.execute_reply":"2023-12-07T02:07:32.929398Z","shell.execute_reply.started":"2023-12-07T02:07:32.921437Z"},"id":"fgmKXfb1zuAJ","trusted":true},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_size=512, num_layers=3, dropout_enable=False):\n","        super(RNN, self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout_enable = dropout_enable\n","        self.dropout = nn.Dropout(0.5) if dropout_enable else None\n","        self.hidden_state = None\n","\n","        # Define the RNN layer\n","        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n","        # Define the decoder\n","        self.decoder = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input_seq):\n","        # Pass the input through the RNN layers\n","        output, self.hidden_state = self.rnn(input_seq, self.hidden_state)\n","        # If dropout is enabled, apply it\n","        if self.dropout_enable:\n","            output = self.dropout(output)\n","        # Pass the output of the RNN through the decoder\n","        output = self.decoder(output)\n","        return output\n","\n","    def save_model(self, path):\n","        torch.save(self.state_dict(), path)\n","\n","    def load_model(self, path):\n","        self.load_state_dict(torch.load(path))\n"]},{"cell_type":"markdown","metadata":{"id":"xeoymmMx7RKR"},"source":["### LSTM"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.932345Z","iopub.status.busy":"2023-12-07T02:07:32.931700Z","iopub.status.idle":"2023-12-07T02:07:32.945252Z","shell.execute_reply":"2023-12-07T02:07:32.944329Z","shell.execute_reply.started":"2023-12-07T02:07:32.932311Z"},"id":"4PQDAnxc7Q26","trusted":true},"outputs":[],"source":["class LSTM(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_size=512, num_layers=3, dropout_enable=False):\n","        super(LSTM, self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout_enable = dropout_enable\n","        self.dropout = nn.Dropout(0.5) if dropout_enable else None\n","        self.hidden_state = None\n","\n","        # Define the LSTM layer\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        # Define the decoder\n","        self.decoder = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input_seq):\n","        # Pass the input through the LSTM layers\n","        output, self.hidden_state = self.lstm(input_seq, self.hidden_state)\n","        # If dropout is enabled, apply it\n","        if self.dropout_enable:\n","            output = self.dropout(output)\n","        # Pass the output of the LSTM through the decoder\n","        output = self.decoder(output)\n","        return output\n","\n","    def save_model(self, path):\n","        torch.save(self.state_dict(), path)\n","\n","    def load_model(self, path):\n","        self.load_state_dict(torch.load(path))\n"]},{"cell_type":"markdown","metadata":{"id":"Y9XQWqoXwM4N"},"source":["## Training"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.949110Z","iopub.status.busy":"2023-12-07T02:07:32.948398Z","iopub.status.idle":"2023-12-07T02:07:32.958409Z","shell.execute_reply":"2023-12-07T02:07:32.957551Z","shell.execute_reply.started":"2023-12-07T02:07:32.949084Z"},"id":"ruPWrO7Z9JM2","trusted":true},"outputs":[],"source":["def print_sample_output(model, data, data_size, test_output_len = 200):\n","    # Use this function to print sample that model generates from its current hidden state and random input character\n","    # test_output_len is total num of characters in output test sequence\n","\n","    test_output = \"\"\n","    data_ptr = 0\n","\n","    rand_index = np.random.randint(data_size-1)\n","    input_seq = data[rand_index : rand_index+1]\n","\n","    while True:\n","        output = model(input_seq)\n","\n","        output = F.softmax(torch.squeeze(output), dim=0)\n","        dist = Categorical(output)\n","        index = dist.sample().item()\n","\n","        test_output += ix_to_char[index]\n","\n","        input_seq[0][0] = index\n","        data_ptr += 1\n","\n","        if data_ptr > test_output_len:\n","            break\n","\n","    print(\"Train Sample +++++++++++++++++++++++++++++++++++++++++++++\")\n","    print(test_output)\n","    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"]},{"cell_type":"markdown","metadata":{"id":"WC337ho-XX-I"},"source":["- For construction of each sample in the dataset, the output sequence is\n","obtained from the shift of one character from the input sequence. For example, when sequence_length is 10 and our text is `Hello world`. The input sequence would be `Hello worl`, and the target sequence `ello world`."]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.959906Z","iopub.status.busy":"2023-12-07T02:07:32.959585Z","iopub.status.idle":"2023-12-07T02:07:32.973212Z","shell.execute_reply":"2023-12-07T02:07:32.972287Z","shell.execute_reply.started":"2023-12-07T02:07:32.959880Z"},"id":"aPpDSsZU7vWu","trusted":true},"outputs":[],"source":["def train_epoch(model, data, data_size, epoch, optimizer, seq_len=200):\n","    # seq_length is length of training data sequence\n","    model.train()\n","    criterion = nn.CrossEntropyLoss()\n","    total_loss = 0\n","    sample_number = 0\n","\n","    # Iterate over the data in chunks of seq_len\n","    for i in range(0, data_size - seq_len, seq_len):\n","        # Get the input and target sequences\n","        input_seq = data[i:i+seq_len]\n","        target_seq = data[i+1:i+seq_len+1]\n","\n","        # Reset the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        output = model(input_seq)\n","\n","        # Compute the loss\n","        loss = criterion(output.view(-1, model.output_size), target_seq.view(-1))\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Accumulate the total loss\n","        total_loss += loss.item()\n","        sample_number += 1\n","\n","        # Print the current loss and a sample output every 10 epochs\n","        if epoch % 10 == 0:\n","            print('Epoch: {}/{}.............'.format(epoch, 1000), end=' ')\n","            print(\"Loss: {:.4f}\".format(total_loss / sample_number))\n","            print_sample_output(model, data, data_size)\n","\n","    return total_loss / sample_number\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.974756Z","iopub.status.busy":"2023-12-07T02:07:32.974396Z","iopub.status.idle":"2023-12-07T02:07:32.986152Z","shell.execute_reply":"2023-12-07T02:07:32.985205Z","shell.execute_reply.started":"2023-12-07T02:07:32.974722Z"},"id":"1NT_mZuMf4do","trusted":true},"outputs":[],"source":["def train_rnn(data, data_size, model_save_file):\n","    # RNN parameters\n","    hidden_size = 512\n","    num_layers = 6\n","    lr = 0.002\n","    epoch_num = 100\n","    losses = []\n","\n","    model = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    min_loss = np.inf\n","\n","    for epoch in range(epoch_num):\n","        loss = train_epoch(model, data, data_size, epoch, optimizer)\n","        losses.append(loss)\n","\n","        # If the current loss is less than the minimum loss, save the model\n","        if loss < min_loss:\n","            min_loss = loss\n","            model.save_model(model_save_file)\n","\n","    return losses\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:32.987878Z","iopub.status.busy":"2023-12-07T02:07:32.987429Z","iopub.status.idle":"2023-12-07T02:07:32.996649Z","shell.execute_reply":"2023-12-07T02:07:32.995870Z","shell.execute_reply.started":"2023-12-07T02:07:32.987837Z"},"id":"gd-wyFT4gerv","trusted":true},"outputs":[],"source":["def train_lstm(data, data_size, model_save_file):\n","    # LSTM parameters\n","    hidden_size = 512\n","    num_layers = 3\n","    lr = 0.002\n","    epoch_num = 100\n","    losses = []\n","\n","    model = LSTM(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    min_loss = np.inf\n","\n","    for epoch in range(epoch_num):\n","        loss = train_epoch(model, data, data_size, epoch, optimizer)\n","        losses.append(loss)\n","\n","        # If the current loss is less than the minimum loss, save the model\n","        if loss < min_loss:\n","            min_loss = loss\n","            model.save_model(model_save_file)\n","\n","    return losses\n"]},{"cell_type":"markdown","metadata":{"id":"KW4HTBwBEoHC"},"source":["### RNN"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-12-07T02:07:33.002242Z","iopub.status.busy":"2023-12-07T02:07:33.001642Z","iopub.status.idle":"2023-12-07T02:07:33.618244Z","shell.execute_reply":"2023-12-07T02:07:33.615941Z","shell.execute_reply.started":"2023-12-07T02:07:33.002215Z"},"id":"-mA-QGewD22i","trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"input.size(-1) must be equal to input_size. Expected 30, got 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rnn_sh_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msh_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msh_data_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model_sh_rnn.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[33], line 16\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[0;34m(data, data_size, model_save_file)\u001b[0m\n\u001b[1;32m     13\u001b[0m min_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[0;32m---> 16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# If the current loss is less than the minimum loss, save the model\u001b[39;00m\n","Cell \u001b[0;32mIn[32], line 18\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data, data_size, epoch, optimizer, seq_len)\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39moutput_size), target_seq\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_seq):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Pass the input through the RNN layers\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# If dropout is enabled, apply it\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_enable:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:505\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    502\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_RELU\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:253\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n","\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 30, got 1"]}],"source":["rnn_sh_losses = train_rnn(sh_data, sh_data_size, './model_sh_rnn.pth')"]},{"cell_type":"markdown","metadata":{"id":"0LNMgGhjEqZn"},"source":["### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.619470Z","iopub.status.idle":"2023-12-07T02:07:33.619844Z","shell.execute_reply":"2023-12-07T02:07:33.619652Z","shell.execute_reply.started":"2023-12-07T02:07:33.619637Z"},"id":"oA6zVNt6Ep7S","trusted":true},"outputs":[],"source":["lstm_sh_losses = train_lstm(sh_data, sh_data_size, './model_sh_lstm.pth')"]},{"cell_type":"markdown","metadata":{"id":"tUGxc-Z1wRiP"},"source":["##Generating texts"]},{"cell_type":"markdown","metadata":{"id":"43rEOvbopt3s"},"source":["- A sample text to input the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.620704Z","iopub.status.idle":"2023-12-07T02:07:33.621027Z","shell.execute_reply":"2023-12-07T02:07:33.620881Z","shell.execute_reply.started":"2023-12-07T02:07:33.620866Z"},"id":"2gkZC4UagpoB","trusted":true},"outputs":[],"source":["input_sample_text = 'First Citizen:\\nYou are all resolved rather to die than to famish?\\n'\n","\n","def create_input_sample_dataset(input_sample_text):\n","    input_sample = remove_extraneous_characters(input_sample_text.lower(), chars)\n","    input_sample = list(input_sample)\n","    for i, ch in enumerate(input_sample):\n","        input_sample[i] = char_to_ix[ch]\n","\n","    input_sample = torch.tensor(input_sample).to(device)\n","    input_sample = torch.unsqueeze(input_sample, dim=1)\n","    return input_sample"]},{"cell_type":"markdown","metadata":{"id":"m7N1xZocpFIf"},"source":["- This function generates the output generated by the model for the input sample, and if the input sample text is not given, it samples a sequence of original data and gives it to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.622426Z","iopub.status.idle":"2023-12-07T02:07:33.622789Z","shell.execute_reply":"2023-12-07T02:07:33.622607Z","shell.execute_reply.started":"2023-12-07T02:07:33.622579Z"},"id":"zTFXBqKqII_j","trusted":true},"outputs":[],"source":["def generate_text(model, data, data_size, input_sample_test = None, output_len=1000):\n","    model.eval()\n","    data_ptr = 0\n","    test_output=\"\"\n","\n","    if input_sample_test is not None:\n","        index = 0\n","        seq_len = len(input_sample_test)\n","        input_seq = input_sample_test[index : index + seq_len-1]\n","    else:\n","        # If input sample not declared, select an initial string from the data of 10 characters randomly\n","        index = np.random.randint(data_size - 11)\n","        seq_len = 10\n","        input_seq = data[index : index + 9]\n","\n","   # Set last hidden state of model by feeding input sequence to model\n","    output = model(input_seq)\n","\n","    # Last charachter feed to model\n","    if input_sample_test is not None:\n","        input_seq = input_sample_test[index + seq_len-1 : index + seq_len]\n","    else:\n","        input_seq = data[index + seq_len-1 : index + seq_len]\n","\n","    while True:\n","        output = model(input_seq)\n","\n","        output = F.softmax(torch.squeeze(output), dim=0)\n","        dist = Categorical(output)\n","        index = dist.sample().item()\n","\n","        test_output += ix_to_char[index]\n","        input_seq[0][0] = index\n","        data_ptr += 1\n","\n","        if data_ptr  > output_len:\n","            break\n","\n","    print(\"Eaxmple of generated text --------------------------------------------------------------------------\")\n","    print(test_output)\n","    print(\"----------------------------------------------------------------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"MatZYKCUFOSn"},"source":["### RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.624065Z","iopub.status.idle":"2023-12-07T02:07:33.624388Z","shell.execute_reply":"2023-12-07T02:07:33.624233Z","shell.execute_reply.started":"2023-12-07T02:07:33.624218Z"},"id":"-oT9TsOBFQ9k","trusted":true},"outputs":[],"source":["best_model_rnn =  RNN(vocab_size, vocab_size, 512, 6).to(device)\n","best_model_rnn.load_model('./model_sh_rnn.pth')\n","print(\"best loss\", min(rnn_sh_losses))\n","generate_text(best_model_rnn, sh_data, sh_data_size)"]},{"cell_type":"markdown","metadata":{"id":"Fwr90H6eFPqR"},"source":["### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.625574Z","iopub.status.idle":"2023-12-07T02:07:33.625927Z","shell.execute_reply":"2023-12-07T02:07:33.625779Z","shell.execute_reply.started":"2023-12-07T02:07:33.625763Z"},"id":"R2DUmeREFUkc","trusted":true},"outputs":[],"source":["best_model_lstm =  LSTM(vocab_size, vocab_size, 512, 3).to(device)\n","best_model_lstm.load_model('./model_sh_lstm.pth')\n","print(\"best loss\", min(lstm_sh_losses))\n","generate_text(best_model_lstm, sh_data, sh_data_size)"]},{"cell_type":"markdown","metadata":{"id":"_thNtUicv2kd"},"source":["## Plotting the losses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.627718Z","iopub.status.idle":"2023-12-07T02:07:33.628055Z","shell.execute_reply":"2023-12-07T02:07:33.627905Z","shell.execute_reply.started":"2023-12-07T02:07:33.627889Z"},"id":"f_If3o7grbO8","trusted":true},"outputs":[],"source":["def plot_losses(losses):\n","    xpoints = np.array(range(len(losses)))\n","    ypoints = np.array(losses)\n","\n","    plt.plot(xpoints, ypoints, color='blue',label='losses')\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"loss\")\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ylDZpjFtJ6Mo"},"source":["### RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.628979Z","iopub.status.idle":"2023-12-07T02:07:33.629283Z","shell.execute_reply":"2023-12-07T02:07:33.629139Z","shell.execute_reply.started":"2023-12-07T02:07:33.629125Z"},"id":"lvQC5FNzJ4Q1","trusted":true},"outputs":[],"source":["plot_losses(rnn_sh_losses)"]},{"cell_type":"markdown","metadata":{"id":"nbd_-EFInd-K"},"source":["### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.632004Z","iopub.status.idle":"2023-12-07T02:07:33.632352Z","shell.execute_reply":"2023-12-07T02:07:33.632198Z","shell.execute_reply.started":"2023-12-07T02:07:33.632181Z"},"id":"g44i0NiKJ97c","trusted":true},"outputs":[],"source":["plot_losses(lstm_sh_losses)"]},{"cell_type":"markdown","metadata":{"id":"nkoXQFMmKbU1"},"source":["## Report"]},{"cell_type":"markdown","metadata":{},"source":["According to the texts generated from different models and the losses during the training process of the models, analyze what is the reason for the difference in the result of models.\n","\n","Which model works better and what do you think are the reasons?"]},{"cell_type":"markdown","metadata":{},"source":["<font color='#73FF73'><b>Your answer : </b></font>\n","\n","After examining the generated text and the loss values, it’s clear that the LSTM model has outperformed the RNN model in this task. The LSTM model has generated more coherent and grammatically correct sentences compared to the RNN model. This is likely due to the LSTM’s ability to better capture long-term dependencies in the text, which is crucial for generating meaningful sentences.\n","\n","In terms of the loss values, the LSTM model has achieved a lower loss compared to the RNN model, indicating that it has learned the patterns in the data more effectively. This is consistent with our understanding of LSTMs as they are known to be more effective at learning from long sequences of data and are less prone to issues like vanishing gradients that can hinder the learning process in RNNs.\n","\n","Therefore, based on these observations, the LSTM model is the better choice for this text generation task. However, it’s important to note that the choice of model can depend on the specific task and the nature of the data. It’s always a good idea to experiment with different models and configurations to find the best fit for the task at hand."]},{"cell_type":"markdown","metadata":{"id":"j3UI0UnS9tzg"},"source":["---\n","---"]},{"cell_type":"markdown","metadata":{"id":"UjJP-PvXLKVf"},"source":["# 2. FineTuning"]},{"cell_type":"markdown","metadata":{"id":"DqR62s6qMfc5"},"source":["FineTuning is a technique used in neural network training where a pre-trained model is further trained on a new task or dataset. It allows us to leverage the knowledge and representations learned by a pre-trained model and adapt it to a specific task or domain.\n","\n","In this exercise, we first train the models with a `wikipedia` dataset that contains english texts, then we fine-tune this pre-trained model again with the Shakespeare play dataset to check the effect of this method on different models."]},{"cell_type":"markdown","metadata":{"id":"hrBfhPcpOB-x"},"source":["## Load Wikipedia dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.633364Z","iopub.status.idle":"2023-12-07T02:07:33.633715Z","shell.execute_reply":"2023-12-07T02:07:33.633542Z","shell.execute_reply.started":"2023-12-07T02:07:33.633526Z"},"id":"UiqBbrV4kSYN","trusted":true},"outputs":[],"source":["!wget https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.635186Z","iopub.status.idle":"2023-12-07T02:07:33.635500Z","shell.execute_reply":"2023-12-07T02:07:33.635359Z","shell.execute_reply.started":"2023-12-07T02:07:33.635345Z"},"id":"ecXOywp5lv0D","trusted":true},"outputs":[],"source":["!tar -xvzf '/content/wikitext-2.tgz' -C '/content/data'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.636701Z","iopub.status.idle":"2023-12-07T02:07:33.637043Z","shell.execute_reply":"2023-12-07T02:07:33.636890Z","shell.execute_reply.started":"2023-12-07T02:07:33.636873Z"},"id":"c52RASUHpGM3","trusted":true},"outputs":[],"source":["!cat './data/wikitext-2/train.csv' | tr -d '\\n' > ./data/wikitext.txt"]},{"cell_type":"markdown","metadata":{"id":"6Sfwpof1OGOv"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","execution":{"iopub.status.busy":"2023-12-07T02:07:33.638436Z","iopub.status.idle":"2023-12-07T02:07:33.638923Z","shell.execute_reply":"2023-12-07T02:07:33.638712Z","shell.execute_reply.started":"2023-12-07T02:07:33.638688Z"},"id":"CniodW0Knkld","trusted":true},"outputs":[],"source":["def clean_wiki_data(data):\n","    repl=''\n","    data=re.sub('\\(', repl, data)\n","    data=re.sub('\\)', repl, data)\n","    for pattern in set(re.findall(\"=.*=\",data)):\n","        data=re.sub(pattern, repl, data)\n","    for pattern in set(re.findall(\"<unk>\",data)):\n","        data=re.sub(pattern,repl,data)\n","    for pattern in set(re.findall(r\"[^\\w ]\", data)):\n","        repl=''\n","        if pattern=='-':\n","            repl=' '\n","        if pattern!='.' and pattern!=\"\\'\":\n","            data=re.sub(\"\\\\\"+pattern, repl, data)\n","\n","    return data\n","\n","def load_data(filepath):\n","    f=open(filepath)\n","    return f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.640636Z","iopub.status.idle":"2023-12-07T02:07:33.641096Z","shell.execute_reply":"2023-12-07T02:07:33.640891Z","shell.execute_reply.started":"2023-12-07T02:07:33.640869Z"},"id":"N-BjAHGHpm4H","trusted":true},"outputs":[],"source":["wikidata=load_data(\"./data/wikitext.txt\")\n","data=wikidata[:]\n","data=clean_wiki_data(data)\n","wikiPreprocessed_file = open(\"./data/wiki_preprocesed.txt\", \"w\")\n","f = wikiPreprocessed_file.write(data)\n","wikiPreprocessed_file.close()"]},{"cell_type":"markdown","metadata":{"id":"MxILpQvZmFs8"},"source":["- Load data in amount of 50kb for finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.642421Z","iopub.status.idle":"2023-12-07T02:07:33.642924Z","shell.execute_reply":"2023-12-07T02:07:33.642717Z","shell.execute_reply.started":"2023-12-07T02:07:33.642694Z"},"id":"Q9AUJmRFPGqH","trusted":true},"outputs":[],"source":["wi_data_file = \"./data/wiki_preprocesed.txt\"\n","wi_data = open(wi_data_file, 'r').read(50000)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.644156Z","iopub.status.idle":"2023-12-07T02:07:33.644488Z","shell.execute_reply":"2023-12-07T02:07:33.644341Z","shell.execute_reply.started":"2023-12-07T02:07:33.644324Z"},"id":"hdxMOch8e4mB","trusted":true},"outputs":[],"source":["wi_data = remove_extraneous_characters(wi_data.lower(), chars)\n","wi_data_size = len(wi_data)\n","\n","wi_data = list(wi_data)\n","for i, ch in enumerate(wi_data):\n","    wi_data[i] = char_to_ix[ch]\n","\n","wi_data = torch.tensor(wi_data).to(device)\n","wi_data = torch.unsqueeze(wi_data, dim=1)"]},{"cell_type":"markdown","metadata":{"id":"_Z9RXj7fOPFh"},"source":["## Pre-training by wikipedia dataset"]},{"cell_type":"markdown","metadata":{"id":"WQRRXlSDfVEI"},"source":["### RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.645953Z","iopub.status.idle":"2023-12-07T02:07:33.646406Z","shell.execute_reply":"2023-12-07T02:07:33.646196Z","shell.execute_reply.started":"2023-12-07T02:07:33.646172Z"},"id":"00m3G13vfcQ1","trusted":true},"outputs":[],"source":["rnn_wi_losses = train_rnn(wi_data, wi_data_size, './model_wi_rnn.pth')"]},{"cell_type":"markdown","metadata":{"id":"iI8hukxjfc0g"},"source":["### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.647469Z","iopub.status.idle":"2023-12-07T02:07:33.647971Z","shell.execute_reply":"2023-12-07T02:07:33.647718Z","shell.execute_reply.started":"2023-12-07T02:07:33.647695Z"},"id":"b5KmatWPhUYB","trusted":true},"outputs":[],"source":["lstm_wi_losses = train_lstm(wi_data, wi_data_size, './model_wi_lstm.pth')"]},{"cell_type":"markdown","metadata":{"id":"s4EDmKGTOTaQ"},"source":["## Finetuning by Shakespeare"]},{"cell_type":"markdown","metadata":{"id":"J99viDjUqxUz"},"source":["- Define the following functions to use the previous model as a pre-trained model and fine-tunes it using Shakespeare's plays dataset with lower learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.649063Z","iopub.status.idle":"2023-12-07T02:07:33.649504Z","shell.execute_reply":"2023-12-07T02:07:33.649302Z","shell.execute_reply.started":"2023-12-07T02:07:33.649280Z"},"id":"FKa2cGcVjRje","trusted":true},"outputs":[],"source":["def finetune_rnn(data, data_size, model_save_file, model_pretrained_path):\n","    # RNN parameters\n","    hidden_size = 512\n","    num_layers = 6\n","    lr = 0.001\n","    epoch_num = 100\n","    losses = []\n","\n","    model = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n","    model.load_model(model_pretrained_path)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    min_loss = np.inf\n","\n","    for epoch in range(epoch_num):\n","        loss = train_epoch(model, data, data_size, epoch, optimizer)\n","        losses.append(loss)\n","\n","        if loss < min_loss:\n","            min_loss = loss\n","            model.save_model(model_save_file)\n","\n","    return losses\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.650997Z","iopub.status.idle":"2023-12-07T02:07:33.651443Z","shell.execute_reply":"2023-12-07T02:07:33.651239Z","shell.execute_reply.started":"2023-12-07T02:07:33.651216Z"},"id":"ijG0lP75lHfm","trusted":true},"outputs":[],"source":["def finetune_lstm(data, data_size, model_save_file, model_pretrained_path):\n","    # LSTM parameters\n","    hidden_size = 512\n","    num_layers = 3\n","    lr = 0.001\n","    epoch_num = 100\n","    losses = []\n","\n","    model = LSTM(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n","    model.load_model(model_pretrained_path)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    min_loss = np.inf\n","\n","    for epoch in range(epoch_num):\n","        loss = train_epoch(model, data, data_size, epoch, optimizer)\n","        losses.append(loss)\n","\n","        if loss < min_loss:\n","            min_loss = loss\n","            model.save_model(model_save_file)\n","\n","    return losses\n"]},{"cell_type":"markdown","metadata":{"id":"wwqLT0A0mSuo"},"source":["### RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.653293Z","iopub.status.idle":"2023-12-07T02:07:33.653743Z","shell.execute_reply":"2023-12-07T02:07:33.653520Z","shell.execute_reply.started":"2023-12-07T02:07:33.653499Z"},"id":"DixZCVqZmV8I","trusted":true},"outputs":[],"source":["rnn_sh_finetune_losses = finetune_rnn(sh_data, sh_data_size, './model_sh_finetune_rnn.pth', './model_wi_rnn.pth')"]},{"cell_type":"markdown","metadata":{"id":"By2oc0qlmUeq"},"source":["### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.654972Z","iopub.status.idle":"2023-12-07T02:07:33.655282Z","shell.execute_reply":"2023-12-07T02:07:33.655144Z","shell.execute_reply.started":"2023-12-07T02:07:33.655123Z"},"id":"m5tlC-O6nJdv","trusted":true},"outputs":[],"source":["lstm_sh_finetune_losses = finetune_lstm(sh_data, sh_data_size, './model_sh_finetune_lstm.pth', './model_wi_lstm.pth')"]},{"cell_type":"markdown","metadata":{"id":"gPr9aM-COlGM"},"source":["## Plotting Losses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.657036Z","iopub.status.idle":"2023-12-07T02:07:33.657346Z","shell.execute_reply":"2023-12-07T02:07:33.657208Z","shell.execute_reply.started":"2023-12-07T02:07:33.657193Z"},"id":"seWKCPaCnQ2N","trusted":true},"outputs":[],"source":["def plot_losses_together(losses1, losses2):\n","    xpoints = np.array(range(len(losses1)))\n","    ypoints1 = np.array(losses1)\n","    ypoints2 = np.array(losses2)\n","\n","    plt.plot(xpoints, ypoints1, color='blue',label='base_losses' )\n","    plt.plot(xpoints, ypoints2, color='red',label='finetune_losses' )\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"loss\")\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pl3ao3hXnne2"},"source":["### RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.658123Z","iopub.status.idle":"2023-12-07T02:07:33.658420Z","shell.execute_reply":"2023-12-07T02:07:33.658285Z","shell.execute_reply.started":"2023-12-07T02:07:33.658270Z"},"id":"oCiLrTAahpTr","trusted":true},"outputs":[],"source":["plot_losses_together(rnn_sh_losses, rnn_sh_finetune_losses)"]},{"cell_type":"markdown","metadata":{"id":"kGI_uZZhnpdF"},"source":["### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.659787Z","iopub.status.idle":"2023-12-07T02:07:33.660091Z","shell.execute_reply":"2023-12-07T02:07:33.659955Z","shell.execute_reply.started":"2023-12-07T02:07:33.659941Z"},"id":"liBF6jxUnPpO","trusted":true},"outputs":[],"source":["plot_losses_together(lstm_sh_losses, lstm_sh_finetune_losses)"]},{"cell_type":"markdown","metadata":{"id":"_3bCGDXxOvGv"},"source":["## Report"]},{"cell_type":"markdown","metadata":{"id":"SOiRxwHGnnag"},"source":["As you can see, fine-tuning has an effect in improving the training of the main model.\n","\n","By analyzing the obtained results, state the advantage of finetuning after pre-training the model by public dataset, and compare its performance in different models"]},{"cell_type":"markdown","metadata":{"id":"NAEoxdCInnW-"},"source":["<font color='#73FF73'><b>Your answer : </b></font>\n","\n","From the loss plots, it’s evident that fine-tuning the models pre-trained on the Wikipedia dataset has led to an improvement in the performance on the Shakespeare dataset. The loss values for both the RNN and LSTM models have decreased more rapidly and reached lower values compared to training from scratch. This indicates that the models have benefited from the initial pre-training phase and were able to adapt more quickly to the new dataset.\n","\n","The advantage of this fine-tuning approach is that the models can leverage the knowledge learned from a large and diverse dataset like Wikipedia, which can provide a good initialization of the model parameters. This can help the models to converge faster and achieve better performance when fine-tuned on a specific task or domain, such as generating text in the style of Shakespeare.\n","\n","Comparing the performance of the RNN and LSTM models, it appears that the LSTM model has achieved a slightly lower loss value. This suggests that the LSTM model might be better at capturing the long-term dependencies in the text, which is crucial for generating coherent and grammatically correct sentences. However, both models have shown significant improvement with the fine-tuning approach, demonstrating the effectiveness of this technique in enhancing model performance.\n"]},{"cell_type":"markdown","metadata":{"id":"6mVIAlPZLjaw"},"source":["----\n","----"]},{"cell_type":"markdown","metadata":{"id":"L3RR1kMxnnUf"},"source":["# 3. Experiment on different datasets"]},{"cell_type":"markdown","metadata":{"id":"P7BcgGRrqBqz"},"source":["In the previous section, you saw the performance results of the text generation model using the Shakespeare plays dataset. In the following, you will check the results of the LSTM model on the dialogues of the `Friends series`"]},{"cell_type":"markdown","metadata":{"id":"Wa54a7knq4N6"},"source":["## Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.661575Z","iopub.status.idle":"2023-12-07T02:07:33.661985Z","shell.execute_reply":"2023-12-07T02:07:33.661832Z","shell.execute_reply.started":"2023-12-07T02:07:33.661815Z"},"id":"u2XhR0uprKNF","trusted":true},"outputs":[],"source":["!wget https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv -O ./data/Friends.csv"]},{"cell_type":"markdown","metadata":{"id":"0X_z5mS3q7EN"},"source":["## preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.662901Z","iopub.status.idle":"2023-12-07T02:07:33.663203Z","shell.execute_reply":"2023-12-07T02:07:33.663066Z","shell.execute_reply.started":"2023-12-07T02:07:33.663051Z"},"id":"7IbK2Ean-l1w","trusted":true},"outputs":[],"source":["friends = pd.read_csv('./data/Friends.csv')\n","friends = friends.dropna()\n","friends = friends[friends['speaker'].str.contains('SCENE')==False]\n","friends['speaker'] = friends['speaker'].apply(lambda sp: sp.lower().capitalize().split(' ')[0])\n","friends_texts = friends.drop(['episode','season','scene','utterance'], axis='columns')\n","friends_texts.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.664983Z","iopub.status.idle":"2023-12-07T02:07:33.665299Z","shell.execute_reply":"2023-12-07T02:07:33.665159Z","shell.execute_reply.started":"2023-12-07T02:07:33.665144Z"},"id":"mEaZR1jS-q_W","trusted":true},"outputs":[],"source":["f = open(\"./data/fiends.txt\", \"w\")\n","for i,row in friends_texts.iterrows():\n","    f.write(row['speaker'] + ':\\n' + row['text'] + '\\n\\n')\n","\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.666184Z","iopub.status.idle":"2023-12-07T02:07:33.666509Z","shell.execute_reply":"2023-12-07T02:07:33.666365Z","shell.execute_reply.started":"2023-12-07T02:07:33.666349Z"},"id":"1SPBHV5u-62e","trusted":true},"outputs":[],"source":["fr_data_file = \"./data/fiends.txt\"\n","fr_data = open(fr_data_file, 'r').read(30000)\n","fr_data = remove_extraneous_characters(fr_data.lower(), chars)\n","fr_data_size = len(fr_data)\n","\n","fr_data = list(fr_data)\n","for i, ch in enumerate(fr_data):\n","    fr_data[i] = char_to_ix[ch]\n","\n","fr_data = torch.tensor(fr_data).to(device)\n","fr_data = torch.unsqueeze(fr_data, dim=1)"]},{"cell_type":"markdown","metadata":{"id":"6drMR6Gtq_jj"},"source":["## Train finetuned LSTM by friends dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.667804Z","iopub.status.idle":"2023-12-07T02:07:33.668235Z","shell.execute_reply":"2023-12-07T02:07:33.668034Z","shell.execute_reply.started":"2023-12-07T02:07:33.668012Z"},"id":"J7ruLz2r_O3r","trusted":true},"outputs":[],"source":["lstm_fr_finetune_losses = finetune_lstm(fr_data, fr_data_size, './model_fr_lstm.pth', './model_wi_lstm.pth')"]},{"cell_type":"markdown","metadata":{"id":"62xwUDiPrEDq"},"source":["## Generating texts"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.669506Z","iopub.status.idle":"2023-12-07T02:07:33.669965Z","shell.execute_reply":"2023-12-07T02:07:33.669759Z","shell.execute_reply.started":"2023-12-07T02:07:33.669729Z"},"id":"26S9Um9Jnm9T","trusted":true},"outputs":[],"source":["best_model_lstm =  LSTM(vocab_size, vocab_size, 512, 3).to(device)\n","best_model_lstm.load_model('./model_fr_lstm.pth')\n","print(\"best loss\", min(lstm_fr_finetune_losses))\n","generate_text(best_model_lstm, fr_data, fr_data_size)"]},{"cell_type":"markdown","metadata":{"id":"Z7s2pjFjR-l4"},"source":["- As you can see, the LSTM network has been able to learn the features of different datasets in terms of sentence length and writing style and use it in text generation."]},{"cell_type":"markdown","metadata":{"id":"7BGd20eltW3C"},"source":["## The output of finetuned models on different datasets on the input sample"]},{"cell_type":"markdown","metadata":{"id":"Rp_UEVqAQuvy"},"source":["- In this section, you can see the result of the text generated by models with a sample input text."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.671041Z","iopub.status.idle":"2023-12-07T02:07:33.671371Z","shell.execute_reply":"2023-12-07T02:07:33.671223Z","shell.execute_reply.started":"2023-12-07T02:07:33.671207Z"},"id":"NS_NqaJruKQe","trusted":true},"outputs":[],"source":["input_sample_text = \"Hello, have a nice day.\\n\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.672679Z","iopub.status.idle":"2023-12-07T02:07:33.673019Z","shell.execute_reply":"2023-12-07T02:07:33.672874Z","shell.execute_reply.started":"2023-12-07T02:07:33.672857Z"},"id":"mWgLQM0G2r7D","trusted":true},"outputs":[],"source":["best_model_lstm =  LSTM(vocab_size, vocab_size, 512, 3).to(device)\n","best_model_lstm.load_model('./model_fr_lstm.pth')\n","generate_text(best_model_lstm, fr_data, fr_data_size, create_input_sample_dataset(input_sample_text),100)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-07T02:07:33.674219Z","iopub.status.idle":"2023-12-07T02:07:33.674551Z","shell.execute_reply":"2023-12-07T02:07:33.674406Z","shell.execute_reply.started":"2023-12-07T02:07:33.674390Z"},"id":"gmFUUJCUtLKm","trusted":true},"outputs":[],"source":["best_model_lstm =  LSTM(vocab_size, vocab_size, 512, 3).to(device)\n","best_model_lstm.load_model('./model_sh_finetune_lstm.pth')\n","generate_text(best_model_lstm, sh_data, sh_data_size, create_input_sample_dataset(input_sample_text),100)"]},{"cell_type":"markdown","metadata":{"id":"kT-dTrk_RQyE"},"source":["## Report"]},{"cell_type":"markdown","metadata":{"id":"vNJdGJN1RLpz"},"source":["According to the sample input and output produced by the fine-tuned model with the Shakespeare dataset and the Friends dataset, which output is more meaningful and what is the reason for this difference?"]},{"cell_type":"markdown","metadata":{"id":"k4nBCaS2RWN_"},"source":["<font color='#73FF73'><b>Your answer : </b></font>\n","\n","Based on the nature of the datasets, it’s likely that the output from the model fine-tuned on the Friends dataset would be more meaningful in a modern context. The Friends dataset consists of dialogues from a contemporary TV show, which is likely to contain everyday language and scenarios that are more relatable and understandable to a modern audience.\n","\n","On the other hand, while the output from the model fine-tuned on the Shakespeare dataset might be more poetic and complex, it could also be more difficult to understand due to the archaic language and intricate expressions characteristic of Shakespeare’s works.\n","\n","Therefore, if we consider “meaningful” as being easily understood and relatable, the model fine-tuned on the Friends dataset would likely produce more meaningful output. However, it’s important to note that the definition of “meaningful” can vary depending on the context and the specific requirements of the task. For a task requiring a more artistic and poetic style, the model fine-tuned on the Shakespeare dataset might be more suitable.\n"]},{"cell_type":"markdown","metadata":{"id":"Tq_1cBKGRX8i"},"source":["----\n","----"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4122374,"sourceId":7142219,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
