{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISKf-ajkZKU6"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "## Course Name: Deep Learning\n",
        "#### Lecturer: Dr. Beigy\n",
        "\n",
        "---\n",
        "\n",
        "#### Notebooks Supervised By: Zeinab Sadat Taghavi\n",
        "#### Notebooks Prepared By: Zahra Rahimi, Zahra Khoramnejad, Mehran Sarmadi\n",
        "\n",
        "**Contact**: Ask your questions in Quera\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: Replace the placeholders (between `## COMPLETE THE FOLLOWING SECTION  ##` and `## THE END ##`) with the appropriate details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7FFJaRgaqC1"
      },
      "source": [
        "---\n",
        "---\n",
        "## 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q8G6YosbKrX"
      },
      "source": [
        "In this notebook you have to design and train models for a time series prediction task on the provided dataset using these three different architectures:\n",
        "\n",
        "- Simple RNN\n",
        "\n",
        "- GRU\n",
        "\n",
        "- LSTM\n",
        "\n",
        "You will compare and rank them at the end of the notebook and explain why they were ranked that way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FANbmLJ2d2dp"
      },
      "source": [
        "---\n",
        "### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdzOu4UGgDFQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prWan6z7eKEp"
      },
      "source": [
        "---\n",
        "---\n",
        "## 2 Dataset\n",
        "Electric Production IP Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtouQ7MVomh0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Electric_Production.csv', index_col='Date', parse_dates=True, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "-m3r2Dw_os75",
        "outputId": "98d8b0b6-6d12-4696-c609-bf24c550ba04"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4c31a4af-0cf0-42f0-b6cc-26902f3149fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1985-01-01</th>\n",
              "      <td>72.505203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-02-01</th>\n",
              "      <td>70.671997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-03-01</th>\n",
              "      <td>62.450199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-04-01</th>\n",
              "      <td>57.471401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-05-01</th>\n",
              "      <td>55.315102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c31a4af-0cf0-42f0-b6cc-26902f3149fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4c31a4af-0cf0-42f0-b6cc-26902f3149fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4c31a4af-0cf0-42f0-b6cc-26902f3149fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-67207f4b-0a73-437d-8ec0-95f2913b1969\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-67207f4b-0a73-437d-8ec0-95f2913b1969')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-67207f4b-0a73-437d-8ec0-95f2913b1969 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                Value\n",
              "Date                 \n",
              "1985-01-01  72.505203\n",
              "1985-02-01  70.671997\n",
              "1985-03-01  62.450199\n",
              "1985-04-01  57.471401\n",
              "1985-05-01  55.315102"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc6T4bkqqVQX",
        "outputId": "e0f489e9-cdd4-48f2-8d47-ad9c83b7fe90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 397 entries, 1985-01-01 to 2018-01-01\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Value   397 non-null    float32\n",
            "dtypes: float32(1)\n",
            "memory usage: 4.7 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "o_WhChFOqYb4",
        "outputId": "6eb1b565-c322-4aa9-81fb-8a1520de9938"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXj1Z6_vgSIZ"
      },
      "source": [
        "---\n",
        "### 2.1 Load and prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd1P3PV4nZ2X"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset, look_back):\n",
        "    \"\"\"Transform a time series data into a prediction dataset\n",
        "\n",
        "    Args:\n",
        "        dataset: A numpy array of time series, first dimension is the time steps\n",
        "        look_back: Size of window for prediction\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    dataset = np.array(dataset)\n",
        "    data_length = len(dataset)\n",
        "    for i in range(look_back, data_length):\n",
        "        input = dataset[i-look_back: i]\n",
        "        output = dataset[i]\n",
        "        X.append(input)\n",
        "        y.append(output)\n",
        "\n",
        "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSFy2eNF5e-",
        "outputId": "38f01038-f712-40d6-e6a5-e18adce26674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH2FMeIhh169",
        "outputId": "a495d89b-6b49-4fb8-b24e-d82986ad98ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train data -> torch.Size([237, 60, 1]) \n",
            "Shape of y_train data -> torch.Size([237, 1]) \n",
            "Shape of X_val data -> torch.Size([40, 60, 1]) \n",
            "Shape of y_val data -> torch.Size([40, 1]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "look_back = 60\n",
        "\n",
        "data_length = len(df)\n",
        "\n",
        "train_data_size = int(data_length * 0.75)\n",
        "validaion_data_size = int(data_length * 0.25)\n",
        "\n",
        "\n",
        "train_data = df[: train_data_size]\n",
        "validation_data = df[train_data_size: ]\n",
        "\n",
        "\n",
        "X_train, y_train = prepare_dataset(train_data, look_back)\n",
        "X_val, y_val = prepare_dataset(validation_data, look_back)\n",
        "\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "X_val = X_val.to(device)\n",
        "y_val = y_val.to(device)\n",
        "\n",
        "\n",
        "print(  f\"Shape of X_train data -> {X_train.shape} \\n\"\n",
        "        f\"Shape of y_train data -> {y_train.shape} \\n\"\n",
        "        f\"Shape of X_val data -> {X_val.shape} \\n\"\n",
        "        f\"Shape of y_val data -> {y_val.shape} \\n\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78V0kKvjp8f"
      },
      "source": [
        "---\n",
        "---\n",
        "## 3 Trainer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEIg5tjsjnEi"
      },
      "outputs": [],
      "source": [
        "def trainer(model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs):\n",
        "    early_stopping_patience = 150\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    valid_loss_min=np.inf\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        #################################################################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        # - You can feed all the data to the model at once because the data is small.\n",
        "        #################################################################################\n",
        "\n",
        "        # Forward and loss\n",
        "\n",
        "\n",
        "        # Backward and optimization\n",
        "\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output_val = model(X_val)\n",
        "            valid_loss = criterion(output_val, y_val)\n",
        "            val_losses.append(valid_loss.item())\n",
        "\n",
        "            if valid_loss <= valid_loss_min:\n",
        "                best_model = best_model = copy.deepcopy(model)\n",
        "                print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}).')\n",
        "                valid_loss_min = valid_loss\n",
        "                early_stopping_counter = 0    # Reset counter if validation loss decreases\n",
        "            else:\n",
        "                print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
        "                early_stopping_counter += 1\n",
        "\n",
        "            if early_stopping_counter > early_stopping_patience:\n",
        "                print('Early stopped at epoch :', epoch)\n",
        "                break\n",
        "\n",
        "            print(f'\\t Train_Loss: {train_loss:.4f} Val_Loss: {valid_loss:.4f}  BEST VAL Loss: {valid_loss_min:.4f}\\n')\n",
        "\n",
        "    return best_model, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIfT0qVkOTzG"
      },
      "source": [
        "---\n",
        "---\n",
        "## 4 RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwmm5DhEBl8d"
      },
      "source": [
        "---\n",
        "### 4.1 Define single RNN cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YeW2H2cKsH7"
      },
      "outputs": [],
      "source": [
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=\"tanh\"):\n",
        "        super(RNNCell, self).__init__()\n",
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "        \"\"\"Define Needed Layers \"\"\"\n",
        "\t\t\t\t#Your Code Here\n",
        "\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "        \"\"\"Define Forward pass\"\"\"\n",
        "\t\t\t\t#Your Code Here\n",
        "\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################\n",
        "        return hy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ay_sL8ZBq1R"
      },
      "source": [
        "---\n",
        "### 4.2 RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAcO44hi5HUf"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size, hidden_size, num_layers, bias=bias, batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msF4j7_fB6S5"
      },
      "source": [
        "---\n",
        "### 4.3 Train RNN model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwHvK1bI5GBz",
        "outputId": "a75074f9-f59b-4ad6-ff1d-02cacd0c9c6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SimpleRNN(\n",
              "  (rnn_cell_list): ModuleList(\n",
              "    (0): RNNCell(\n",
              "      (Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate model\n",
        "SimpleRNN_model = SimpleRNN(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "SimpleRNN_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "431lZ_uOlpsQ"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(SimpleRNN_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWMAFQBKQ5mq",
        "outputId": "011bedb9-12cb-4925-d578-e002b0215faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\t Train_Loss: 188.6313 Val_Loss: 558.1227  BEST VAL Loss: 558.1227\n",
            "\n",
            "Epoch 334: Validation loss decreased (558.122681 --> 554.916138).\n",
            "\t Train_Loss: 187.5602 Val_Loss: 554.9161  BEST VAL Loss: 554.9161\n",
            "\n",
            "Epoch 335: Validation loss decreased (554.916138 --> 551.749023).\n",
            "\t Train_Loss: 186.5098 Val_Loss: 551.7490  BEST VAL Loss: 551.7490\n",
            "\n",
            "Epoch 336: Validation loss decreased (551.749023 --> 548.621399).\n",
            "\t Train_Loss: 185.4801 Val_Loss: 548.6214  BEST VAL Loss: 548.6214\n",
            "\n",
            "Epoch 337: Validation loss decreased (548.621399 --> 545.532349).\n",
            "\t Train_Loss: 184.4703 Val_Loss: 545.5323  BEST VAL Loss: 545.5323\n",
            "\n",
            "Epoch 338: Validation loss decreased (545.532349 --> 542.481873).\n",
            "\t Train_Loss: 183.4805 Val_Loss: 542.4819  BEST VAL Loss: 542.4819\n",
            "\n",
            "Epoch 339: Validation loss decreased (542.481873 --> 539.469360).\n",
            "\t Train_Loss: 182.5100 Val_Loss: 539.4694  BEST VAL Loss: 539.4694\n",
            "\n",
            "Epoch 340: Validation loss decreased (539.469360 --> 536.492859).\n",
            "\t Train_Loss: 181.5587 Val_Loss: 536.4929  BEST VAL Loss: 536.4929\n",
            "\n",
            "Epoch 341: Validation loss decreased (536.492859 --> 533.554138).\n",
            "\t Train_Loss: 180.6261 Val_Loss: 533.5541  BEST VAL Loss: 533.5541\n",
            "\n",
            "Epoch 342: Validation loss decreased (533.554138 --> 530.650391).\n",
            "\t Train_Loss: 179.7119 Val_Loss: 530.6504  BEST VAL Loss: 530.6504\n",
            "\n",
            "Epoch 343: Validation loss decreased (530.650391 --> 527.784363).\n",
            "\t Train_Loss: 178.8158 Val_Loss: 527.7844  BEST VAL Loss: 527.7844\n",
            "\n",
            "Epoch 344: Validation loss decreased (527.784363 --> 524.953125).\n",
            "\t Train_Loss: 177.9376 Val_Loss: 524.9531  BEST VAL Loss: 524.9531\n",
            "\n",
            "Epoch 345: Validation loss decreased (524.953125 --> 522.156555).\n",
            "\t Train_Loss: 177.0768 Val_Loss: 522.1566  BEST VAL Loss: 522.1566\n",
            "\n",
            "Epoch 346: Validation loss decreased (522.156555 --> 519.394958).\n",
            "\t Train_Loss: 176.2331 Val_Loss: 519.3950  BEST VAL Loss: 519.3950\n",
            "\n",
            "Epoch 347: Validation loss decreased (519.394958 --> 516.667480).\n",
            "\t Train_Loss: 175.4064 Val_Loss: 516.6675  BEST VAL Loss: 516.6675\n",
            "\n",
            "Epoch 348: Validation loss decreased (516.667480 --> 513.974426).\n",
            "\t Train_Loss: 174.5963 Val_Loss: 513.9744  BEST VAL Loss: 513.9744\n",
            "\n",
            "Epoch 349: Validation loss decreased (513.974426 --> 511.314056).\n",
            "\t Train_Loss: 173.8023 Val_Loss: 511.3141  BEST VAL Loss: 511.3141\n",
            "\n",
            "Epoch 350: Validation loss decreased (511.314056 --> 508.686829).\n",
            "\t Train_Loss: 173.0244 Val_Loss: 508.6868  BEST VAL Loss: 508.6868\n",
            "\n",
            "Epoch 351: Validation loss decreased (508.686829 --> 506.092438).\n",
            "\t Train_Loss: 172.2622 Val_Loss: 506.0924  BEST VAL Loss: 506.0924\n",
            "\n",
            "Epoch 352: Validation loss decreased (506.092438 --> 503.529877).\n",
            "\t Train_Loss: 171.5153 Val_Loss: 503.5299  BEST VAL Loss: 503.5299\n",
            "\n",
            "Epoch 353: Validation loss decreased (503.529877 --> 500.999603).\n",
            "\t Train_Loss: 170.7836 Val_Loss: 500.9996  BEST VAL Loss: 500.9996\n",
            "\n",
            "Epoch 354: Validation loss decreased (500.999603 --> 498.500702).\n",
            "\t Train_Loss: 170.0667 Val_Loss: 498.5007  BEST VAL Loss: 498.5007\n",
            "\n",
            "Epoch 355: Validation loss decreased (498.500702 --> 496.032806).\n",
            "\t Train_Loss: 169.3644 Val_Loss: 496.0328  BEST VAL Loss: 496.0328\n",
            "\n",
            "Epoch 356: Validation loss decreased (496.032806 --> 493.595764).\n",
            "\t Train_Loss: 168.6764 Val_Loss: 493.5958  BEST VAL Loss: 493.5958\n",
            "\n",
            "Epoch 357: Validation loss decreased (493.595764 --> 491.188202).\n",
            "\t Train_Loss: 168.0026 Val_Loss: 491.1882  BEST VAL Loss: 491.1882\n",
            "\n",
            "Epoch 358: Validation loss decreased (491.188202 --> 488.811371).\n",
            "\t Train_Loss: 167.3423 Val_Loss: 488.8114  BEST VAL Loss: 488.8114\n",
            "\n",
            "Epoch 359: Validation loss decreased (488.811371 --> 486.464569).\n",
            "\t Train_Loss: 166.6958 Val_Loss: 486.4646  BEST VAL Loss: 486.4646\n",
            "\n",
            "Epoch 360: Validation loss decreased (486.464569 --> 484.145752).\n",
            "\t Train_Loss: 166.0625 Val_Loss: 484.1458  BEST VAL Loss: 484.1458\n",
            "\n",
            "Epoch 361: Validation loss decreased (484.145752 --> 481.856903).\n",
            "\t Train_Loss: 165.4423 Val_Loss: 481.8569  BEST VAL Loss: 481.8569\n",
            "\n",
            "Epoch 362: Validation loss decreased (481.856903 --> 479.596191).\n",
            "\t Train_Loss: 164.8347 Val_Loss: 479.5962  BEST VAL Loss: 479.5962\n",
            "\n",
            "Epoch 363: Validation loss decreased (479.596191 --> 477.363525).\n",
            "\t Train_Loss: 164.2398 Val_Loss: 477.3635  BEST VAL Loss: 477.3635\n",
            "\n",
            "Epoch 364: Validation loss decreased (477.363525 --> 475.158997).\n",
            "\t Train_Loss: 163.6574 Val_Loss: 475.1590  BEST VAL Loss: 475.1590\n",
            "\n",
            "Epoch 365: Validation loss decreased (475.158997 --> 472.981506).\n",
            "\t Train_Loss: 163.0869 Val_Loss: 472.9815  BEST VAL Loss: 472.9815\n",
            "\n",
            "Epoch 366: Validation loss decreased (472.981506 --> 470.831451).\n",
            "\t Train_Loss: 162.5283 Val_Loss: 470.8315  BEST VAL Loss: 470.8315\n",
            "\n",
            "Epoch 367: Validation loss decreased (470.831451 --> 468.708099).\n",
            "\t Train_Loss: 161.9815 Val_Loss: 468.7081  BEST VAL Loss: 468.7081\n",
            "\n",
            "Epoch 368: Validation loss decreased (468.708099 --> 466.611816).\n",
            "\t Train_Loss: 161.4460 Val_Loss: 466.6118  BEST VAL Loss: 466.6118\n",
            "\n",
            "Epoch 369: Validation loss decreased (466.611816 --> 464.541656).\n",
            "\t Train_Loss: 160.9217 Val_Loss: 464.5417  BEST VAL Loss: 464.5417\n",
            "\n",
            "Epoch 370: Validation loss decreased (464.541656 --> 462.496490).\n",
            "\t Train_Loss: 160.4085 Val_Loss: 462.4965  BEST VAL Loss: 462.4965\n",
            "\n",
            "Epoch 371: Validation loss decreased (462.496490 --> 460.477631).\n",
            "\t Train_Loss: 159.9062 Val_Loss: 460.4776  BEST VAL Loss: 460.4776\n",
            "\n",
            "Epoch 372: Validation loss decreased (460.477631 --> 458.484100).\n",
            "\t Train_Loss: 159.4144 Val_Loss: 458.4841  BEST VAL Loss: 458.4841\n",
            "\n",
            "Epoch 373: Validation loss decreased (458.484100 --> 456.515198).\n",
            "\t Train_Loss: 158.9329 Val_Loss: 456.5152  BEST VAL Loss: 456.5152\n",
            "\n",
            "Epoch 374: Validation loss decreased (456.515198 --> 454.570404).\n",
            "\t Train_Loss: 158.4618 Val_Loss: 454.5704  BEST VAL Loss: 454.5704\n",
            "\n",
            "Epoch 375: Validation loss decreased (454.570404 --> 452.650848).\n",
            "\t Train_Loss: 158.0005 Val_Loss: 452.6508  BEST VAL Loss: 452.6508\n",
            "\n",
            "Epoch 376: Validation loss decreased (452.650848 --> 450.755371).\n",
            "\t Train_Loss: 157.5490 Val_Loss: 450.7554  BEST VAL Loss: 450.7554\n",
            "\n",
            "Epoch 377: Validation loss decreased (450.755371 --> 448.883209).\n",
            "\t Train_Loss: 157.1072 Val_Loss: 448.8832  BEST VAL Loss: 448.8832\n",
            "\n",
            "Epoch 378: Validation loss decreased (448.883209 --> 447.034485).\n",
            "\t Train_Loss: 156.6748 Val_Loss: 447.0345  BEST VAL Loss: 447.0345\n",
            "\n",
            "Epoch 379: Validation loss decreased (447.034485 --> 445.209045).\n",
            "\t Train_Loss: 156.2517 Val_Loss: 445.2090  BEST VAL Loss: 445.2090\n",
            "\n",
            "Epoch 380: Validation loss decreased (445.209045 --> 443.407043).\n",
            "\t Train_Loss: 155.8378 Val_Loss: 443.4070  BEST VAL Loss: 443.4070\n",
            "\n",
            "Epoch 381: Validation loss decreased (443.407043 --> 441.626678).\n",
            "\t Train_Loss: 155.4325 Val_Loss: 441.6267  BEST VAL Loss: 441.6267\n",
            "\n",
            "Epoch 382: Validation loss decreased (441.626678 --> 439.868866).\n",
            "\t Train_Loss: 155.0360 Val_Loss: 439.8689  BEST VAL Loss: 439.8689\n",
            "\n",
            "Epoch 383: Validation loss decreased (439.868866 --> 438.133209).\n",
            "\t Train_Loss: 154.6481 Val_Loss: 438.1332  BEST VAL Loss: 438.1332\n",
            "\n",
            "Epoch 384: Validation loss decreased (438.133209 --> 436.420135).\n",
            "\t Train_Loss: 154.2686 Val_Loss: 436.4201  BEST VAL Loss: 436.4201\n",
            "\n",
            "Epoch 385: Validation loss decreased (436.420135 --> 434.727844).\n",
            "\t Train_Loss: 153.8973 Val_Loss: 434.7278  BEST VAL Loss: 434.7278\n",
            "\n",
            "Epoch 386: Validation loss decreased (434.727844 --> 433.056549).\n",
            "\t Train_Loss: 153.5342 Val_Loss: 433.0565  BEST VAL Loss: 433.0565\n",
            "\n",
            "Epoch 387: Validation loss decreased (433.056549 --> 431.406799).\n",
            "\t Train_Loss: 153.1789 Val_Loss: 431.4068  BEST VAL Loss: 431.4068\n",
            "\n",
            "Epoch 388: Validation loss decreased (431.406799 --> 429.777252).\n",
            "\t Train_Loss: 152.8312 Val_Loss: 429.7773  BEST VAL Loss: 429.7773\n",
            "\n",
            "Epoch 389: Validation loss decreased (429.777252 --> 428.168762).\n",
            "\t Train_Loss: 152.4912 Val_Loss: 428.1688  BEST VAL Loss: 428.1688\n",
            "\n",
            "Epoch 390: Validation loss decreased (428.168762 --> 426.580414).\n",
            "\t Train_Loss: 152.1586 Val_Loss: 426.5804  BEST VAL Loss: 426.5804\n",
            "\n",
            "Epoch 391: Validation loss decreased (426.580414 --> 425.012177).\n",
            "\t Train_Loss: 151.8334 Val_Loss: 425.0122  BEST VAL Loss: 425.0122\n",
            "\n",
            "Epoch 392: Validation loss decreased (425.012177 --> 423.463287).\n",
            "\t Train_Loss: 151.5153 Val_Loss: 423.4633  BEST VAL Loss: 423.4633\n",
            "\n",
            "Epoch 393: Validation loss decreased (423.463287 --> 421.934174).\n",
            "\t Train_Loss: 151.2042 Val_Loss: 421.9342  BEST VAL Loss: 421.9342\n",
            "\n",
            "Epoch 394: Validation loss decreased (421.934174 --> 420.424225).\n",
            "\t Train_Loss: 150.8998 Val_Loss: 420.4242  BEST VAL Loss: 420.4242\n",
            "\n",
            "Epoch 395: Validation loss decreased (420.424225 --> 418.933502).\n",
            "\t Train_Loss: 150.6023 Val_Loss: 418.9335  BEST VAL Loss: 418.9335\n",
            "\n",
            "Epoch 396: Validation loss decreased (418.933502 --> 417.461151).\n",
            "\t Train_Loss: 150.3115 Val_Loss: 417.4612  BEST VAL Loss: 417.4612\n",
            "\n",
            "Epoch 397: Validation loss decreased (417.461151 --> 416.007904).\n",
            "\t Train_Loss: 150.0270 Val_Loss: 416.0079  BEST VAL Loss: 416.0079\n",
            "\n",
            "Epoch 398: Validation loss decreased (416.007904 --> 414.572998).\n",
            "\t Train_Loss: 149.7488 Val_Loss: 414.5730  BEST VAL Loss: 414.5730\n",
            "\n",
            "Epoch 399: Validation loss decreased (414.572998 --> 413.155975).\n",
            "\t Train_Loss: 149.4767 Val_Loss: 413.1560  BEST VAL Loss: 413.1560\n",
            "\n",
            "Epoch 400: Validation loss decreased (413.155975 --> 411.757050).\n",
            "\t Train_Loss: 149.2110 Val_Loss: 411.7570  BEST VAL Loss: 411.7570\n",
            "\n",
            "Epoch 401: Validation loss decreased (411.757050 --> 410.375397).\n",
            "\t Train_Loss: 148.9509 Val_Loss: 410.3754  BEST VAL Loss: 410.3754\n",
            "\n",
            "Epoch 402: Validation loss decreased (410.375397 --> 409.011566).\n",
            "\t Train_Loss: 148.6968 Val_Loss: 409.0116  BEST VAL Loss: 409.0116\n",
            "\n",
            "Epoch 403: Validation loss decreased (409.011566 --> 407.664886).\n",
            "\t Train_Loss: 148.4483 Val_Loss: 407.6649  BEST VAL Loss: 407.6649\n",
            "\n",
            "Epoch 404: Validation loss decreased (407.664886 --> 406.334961).\n",
            "\t Train_Loss: 148.2056 Val_Loss: 406.3350  BEST VAL Loss: 406.3350\n",
            "\n",
            "Epoch 405: Validation loss decreased (406.334961 --> 405.022278).\n",
            "\t Train_Loss: 147.9682 Val_Loss: 405.0223  BEST VAL Loss: 405.0223\n",
            "\n",
            "Epoch 406: Validation loss decreased (405.022278 --> 403.726318).\n",
            "\t Train_Loss: 147.7361 Val_Loss: 403.7263  BEST VAL Loss: 403.7263\n",
            "\n",
            "Epoch 407: Validation loss decreased (403.726318 --> 402.446594).\n",
            "\t Train_Loss: 147.5094 Val_Loss: 402.4466  BEST VAL Loss: 402.4466\n",
            "\n",
            "Epoch 408: Validation loss decreased (402.446594 --> 401.183197).\n",
            "\t Train_Loss: 147.2878 Val_Loss: 401.1832  BEST VAL Loss: 401.1832\n",
            "\n",
            "Epoch 409: Validation loss decreased (401.183197 --> 399.935364).\n",
            "\t Train_Loss: 147.0712 Val_Loss: 399.9354  BEST VAL Loss: 399.9354\n",
            "\n",
            "Epoch 410: Validation loss decreased (399.935364 --> 398.703705).\n",
            "\t Train_Loss: 146.8596 Val_Loss: 398.7037  BEST VAL Loss: 398.7037\n",
            "\n",
            "Epoch 411: Validation loss decreased (398.703705 --> 397.487762).\n",
            "\t Train_Loss: 146.6528 Val_Loss: 397.4878  BEST VAL Loss: 397.4878\n",
            "\n",
            "Epoch 412: Validation loss decreased (397.487762 --> 396.287079).\n",
            "\t Train_Loss: 146.4505 Val_Loss: 396.2871  BEST VAL Loss: 396.2871\n",
            "\n",
            "Epoch 413: Validation loss decreased (396.287079 --> 395.101410).\n",
            "\t Train_Loss: 146.2531 Val_Loss: 395.1014  BEST VAL Loss: 395.1014\n",
            "\n",
            "Epoch 414: Validation loss decreased (395.101410 --> 393.931915).\n",
            "\t Train_Loss: 146.0601 Val_Loss: 393.9319  BEST VAL Loss: 393.9319\n",
            "\n",
            "Epoch 415: Validation loss decreased (393.931915 --> 392.775970).\n",
            "\t Train_Loss: 145.8716 Val_Loss: 392.7760  BEST VAL Loss: 392.7760\n",
            "\n",
            "Epoch 416: Validation loss decreased (392.775970 --> 391.635406).\n",
            "\t Train_Loss: 145.6875 Val_Loss: 391.6354  BEST VAL Loss: 391.6354\n",
            "\n",
            "Epoch 417: Validation loss decreased (391.635406 --> 390.509369).\n",
            "\t Train_Loss: 145.5076 Val_Loss: 390.5094  BEST VAL Loss: 390.5094\n",
            "\n",
            "Epoch 418: Validation loss decreased (390.509369 --> 389.397369).\n",
            "\t Train_Loss: 145.3317 Val_Loss: 389.3974  BEST VAL Loss: 389.3974\n",
            "\n",
            "Epoch 419: Validation loss decreased (389.397369 --> 388.300140).\n",
            "\t Train_Loss: 145.1600 Val_Loss: 388.3001  BEST VAL Loss: 388.3001\n",
            "\n",
            "Epoch 420: Validation loss decreased (388.300140 --> 387.216064).\n",
            "\t Train_Loss: 144.9923 Val_Loss: 387.2161  BEST VAL Loss: 387.2161\n",
            "\n",
            "Epoch 421: Validation loss decreased (387.216064 --> 386.146149).\n",
            "\t Train_Loss: 144.8285 Val_Loss: 386.1461  BEST VAL Loss: 386.1461\n",
            "\n",
            "Epoch 422: Validation loss decreased (386.146149 --> 385.090057).\n",
            "\t Train_Loss: 144.6684 Val_Loss: 385.0901  BEST VAL Loss: 385.0901\n",
            "\n",
            "Epoch 423: Validation loss decreased (385.090057 --> 384.047333).\n",
            "\t Train_Loss: 144.5121 Val_Loss: 384.0473  BEST VAL Loss: 384.0473\n",
            "\n",
            "Epoch 424: Validation loss decreased (384.047333 --> 383.018097).\n",
            "\t Train_Loss: 144.3595 Val_Loss: 383.0181  BEST VAL Loss: 383.0181\n",
            "\n",
            "Epoch 425: Validation loss decreased (383.018097 --> 382.001709).\n",
            "\t Train_Loss: 144.2104 Val_Loss: 382.0017  BEST VAL Loss: 382.0017\n",
            "\n",
            "Epoch 426: Validation loss decreased (382.001709 --> 380.998627).\n",
            "\t Train_Loss: 144.0649 Val_Loss: 380.9986  BEST VAL Loss: 380.9986\n",
            "\n",
            "Epoch 427: Validation loss decreased (380.998627 --> 380.008453).\n",
            "\t Train_Loss: 143.9228 Val_Loss: 380.0085  BEST VAL Loss: 380.0085\n",
            "\n",
            "Epoch 428: Validation loss decreased (380.008453 --> 379.030487).\n",
            "\t Train_Loss: 143.7840 Val_Loss: 379.0305  BEST VAL Loss: 379.0305\n",
            "\n",
            "Epoch 429: Validation loss decreased (379.030487 --> 378.065643).\n",
            "\t Train_Loss: 143.6484 Val_Loss: 378.0656  BEST VAL Loss: 378.0656\n",
            "\n",
            "Epoch 430: Validation loss decreased (378.065643 --> 377.112549).\n",
            "\t Train_Loss: 143.5162 Val_Loss: 377.1125  BEST VAL Loss: 377.1125\n",
            "\n",
            "Epoch 431: Validation loss decreased (377.112549 --> 376.171478).\n",
            "\t Train_Loss: 143.3869 Val_Loss: 376.1715  BEST VAL Loss: 376.1715\n",
            "\n",
            "Epoch 432: Validation loss decreased (376.171478 --> 375.242828).\n",
            "\t Train_Loss: 143.2608 Val_Loss: 375.2428  BEST VAL Loss: 375.2428\n",
            "\n",
            "Epoch 433: Validation loss decreased (375.242828 --> 374.326569).\n",
            "\t Train_Loss: 143.1376 Val_Loss: 374.3266  BEST VAL Loss: 374.3266\n",
            "\n",
            "Epoch 434: Validation loss decreased (374.326569 --> 373.422119).\n",
            "\t Train_Loss: 143.0175 Val_Loss: 373.4221  BEST VAL Loss: 373.4221\n",
            "\n",
            "Epoch 435: Validation loss decreased (373.422119 --> 372.528839).\n",
            "\t Train_Loss: 142.9001 Val_Loss: 372.5288  BEST VAL Loss: 372.5288\n",
            "\n",
            "Epoch 436: Validation loss decreased (372.528839 --> 371.647064).\n",
            "\t Train_Loss: 142.7856 Val_Loss: 371.6471  BEST VAL Loss: 371.6471\n",
            "\n",
            "Epoch 437: Validation loss decreased (371.647064 --> 370.776520).\n",
            "\t Train_Loss: 142.6738 Val_Loss: 370.7765  BEST VAL Loss: 370.7765\n",
            "\n",
            "Epoch 438: Validation loss decreased (370.776520 --> 369.917450).\n",
            "\t Train_Loss: 142.5647 Val_Loss: 369.9174  BEST VAL Loss: 369.9174\n",
            "\n",
            "Epoch 439: Validation loss decreased (369.917450 --> 369.069611).\n",
            "\t Train_Loss: 142.4582 Val_Loss: 369.0696  BEST VAL Loss: 369.0696\n",
            "\n",
            "Epoch 440: Validation loss decreased (369.069611 --> 368.232117).\n",
            "\t Train_Loss: 142.3543 Val_Loss: 368.2321  BEST VAL Loss: 368.2321\n",
            "\n",
            "Epoch 441: Validation loss decreased (368.232117 --> 367.405640).\n",
            "\t Train_Loss: 142.2529 Val_Loss: 367.4056  BEST VAL Loss: 367.4056\n",
            "\n",
            "Epoch 442: Validation loss decreased (367.405640 --> 366.590302).\n",
            "\t Train_Loss: 142.1539 Val_Loss: 366.5903  BEST VAL Loss: 366.5903\n",
            "\n",
            "Epoch 443: Validation loss decreased (366.590302 --> 365.785126).\n",
            "\t Train_Loss: 142.0573 Val_Loss: 365.7851  BEST VAL Loss: 365.7851\n",
            "\n",
            "Epoch 444: Validation loss decreased (365.785126 --> 364.990326).\n",
            "\t Train_Loss: 141.9632 Val_Loss: 364.9903  BEST VAL Loss: 364.9903\n",
            "\n",
            "Epoch 445: Validation loss decreased (364.990326 --> 364.206177).\n",
            "\t Train_Loss: 141.8711 Val_Loss: 364.2062  BEST VAL Loss: 364.2062\n",
            "\n",
            "Epoch 446: Validation loss decreased (364.206177 --> 363.431641).\n",
            "\t Train_Loss: 141.7814 Val_Loss: 363.4316  BEST VAL Loss: 363.4316\n",
            "\n",
            "Epoch 447: Validation loss decreased (363.431641 --> 362.667206).\n",
            "\t Train_Loss: 141.6938 Val_Loss: 362.6672  BEST VAL Loss: 362.6672\n",
            "\n",
            "Epoch 448: Validation loss decreased (362.667206 --> 361.912720).\n",
            "\t Train_Loss: 141.6086 Val_Loss: 361.9127  BEST VAL Loss: 361.9127\n",
            "\n",
            "Epoch 449: Validation loss decreased (361.912720 --> 361.168182).\n",
            "\t Train_Loss: 141.5253 Val_Loss: 361.1682  BEST VAL Loss: 361.1682\n",
            "\n",
            "Epoch 450: Validation loss decreased (361.168182 --> 360.433685).\n",
            "\t Train_Loss: 141.4440 Val_Loss: 360.4337  BEST VAL Loss: 360.4337\n",
            "\n",
            "Epoch 451: Validation loss decreased (360.433685 --> 359.707886).\n",
            "\t Train_Loss: 141.3647 Val_Loss: 359.7079  BEST VAL Loss: 359.7079\n",
            "\n",
            "Epoch 452: Validation loss decreased (359.707886 --> 358.991882).\n",
            "\t Train_Loss: 141.2874 Val_Loss: 358.9919  BEST VAL Loss: 358.9919\n",
            "\n",
            "Epoch 453: Validation loss decreased (358.991882 --> 358.285156).\n",
            "\t Train_Loss: 141.2120 Val_Loss: 358.2852  BEST VAL Loss: 358.2852\n",
            "\n",
            "Epoch 454: Validation loss decreased (358.285156 --> 357.587708).\n",
            "\t Train_Loss: 141.1384 Val_Loss: 357.5877  BEST VAL Loss: 357.5877\n",
            "\n",
            "Epoch 455: Validation loss decreased (357.587708 --> 356.899414).\n",
            "\t Train_Loss: 141.0668 Val_Loss: 356.8994  BEST VAL Loss: 356.8994\n",
            "\n",
            "Epoch 456: Validation loss decreased (356.899414 --> 356.219604).\n",
            "\t Train_Loss: 140.9968 Val_Loss: 356.2196  BEST VAL Loss: 356.2196\n",
            "\n",
            "Epoch 457: Validation loss decreased (356.219604 --> 355.548950).\n",
            "\t Train_Loss: 140.9286 Val_Loss: 355.5490  BEST VAL Loss: 355.5490\n",
            "\n",
            "Epoch 458: Validation loss decreased (355.548950 --> 354.887238).\n",
            "\t Train_Loss: 140.8621 Val_Loss: 354.8872  BEST VAL Loss: 354.8872\n",
            "\n",
            "Epoch 459: Validation loss decreased (354.887238 --> 354.233978).\n",
            "\t Train_Loss: 140.7971 Val_Loss: 354.2340  BEST VAL Loss: 354.2340\n",
            "\n",
            "Epoch 460: Validation loss decreased (354.233978 --> 353.589203).\n",
            "\t Train_Loss: 140.7339 Val_Loss: 353.5892  BEST VAL Loss: 353.5892\n",
            "\n",
            "Epoch 461: Validation loss decreased (353.589203 --> 352.953339).\n",
            "\t Train_Loss: 140.6722 Val_Loss: 352.9533  BEST VAL Loss: 352.9533\n",
            "\n",
            "Epoch 462: Validation loss decreased (352.953339 --> 352.325073).\n",
            "\t Train_Loss: 140.6121 Val_Loss: 352.3251  BEST VAL Loss: 352.3251\n",
            "\n",
            "Epoch 463: Validation loss decreased (352.325073 --> 351.705780).\n",
            "\t Train_Loss: 140.5535 Val_Loss: 351.7058  BEST VAL Loss: 351.7058\n",
            "\n",
            "Epoch 464: Validation loss decreased (351.705780 --> 351.093750).\n",
            "\t Train_Loss: 140.4963 Val_Loss: 351.0938  BEST VAL Loss: 351.0938\n",
            "\n",
            "Epoch 465: Validation loss decreased (351.093750 --> 350.490570).\n",
            "\t Train_Loss: 140.4407 Val_Loss: 350.4906  BEST VAL Loss: 350.4906\n",
            "\n",
            "Epoch 466: Validation loss decreased (350.490570 --> 349.894653).\n",
            "\t Train_Loss: 140.3863 Val_Loss: 349.8947  BEST VAL Loss: 349.8947\n",
            "\n",
            "Epoch 467: Validation loss decreased (349.894653 --> 349.307343).\n",
            "\t Train_Loss: 140.3334 Val_Loss: 349.3073  BEST VAL Loss: 349.3073\n",
            "\n",
            "Epoch 468: Validation loss decreased (349.307343 --> 348.726929).\n",
            "\t Train_Loss: 140.2818 Val_Loss: 348.7269  BEST VAL Loss: 348.7269\n",
            "\n",
            "Epoch 469: Validation loss decreased (348.726929 --> 348.154388).\n",
            "\t Train_Loss: 140.2315 Val_Loss: 348.1544  BEST VAL Loss: 348.1544\n",
            "\n",
            "Epoch 470: Validation loss decreased (348.154388 --> 347.589661).\n",
            "\t Train_Loss: 140.1824 Val_Loss: 347.5897  BEST VAL Loss: 347.5897\n",
            "\n",
            "Epoch 471: Validation loss decreased (347.589661 --> 347.031982).\n",
            "\t Train_Loss: 140.1347 Val_Loss: 347.0320  BEST VAL Loss: 347.0320\n",
            "\n",
            "Epoch 472: Validation loss decreased (347.031982 --> 346.481750).\n",
            "\t Train_Loss: 140.0881 Val_Loss: 346.4818  BEST VAL Loss: 346.4818\n",
            "\n",
            "Epoch 473: Validation loss decreased (346.481750 --> 345.938568).\n",
            "\t Train_Loss: 140.0428 Val_Loss: 345.9386  BEST VAL Loss: 345.9386\n",
            "\n",
            "Epoch 474: Validation loss decreased (345.938568 --> 345.403198).\n",
            "\t Train_Loss: 139.9986 Val_Loss: 345.4032  BEST VAL Loss: 345.4032\n",
            "\n",
            "Epoch 475: Validation loss decreased (345.403198 --> 344.874847).\n",
            "\t Train_Loss: 139.9555 Val_Loss: 344.8748  BEST VAL Loss: 344.8748\n",
            "\n",
            "Epoch 476: Validation loss decreased (344.874847 --> 344.353241).\n",
            "\t Train_Loss: 139.9135 Val_Loss: 344.3532  BEST VAL Loss: 344.3532\n",
            "\n",
            "Epoch 477: Validation loss decreased (344.353241 --> 343.838104).\n",
            "\t Train_Loss: 139.8726 Val_Loss: 343.8381  BEST VAL Loss: 343.8381\n",
            "\n",
            "Epoch 478: Validation loss decreased (343.838104 --> 343.330627).\n",
            "\t Train_Loss: 139.8329 Val_Loss: 343.3306  BEST VAL Loss: 343.3306\n",
            "\n",
            "Epoch 479: Validation loss decreased (343.330627 --> 342.829071).\n",
            "\t Train_Loss: 139.7939 Val_Loss: 342.8291  BEST VAL Loss: 342.8291\n",
            "\n",
            "Epoch 480: Validation loss decreased (342.829071 --> 342.334930).\n",
            "\t Train_Loss: 139.7561 Val_Loss: 342.3349  BEST VAL Loss: 342.3349\n",
            "\n",
            "Epoch 481: Validation loss decreased (342.334930 --> 341.847168).\n",
            "\t Train_Loss: 139.7193 Val_Loss: 341.8472  BEST VAL Loss: 341.8472\n",
            "\n",
            "Epoch 482: Validation loss decreased (341.847168 --> 341.365723).\n",
            "\t Train_Loss: 139.6834 Val_Loss: 341.3657  BEST VAL Loss: 341.3657\n",
            "\n",
            "Epoch 483: Validation loss decreased (341.365723 --> 340.890533).\n",
            "\t Train_Loss: 139.6485 Val_Loss: 340.8905  BEST VAL Loss: 340.8905\n",
            "\n",
            "Epoch 484: Validation loss decreased (340.890533 --> 340.421753).\n",
            "\t Train_Loss: 139.6145 Val_Loss: 340.4218  BEST VAL Loss: 340.4218\n",
            "\n",
            "Epoch 485: Validation loss decreased (340.421753 --> 339.959045).\n",
            "\t Train_Loss: 139.5813 Val_Loss: 339.9590  BEST VAL Loss: 339.9590\n",
            "\n",
            "Epoch 486: Validation loss decreased (339.959045 --> 339.503021).\n",
            "\t Train_Loss: 139.5490 Val_Loss: 339.5030  BEST VAL Loss: 339.5030\n",
            "\n",
            "Epoch 487: Validation loss decreased (339.503021 --> 339.053070).\n",
            "\t Train_Loss: 139.5175 Val_Loss: 339.0531  BEST VAL Loss: 339.0531\n",
            "\n",
            "Epoch 488: Validation loss decreased (339.053070 --> 338.608643).\n",
            "\t Train_Loss: 139.4869 Val_Loss: 338.6086  BEST VAL Loss: 338.6086\n",
            "\n",
            "Epoch 489: Validation loss decreased (338.608643 --> 338.170563).\n",
            "\t Train_Loss: 139.4571 Val_Loss: 338.1706  BEST VAL Loss: 338.1706\n",
            "\n",
            "Epoch 490: Validation loss decreased (338.170563 --> 337.737976).\n",
            "\t Train_Loss: 139.4281 Val_Loss: 337.7380  BEST VAL Loss: 337.7380\n",
            "\n",
            "Epoch 491: Validation loss decreased (337.737976 --> 337.311401).\n",
            "\t Train_Loss: 139.3998 Val_Loss: 337.3114  BEST VAL Loss: 337.3114\n",
            "\n",
            "Epoch 492: Validation loss decreased (337.311401 --> 336.890839).\n",
            "\t Train_Loss: 139.3722 Val_Loss: 336.8908  BEST VAL Loss: 336.8908\n",
            "\n",
            "Epoch 493: Validation loss decreased (336.890839 --> 336.475555).\n",
            "\t Train_Loss: 139.3454 Val_Loss: 336.4756  BEST VAL Loss: 336.4756\n",
            "\n",
            "Epoch 494: Validation loss decreased (336.475555 --> 336.065887).\n",
            "\t Train_Loss: 139.3193 Val_Loss: 336.0659  BEST VAL Loss: 336.0659\n",
            "\n",
            "Epoch 495: Validation loss decreased (336.065887 --> 335.661835).\n",
            "\t Train_Loss: 139.2940 Val_Loss: 335.6618  BEST VAL Loss: 335.6618\n",
            "\n",
            "Epoch 496: Validation loss decreased (335.661835 --> 335.262878).\n",
            "\t Train_Loss: 139.2692 Val_Loss: 335.2629  BEST VAL Loss: 335.2629\n",
            "\n",
            "Epoch 497: Validation loss decreased (335.262878 --> 334.869690).\n",
            "\t Train_Loss: 139.2451 Val_Loss: 334.8697  BEST VAL Loss: 334.8697\n",
            "\n",
            "Epoch 498: Validation loss decreased (334.869690 --> 334.481812).\n",
            "\t Train_Loss: 139.2217 Val_Loss: 334.4818  BEST VAL Loss: 334.4818\n",
            "\n",
            "Epoch 499: Validation loss decreased (334.481812 --> 334.099091).\n",
            "\t Train_Loss: 139.1989 Val_Loss: 334.0991  BEST VAL Loss: 334.0991\n",
            "\n",
            "Epoch 500: Validation loss decreased (334.099091 --> 333.721924).\n",
            "\t Train_Loss: 139.1767 Val_Loss: 333.7219  BEST VAL Loss: 333.7219\n",
            "\n",
            "Epoch 501: Validation loss decreased (333.721924 --> 333.348999).\n",
            "\t Train_Loss: 139.1550 Val_Loss: 333.3490  BEST VAL Loss: 333.3490\n",
            "\n",
            "Epoch 502: Validation loss decreased (333.348999 --> 332.981750).\n",
            "\t Train_Loss: 139.1340 Val_Loss: 332.9818  BEST VAL Loss: 332.9818\n",
            "\n",
            "Epoch 503: Validation loss decreased (332.981750 --> 332.619781).\n",
            "\t Train_Loss: 139.1135 Val_Loss: 332.6198  BEST VAL Loss: 332.6198\n",
            "\n",
            "Epoch 504: Validation loss decreased (332.619781 --> 332.262054).\n",
            "\t Train_Loss: 139.0936 Val_Loss: 332.2621  BEST VAL Loss: 332.2621\n",
            "\n",
            "Epoch 505: Validation loss decreased (332.262054 --> 331.909027).\n",
            "\t Train_Loss: 139.0742 Val_Loss: 331.9090  BEST VAL Loss: 331.9090\n",
            "\n",
            "Epoch 506: Validation loss decreased (331.909027 --> 331.561554).\n",
            "\t Train_Loss: 139.0553 Val_Loss: 331.5616  BEST VAL Loss: 331.5616\n",
            "\n",
            "Epoch 507: Validation loss decreased (331.561554 --> 331.218475).\n",
            "\t Train_Loss: 139.0370 Val_Loss: 331.2185  BEST VAL Loss: 331.2185\n",
            "\n",
            "Epoch 508: Validation loss decreased (331.218475 --> 330.880035).\n",
            "\t Train_Loss: 139.0191 Val_Loss: 330.8800  BEST VAL Loss: 330.8800\n",
            "\n",
            "Epoch 509: Validation loss decreased (330.880035 --> 330.546478).\n",
            "\t Train_Loss: 139.0018 Val_Loss: 330.5465  BEST VAL Loss: 330.5465\n",
            "\n",
            "Epoch 510: Validation loss decreased (330.546478 --> 330.216919).\n",
            "\t Train_Loss: 138.9848 Val_Loss: 330.2169  BEST VAL Loss: 330.2169\n",
            "\n",
            "Epoch 511: Validation loss decreased (330.216919 --> 329.891937).\n",
            "\t Train_Loss: 138.9684 Val_Loss: 329.8919  BEST VAL Loss: 329.8919\n",
            "\n",
            "Epoch 512: Validation loss decreased (329.891937 --> 329.571716).\n",
            "\t Train_Loss: 138.9524 Val_Loss: 329.5717  BEST VAL Loss: 329.5717\n",
            "\n",
            "Epoch 513: Validation loss decreased (329.571716 --> 329.256042).\n",
            "\t Train_Loss: 138.9369 Val_Loss: 329.2560  BEST VAL Loss: 329.2560\n",
            "\n",
            "Epoch 514: Validation loss decreased (329.256042 --> 328.944702).\n",
            "\t Train_Loss: 138.9217 Val_Loss: 328.9447  BEST VAL Loss: 328.9447\n",
            "\n",
            "Epoch 515: Validation loss decreased (328.944702 --> 328.636810).\n",
            "\t Train_Loss: 138.9070 Val_Loss: 328.6368  BEST VAL Loss: 328.6368\n",
            "\n",
            "Epoch 516: Validation loss decreased (328.636810 --> 328.334198).\n",
            "\t Train_Loss: 138.8926 Val_Loss: 328.3342  BEST VAL Loss: 328.3342\n",
            "\n",
            "Epoch 517: Validation loss decreased (328.334198 --> 328.034973).\n",
            "\t Train_Loss: 138.8787 Val_Loss: 328.0350  BEST VAL Loss: 328.0350\n",
            "\n",
            "Epoch 518: Validation loss decreased (328.034973 --> 327.740479).\n",
            "\t Train_Loss: 138.8652 Val_Loss: 327.7405  BEST VAL Loss: 327.7405\n",
            "\n",
            "Epoch 519: Validation loss decreased (327.740479 --> 327.449463).\n",
            "\t Train_Loss: 138.8521 Val_Loss: 327.4495  BEST VAL Loss: 327.4495\n",
            "\n",
            "Epoch 520: Validation loss decreased (327.449463 --> 327.162903).\n",
            "\t Train_Loss: 138.8393 Val_Loss: 327.1629  BEST VAL Loss: 327.1629\n",
            "\n",
            "Epoch 521: Validation loss decreased (327.162903 --> 326.880554).\n",
            "\t Train_Loss: 138.8268 Val_Loss: 326.8806  BEST VAL Loss: 326.8806\n",
            "\n",
            "Epoch 522: Validation loss decreased (326.880554 --> 326.601532).\n",
            "\t Train_Loss: 138.8147 Val_Loss: 326.6015  BEST VAL Loss: 326.6015\n",
            "\n",
            "Epoch 523: Validation loss decreased (326.601532 --> 326.326630).\n",
            "\t Train_Loss: 138.8029 Val_Loss: 326.3266  BEST VAL Loss: 326.3266\n",
            "\n",
            "Epoch 524: Validation loss decreased (326.326630 --> 326.055145).\n",
            "\t Train_Loss: 138.7914 Val_Loss: 326.0551  BEST VAL Loss: 326.0551\n",
            "\n",
            "Epoch 525: Validation loss decreased (326.055145 --> 325.788116).\n",
            "\t Train_Loss: 138.7803 Val_Loss: 325.7881  BEST VAL Loss: 325.7881\n",
            "\n",
            "Epoch 526: Validation loss decreased (325.788116 --> 325.524414).\n",
            "\t Train_Loss: 138.7695 Val_Loss: 325.5244  BEST VAL Loss: 325.5244\n",
            "\n",
            "Epoch 527: Validation loss decreased (325.524414 --> 325.264130).\n",
            "\t Train_Loss: 138.7589 Val_Loss: 325.2641  BEST VAL Loss: 325.2641\n",
            "\n",
            "Epoch 528: Validation loss decreased (325.264130 --> 325.007568).\n",
            "\t Train_Loss: 138.7487 Val_Loss: 325.0076  BEST VAL Loss: 325.0076\n",
            "\n",
            "Epoch 529: Validation loss decreased (325.007568 --> 324.755066).\n",
            "\t Train_Loss: 138.7388 Val_Loss: 324.7551  BEST VAL Loss: 324.7551\n",
            "\n",
            "Epoch 530: Validation loss decreased (324.755066 --> 324.506073).\n",
            "\t Train_Loss: 138.7292 Val_Loss: 324.5061  BEST VAL Loss: 324.5061\n",
            "\n",
            "Epoch 531: Validation loss decreased (324.506073 --> 324.259979).\n",
            "\t Train_Loss: 138.7198 Val_Loss: 324.2600  BEST VAL Loss: 324.2600\n",
            "\n",
            "Epoch 532: Validation loss decreased (324.259979 --> 324.017639).\n",
            "\t Train_Loss: 138.7106 Val_Loss: 324.0176  BEST VAL Loss: 324.0176\n",
            "\n",
            "Epoch 533: Validation loss decreased (324.017639 --> 323.778564).\n",
            "\t Train_Loss: 138.7017 Val_Loss: 323.7786  BEST VAL Loss: 323.7786\n",
            "\n",
            "Epoch 534: Validation loss decreased (323.778564 --> 323.543304).\n",
            "\t Train_Loss: 138.6931 Val_Loss: 323.5433  BEST VAL Loss: 323.5433\n",
            "\n",
            "Epoch 535: Validation loss decreased (323.543304 --> 323.310913).\n",
            "\t Train_Loss: 138.6847 Val_Loss: 323.3109  BEST VAL Loss: 323.3109\n",
            "\n",
            "Epoch 536: Validation loss decreased (323.310913 --> 323.081421).\n",
            "\t Train_Loss: 138.6766 Val_Loss: 323.0814  BEST VAL Loss: 323.0814\n",
            "\n",
            "Epoch 537: Validation loss decreased (323.081421 --> 322.856232).\n",
            "\t Train_Loss: 138.6687 Val_Loss: 322.8562  BEST VAL Loss: 322.8562\n",
            "\n",
            "Epoch 538: Validation loss decreased (322.856232 --> 322.633209).\n",
            "\t Train_Loss: 138.6610 Val_Loss: 322.6332  BEST VAL Loss: 322.6332\n",
            "\n",
            "Epoch 539: Validation loss decreased (322.633209 --> 322.413818).\n",
            "\t Train_Loss: 138.6535 Val_Loss: 322.4138  BEST VAL Loss: 322.4138\n",
            "\n",
            "Epoch 540: Validation loss decreased (322.413818 --> 322.197540).\n",
            "\t Train_Loss: 138.6463 Val_Loss: 322.1975  BEST VAL Loss: 322.1975\n",
            "\n",
            "Epoch 541: Validation loss decreased (322.197540 --> 321.983887).\n",
            "\t Train_Loss: 138.6392 Val_Loss: 321.9839  BEST VAL Loss: 321.9839\n",
            "\n",
            "Epoch 542: Validation loss decreased (321.983887 --> 321.773651).\n",
            "\t Train_Loss: 138.6323 Val_Loss: 321.7737  BEST VAL Loss: 321.7737\n",
            "\n",
            "Epoch 543: Validation loss decreased (321.773651 --> 321.566254).\n",
            "\t Train_Loss: 138.6257 Val_Loss: 321.5663  BEST VAL Loss: 321.5663\n",
            "\n",
            "Epoch 544: Validation loss decreased (321.566254 --> 321.361877).\n",
            "\t Train_Loss: 138.6192 Val_Loss: 321.3619  BEST VAL Loss: 321.3619\n",
            "\n",
            "Epoch 545: Validation loss decreased (321.361877 --> 321.160553).\n",
            "\t Train_Loss: 138.6130 Val_Loss: 321.1606  BEST VAL Loss: 321.1606\n",
            "\n",
            "Epoch 546: Validation loss decreased (321.160553 --> 320.961884).\n",
            "\t Train_Loss: 138.6069 Val_Loss: 320.9619  BEST VAL Loss: 320.9619\n",
            "\n",
            "Epoch 547: Validation loss decreased (320.961884 --> 320.765961).\n",
            "\t Train_Loss: 138.6010 Val_Loss: 320.7660  BEST VAL Loss: 320.7660\n",
            "\n",
            "Epoch 548: Validation loss decreased (320.765961 --> 320.573059).\n",
            "\t Train_Loss: 138.5952 Val_Loss: 320.5731  BEST VAL Loss: 320.5731\n",
            "\n",
            "Epoch 549: Validation loss decreased (320.573059 --> 320.382782).\n",
            "\t Train_Loss: 138.5896 Val_Loss: 320.3828  BEST VAL Loss: 320.3828\n",
            "\n",
            "Epoch 550: Validation loss decreased (320.382782 --> 320.194977).\n",
            "\t Train_Loss: 138.5842 Val_Loss: 320.1950  BEST VAL Loss: 320.1950\n",
            "\n",
            "Epoch 551: Validation loss decreased (320.194977 --> 320.010468).\n",
            "\t Train_Loss: 138.5789 Val_Loss: 320.0105  BEST VAL Loss: 320.0105\n",
            "\n",
            "Epoch 552: Validation loss decreased (320.010468 --> 319.828186).\n",
            "\t Train_Loss: 138.5738 Val_Loss: 319.8282  BEST VAL Loss: 319.8282\n",
            "\n",
            "Epoch 553: Validation loss decreased (319.828186 --> 319.648834).\n",
            "\t Train_Loss: 138.5688 Val_Loss: 319.6488  BEST VAL Loss: 319.6488\n",
            "\n",
            "Epoch 554: Validation loss decreased (319.648834 --> 319.471344).\n",
            "\t Train_Loss: 138.5640 Val_Loss: 319.4713  BEST VAL Loss: 319.4713\n",
            "\n",
            "Epoch 555: Validation loss decreased (319.471344 --> 319.297119).\n",
            "\t Train_Loss: 138.5593 Val_Loss: 319.2971  BEST VAL Loss: 319.2971\n",
            "\n",
            "Epoch 556: Validation loss decreased (319.297119 --> 319.124969).\n",
            "\t Train_Loss: 138.5548 Val_Loss: 319.1250  BEST VAL Loss: 319.1250\n",
            "\n",
            "Epoch 557: Validation loss decreased (319.124969 --> 318.955566).\n",
            "\t Train_Loss: 138.5504 Val_Loss: 318.9556  BEST VAL Loss: 318.9556\n",
            "\n",
            "Epoch 558: Validation loss decreased (318.955566 --> 318.788696).\n",
            "\t Train_Loss: 138.5461 Val_Loss: 318.7887  BEST VAL Loss: 318.7887\n",
            "\n",
            "Epoch 559: Validation loss decreased (318.788696 --> 318.624207).\n",
            "\t Train_Loss: 138.5420 Val_Loss: 318.6242  BEST VAL Loss: 318.6242\n",
            "\n",
            "Epoch 560: Validation loss decreased (318.624207 --> 318.461395).\n",
            "\t Train_Loss: 138.5379 Val_Loss: 318.4614  BEST VAL Loss: 318.4614\n",
            "\n",
            "Epoch 561: Validation loss decreased (318.461395 --> 318.301666).\n",
            "\t Train_Loss: 138.5340 Val_Loss: 318.3017  BEST VAL Loss: 318.3017\n",
            "\n",
            "Epoch 562: Validation loss decreased (318.301666 --> 318.143982).\n",
            "\t Train_Loss: 138.5302 Val_Loss: 318.1440  BEST VAL Loss: 318.1440\n",
            "\n",
            "Epoch 563: Validation loss decreased (318.143982 --> 317.989075).\n",
            "\t Train_Loss: 138.5265 Val_Loss: 317.9891  BEST VAL Loss: 317.9891\n",
            "\n",
            "Epoch 564: Validation loss decreased (317.989075 --> 317.835571).\n",
            "\t Train_Loss: 138.5229 Val_Loss: 317.8356  BEST VAL Loss: 317.8356\n",
            "\n",
            "Epoch 565: Validation loss decreased (317.835571 --> 317.685181).\n",
            "\t Train_Loss: 138.5194 Val_Loss: 317.6852  BEST VAL Loss: 317.6852\n",
            "\n",
            "Epoch 566: Validation loss decreased (317.685181 --> 317.536438).\n",
            "\t Train_Loss: 138.5160 Val_Loss: 317.5364  BEST VAL Loss: 317.5364\n",
            "\n",
            "Epoch 567: Validation loss decreased (317.536438 --> 317.389862).\n",
            "\t Train_Loss: 138.5128 Val_Loss: 317.3899  BEST VAL Loss: 317.3899\n",
            "\n",
            "Epoch 568: Validation loss decreased (317.389862 --> 317.245850).\n",
            "\t Train_Loss: 138.5096 Val_Loss: 317.2458  BEST VAL Loss: 317.2458\n",
            "\n",
            "Epoch 569: Validation loss decreased (317.245850 --> 317.103424).\n",
            "\t Train_Loss: 138.5065 Val_Loss: 317.1034  BEST VAL Loss: 317.1034\n",
            "\n",
            "Epoch 570: Validation loss decreased (317.103424 --> 316.963165).\n",
            "\t Train_Loss: 138.5035 Val_Loss: 316.9632  BEST VAL Loss: 316.9632\n",
            "\n",
            "Epoch 571: Validation loss decreased (316.963165 --> 316.825409).\n",
            "\t Train_Loss: 138.5006 Val_Loss: 316.8254  BEST VAL Loss: 316.8254\n",
            "\n",
            "Epoch 572: Validation loss decreased (316.825409 --> 316.689209).\n",
            "\t Train_Loss: 138.4978 Val_Loss: 316.6892  BEST VAL Loss: 316.6892\n",
            "\n",
            "Epoch 573: Validation loss decreased (316.689209 --> 316.555267).\n",
            "\t Train_Loss: 138.4950 Val_Loss: 316.5553  BEST VAL Loss: 316.5553\n",
            "\n",
            "Epoch 574: Validation loss decreased (316.555267 --> 316.423187).\n",
            "\t Train_Loss: 138.4924 Val_Loss: 316.4232  BEST VAL Loss: 316.4232\n",
            "\n",
            "Epoch 575: Validation loss decreased (316.423187 --> 316.292969).\n",
            "\t Train_Loss: 138.4899 Val_Loss: 316.2930  BEST VAL Loss: 316.2930\n",
            "\n",
            "Epoch 576: Validation loss decreased (316.292969 --> 316.164886).\n",
            "\t Train_Loss: 138.4873 Val_Loss: 316.1649  BEST VAL Loss: 316.1649\n",
            "\n",
            "Epoch 577: Validation loss decreased (316.164886 --> 316.038544).\n",
            "\t Train_Loss: 138.4850 Val_Loss: 316.0385  BEST VAL Loss: 316.0385\n",
            "\n",
            "Epoch 578: Validation loss decreased (316.038544 --> 315.913910).\n",
            "\t Train_Loss: 138.4826 Val_Loss: 315.9139  BEST VAL Loss: 315.9139\n",
            "\n",
            "Epoch 579: Validation loss decreased (315.913910 --> 315.791595).\n",
            "\t Train_Loss: 138.4803 Val_Loss: 315.7916  BEST VAL Loss: 315.7916\n",
            "\n",
            "Epoch 580: Validation loss decreased (315.791595 --> 315.670868).\n",
            "\t Train_Loss: 138.4781 Val_Loss: 315.6709  BEST VAL Loss: 315.6709\n",
            "\n",
            "Epoch 581: Validation loss decreased (315.670868 --> 315.551971).\n",
            "\t Train_Loss: 138.4760 Val_Loss: 315.5520  BEST VAL Loss: 315.5520\n",
            "\n",
            "Epoch 582: Validation loss decreased (315.551971 --> 315.434601).\n",
            "\t Train_Loss: 138.4738 Val_Loss: 315.4346  BEST VAL Loss: 315.4346\n",
            "\n",
            "Epoch 583: Validation loss decreased (315.434601 --> 315.318848).\n",
            "\t Train_Loss: 138.4719 Val_Loss: 315.3188  BEST VAL Loss: 315.3188\n",
            "\n",
            "Epoch 584: Validation loss decreased (315.318848 --> 315.205566).\n",
            "\t Train_Loss: 138.4699 Val_Loss: 315.2056  BEST VAL Loss: 315.2056\n",
            "\n",
            "Epoch 585: Validation loss decreased (315.205566 --> 315.093445).\n",
            "\t Train_Loss: 138.4680 Val_Loss: 315.0934  BEST VAL Loss: 315.0934\n",
            "\n",
            "Epoch 586: Validation loss decreased (315.093445 --> 314.983368).\n",
            "\t Train_Loss: 138.4662 Val_Loss: 314.9834  BEST VAL Loss: 314.9834\n",
            "\n",
            "Epoch 587: Validation loss decreased (314.983368 --> 314.874359).\n",
            "\t Train_Loss: 138.4645 Val_Loss: 314.8744  BEST VAL Loss: 314.8744\n",
            "\n",
            "Epoch 588: Validation loss decreased (314.874359 --> 314.767426).\n",
            "\t Train_Loss: 138.4627 Val_Loss: 314.7674  BEST VAL Loss: 314.7674\n",
            "\n",
            "Epoch 589: Validation loss decreased (314.767426 --> 314.662048).\n",
            "\t Train_Loss: 138.4610 Val_Loss: 314.6620  BEST VAL Loss: 314.6620\n",
            "\n",
            "Epoch 590: Validation loss decreased (314.662048 --> 314.558258).\n",
            "\t Train_Loss: 138.4594 Val_Loss: 314.5583  BEST VAL Loss: 314.5583\n",
            "\n",
            "Epoch 591: Validation loss decreased (314.558258 --> 314.455872).\n",
            "\t Train_Loss: 138.4578 Val_Loss: 314.4559  BEST VAL Loss: 314.4559\n",
            "\n",
            "Epoch 592: Validation loss decreased (314.455872 --> 314.354980).\n",
            "\t Train_Loss: 138.4563 Val_Loss: 314.3550  BEST VAL Loss: 314.3550\n",
            "\n",
            "Epoch 593: Validation loss decreased (314.354980 --> 314.255707).\n",
            "\t Train_Loss: 138.4548 Val_Loss: 314.2557  BEST VAL Loss: 314.2557\n",
            "\n",
            "Epoch 594: Validation loss decreased (314.255707 --> 314.158234).\n",
            "\t Train_Loss: 138.4534 Val_Loss: 314.1582  BEST VAL Loss: 314.1582\n",
            "\n",
            "Epoch 595: Validation loss decreased (314.158234 --> 314.061462).\n",
            "\t Train_Loss: 138.4520 Val_Loss: 314.0615  BEST VAL Loss: 314.0615\n",
            "\n",
            "Epoch 596: Validation loss decreased (314.061462 --> 313.966766).\n",
            "\t Train_Loss: 138.4507 Val_Loss: 313.9668  BEST VAL Loss: 313.9668\n",
            "\n",
            "Epoch 597: Validation loss decreased (313.966766 --> 313.873444).\n",
            "\t Train_Loss: 138.4494 Val_Loss: 313.8734  BEST VAL Loss: 313.8734\n",
            "\n",
            "Epoch 598: Validation loss decreased (313.873444 --> 313.781372).\n",
            "\t Train_Loss: 138.4481 Val_Loss: 313.7814  BEST VAL Loss: 313.7814\n",
            "\n",
            "Epoch 599: Validation loss decreased (313.781372 --> 313.691071).\n",
            "\t Train_Loss: 138.4469 Val_Loss: 313.6911  BEST VAL Loss: 313.6911\n",
            "\n",
            "Epoch 600: Validation loss decreased (313.691071 --> 313.601685).\n",
            "\t Train_Loss: 138.4458 Val_Loss: 313.6017  BEST VAL Loss: 313.6017\n",
            "\n",
            "Epoch 601: Validation loss decreased (313.601685 --> 313.514130).\n",
            "\t Train_Loss: 138.4446 Val_Loss: 313.5141  BEST VAL Loss: 313.5141\n",
            "\n",
            "Epoch 602: Validation loss decreased (313.514130 --> 313.427582).\n",
            "\t Train_Loss: 138.4434 Val_Loss: 313.4276  BEST VAL Loss: 313.4276\n",
            "\n",
            "Epoch 603: Validation loss decreased (313.427582 --> 313.342255).\n",
            "\t Train_Loss: 138.4424 Val_Loss: 313.3423  BEST VAL Loss: 313.3423\n",
            "\n",
            "Epoch 604: Validation loss decreased (313.342255 --> 313.258484).\n",
            "\t Train_Loss: 138.4413 Val_Loss: 313.2585  BEST VAL Loss: 313.2585\n",
            "\n",
            "Epoch 605: Validation loss decreased (313.258484 --> 313.176025).\n",
            "\t Train_Loss: 138.4403 Val_Loss: 313.1760  BEST VAL Loss: 313.1760\n",
            "\n",
            "Epoch 606: Validation loss decreased (313.176025 --> 313.094482).\n",
            "\t Train_Loss: 138.4394 Val_Loss: 313.0945  BEST VAL Loss: 313.0945\n",
            "\n",
            "Epoch 607: Validation loss decreased (313.094482 --> 313.014221).\n",
            "\t Train_Loss: 138.4385 Val_Loss: 313.0142  BEST VAL Loss: 313.0142\n",
            "\n",
            "Epoch 608: Validation loss decreased (313.014221 --> 312.935394).\n",
            "\t Train_Loss: 138.4375 Val_Loss: 312.9354  BEST VAL Loss: 312.9354\n",
            "\n",
            "Epoch 609: Validation loss decreased (312.935394 --> 312.858063).\n",
            "\t Train_Loss: 138.4366 Val_Loss: 312.8581  BEST VAL Loss: 312.8581\n",
            "\n",
            "Epoch 610: Validation loss decreased (312.858063 --> 312.781525).\n",
            "\t Train_Loss: 138.4357 Val_Loss: 312.7815  BEST VAL Loss: 312.7815\n",
            "\n",
            "Epoch 611: Validation loss decreased (312.781525 --> 312.706207).\n",
            "\t Train_Loss: 138.4349 Val_Loss: 312.7062  BEST VAL Loss: 312.7062\n",
            "\n",
            "Epoch 612: Validation loss decreased (312.706207 --> 312.632263).\n",
            "\t Train_Loss: 138.4341 Val_Loss: 312.6323  BEST VAL Loss: 312.6323\n",
            "\n",
            "Epoch 613: Validation loss decreased (312.632263 --> 312.559204).\n",
            "\t Train_Loss: 138.4333 Val_Loss: 312.5592  BEST VAL Loss: 312.5592\n",
            "\n",
            "Epoch 614: Validation loss decreased (312.559204 --> 312.487518).\n",
            "\t Train_Loss: 138.4326 Val_Loss: 312.4875  BEST VAL Loss: 312.4875\n",
            "\n",
            "Epoch 615: Validation loss decreased (312.487518 --> 312.416687).\n",
            "\t Train_Loss: 138.4318 Val_Loss: 312.4167  BEST VAL Loss: 312.4167\n",
            "\n",
            "Epoch 616: Validation loss decreased (312.416687 --> 312.347168).\n",
            "\t Train_Loss: 138.4311 Val_Loss: 312.3472  BEST VAL Loss: 312.3472\n",
            "\n",
            "Epoch 617: Validation loss decreased (312.347168 --> 312.279022).\n",
            "\t Train_Loss: 138.4304 Val_Loss: 312.2790  BEST VAL Loss: 312.2790\n",
            "\n",
            "Epoch 618: Validation loss decreased (312.279022 --> 312.211639).\n",
            "\t Train_Loss: 138.4297 Val_Loss: 312.2116  BEST VAL Loss: 312.2116\n",
            "\n",
            "Epoch 619: Validation loss decreased (312.211639 --> 312.144806).\n",
            "\t Train_Loss: 138.4291 Val_Loss: 312.1448  BEST VAL Loss: 312.1448\n",
            "\n",
            "Epoch 620: Validation loss decreased (312.144806 --> 312.079620).\n",
            "\t Train_Loss: 138.4285 Val_Loss: 312.0796  BEST VAL Loss: 312.0796\n",
            "\n",
            "Epoch 621: Validation loss decreased (312.079620 --> 312.014954).\n",
            "\t Train_Loss: 138.4279 Val_Loss: 312.0150  BEST VAL Loss: 312.0150\n",
            "\n",
            "Epoch 622: Validation loss decreased (312.014954 --> 311.951904).\n",
            "\t Train_Loss: 138.4273 Val_Loss: 311.9519  BEST VAL Loss: 311.9519\n",
            "\n",
            "Epoch 623: Validation loss decreased (311.951904 --> 311.889740).\n",
            "\t Train_Loss: 138.4267 Val_Loss: 311.8897  BEST VAL Loss: 311.8897\n",
            "\n",
            "Epoch 624: Validation loss decreased (311.889740 --> 311.828033).\n",
            "\t Train_Loss: 138.4262 Val_Loss: 311.8280  BEST VAL Loss: 311.8280\n",
            "\n",
            "Epoch 625: Validation loss decreased (311.828033 --> 311.767487).\n",
            "\t Train_Loss: 138.4257 Val_Loss: 311.7675  BEST VAL Loss: 311.7675\n",
            "\n",
            "Epoch 626: Validation loss decreased (311.767487 --> 311.708466).\n",
            "\t Train_Loss: 138.4252 Val_Loss: 311.7085  BEST VAL Loss: 311.7085\n",
            "\n",
            "Epoch 627: Validation loss decreased (311.708466 --> 311.650116).\n",
            "\t Train_Loss: 138.4247 Val_Loss: 311.6501  BEST VAL Loss: 311.6501\n",
            "\n",
            "Epoch 628: Validation loss decreased (311.650116 --> 311.592407).\n",
            "\t Train_Loss: 138.4242 Val_Loss: 311.5924  BEST VAL Loss: 311.5924\n",
            "\n",
            "Epoch 629: Validation loss decreased (311.592407 --> 311.535828).\n",
            "\t Train_Loss: 138.4237 Val_Loss: 311.5358  BEST VAL Loss: 311.5358\n",
            "\n",
            "Epoch 630: Validation loss decreased (311.535828 --> 311.480133).\n",
            "\t Train_Loss: 138.4232 Val_Loss: 311.4801  BEST VAL Loss: 311.4801\n",
            "\n",
            "Epoch 631: Validation loss decreased (311.480133 --> 311.425110).\n",
            "\t Train_Loss: 138.4228 Val_Loss: 311.4251  BEST VAL Loss: 311.4251\n",
            "\n",
            "Epoch 632: Validation loss decreased (311.425110 --> 311.370728).\n",
            "\t Train_Loss: 138.4224 Val_Loss: 311.3707  BEST VAL Loss: 311.3707\n",
            "\n",
            "Epoch 633: Validation loss decreased (311.370728 --> 311.317932).\n",
            "\t Train_Loss: 138.4220 Val_Loss: 311.3179  BEST VAL Loss: 311.3179\n",
            "\n",
            "Epoch 634: Validation loss decreased (311.317932 --> 311.265594).\n",
            "\t Train_Loss: 138.4216 Val_Loss: 311.2656  BEST VAL Loss: 311.2656\n",
            "\n",
            "Epoch 635: Validation loss decreased (311.265594 --> 311.214081).\n",
            "\t Train_Loss: 138.4212 Val_Loss: 311.2141  BEST VAL Loss: 311.2141\n",
            "\n",
            "Epoch 636: Validation loss decreased (311.214081 --> 311.163544).\n",
            "\t Train_Loss: 138.4208 Val_Loss: 311.1635  BEST VAL Loss: 311.1635\n",
            "\n",
            "Epoch 637: Validation loss decreased (311.163544 --> 311.113373).\n",
            "\t Train_Loss: 138.4205 Val_Loss: 311.1134  BEST VAL Loss: 311.1134\n",
            "\n",
            "Epoch 638: Validation loss decreased (311.113373 --> 311.064545).\n",
            "\t Train_Loss: 138.4201 Val_Loss: 311.0645  BEST VAL Loss: 311.0645\n",
            "\n",
            "Epoch 639: Validation loss decreased (311.064545 --> 311.016388).\n",
            "\t Train_Loss: 138.4199 Val_Loss: 311.0164  BEST VAL Loss: 311.0164\n",
            "\n",
            "Epoch 640: Validation loss decreased (311.016388 --> 310.968903).\n",
            "\t Train_Loss: 138.4194 Val_Loss: 310.9689  BEST VAL Loss: 310.9689\n",
            "\n",
            "Epoch 641: Validation loss decreased (310.968903 --> 310.921936).\n",
            "\t Train_Loss: 138.4191 Val_Loss: 310.9219  BEST VAL Loss: 310.9219\n",
            "\n",
            "Epoch 642: Validation loss decreased (310.921936 --> 310.876373).\n",
            "\t Train_Loss: 138.4189 Val_Loss: 310.8764  BEST VAL Loss: 310.8764\n",
            "\n",
            "Epoch 643: Validation loss decreased (310.876373 --> 310.830872).\n",
            "\t Train_Loss: 138.4186 Val_Loss: 310.8309  BEST VAL Loss: 310.8309\n",
            "\n",
            "Epoch 644: Validation loss decreased (310.830872 --> 310.786224).\n",
            "\t Train_Loss: 138.4183 Val_Loss: 310.7862  BEST VAL Loss: 310.7862\n",
            "\n",
            "Epoch 645: Validation loss decreased (310.786224 --> 310.742523).\n",
            "\t Train_Loss: 138.4180 Val_Loss: 310.7425  BEST VAL Loss: 310.7425\n",
            "\n",
            "Epoch 646: Validation loss decreased (310.742523 --> 310.699463).\n",
            "\t Train_Loss: 138.4177 Val_Loss: 310.6995  BEST VAL Loss: 310.6995\n",
            "\n",
            "Epoch 647: Validation loss decreased (310.699463 --> 310.657074).\n",
            "\t Train_Loss: 138.4175 Val_Loss: 310.6571  BEST VAL Loss: 310.6571\n",
            "\n",
            "Epoch 648: Validation loss decreased (310.657074 --> 310.615753).\n",
            "\t Train_Loss: 138.4172 Val_Loss: 310.6158  BEST VAL Loss: 310.6158\n",
            "\n",
            "Epoch 649: Validation loss decreased (310.615753 --> 310.574921).\n",
            "\t Train_Loss: 138.4170 Val_Loss: 310.5749  BEST VAL Loss: 310.5749\n",
            "\n",
            "Epoch 650: Validation loss decreased (310.574921 --> 310.534027).\n",
            "\t Train_Loss: 138.4168 Val_Loss: 310.5340  BEST VAL Loss: 310.5340\n",
            "\n",
            "Epoch 651: Validation loss decreased (310.534027 --> 310.494537).\n",
            "\t Train_Loss: 138.4165 Val_Loss: 310.4945  BEST VAL Loss: 310.4945\n",
            "\n",
            "Epoch 652: Validation loss decreased (310.494537 --> 310.455505).\n",
            "\t Train_Loss: 138.4163 Val_Loss: 310.4555  BEST VAL Loss: 310.4555\n",
            "\n",
            "Epoch 653: Validation loss decreased (310.455505 --> 310.417480).\n",
            "\t Train_Loss: 138.4161 Val_Loss: 310.4175  BEST VAL Loss: 310.4175\n",
            "\n",
            "Epoch 654: Validation loss decreased (310.417480 --> 310.379486).\n",
            "\t Train_Loss: 138.4159 Val_Loss: 310.3795  BEST VAL Loss: 310.3795\n",
            "\n",
            "Epoch 655: Validation loss decreased (310.379486 --> 310.342377).\n",
            "\t Train_Loss: 138.4158 Val_Loss: 310.3424  BEST VAL Loss: 310.3424\n",
            "\n",
            "Epoch 656: Validation loss decreased (310.342377 --> 310.306183).\n",
            "\t Train_Loss: 138.4156 Val_Loss: 310.3062  BEST VAL Loss: 310.3062\n",
            "\n",
            "Epoch 657: Validation loss decreased (310.306183 --> 310.269928).\n",
            "\t Train_Loss: 138.4154 Val_Loss: 310.2699  BEST VAL Loss: 310.2699\n",
            "\n",
            "Epoch 658: Validation loss decreased (310.269928 --> 310.234589).\n",
            "\t Train_Loss: 138.4152 Val_Loss: 310.2346  BEST VAL Loss: 310.2346\n",
            "\n",
            "Epoch 659: Validation loss decreased (310.234589 --> 310.199951).\n",
            "\t Train_Loss: 138.4149 Val_Loss: 310.2000  BEST VAL Loss: 310.2000\n",
            "\n",
            "Epoch 660: Validation loss decreased (310.199951 --> 310.165680).\n",
            "\t Train_Loss: 138.4148 Val_Loss: 310.1657  BEST VAL Loss: 310.1657\n",
            "\n",
            "Epoch 661: Validation loss decreased (310.165680 --> 310.132141).\n",
            "\t Train_Loss: 138.4147 Val_Loss: 310.1321  BEST VAL Loss: 310.1321\n",
            "\n",
            "Epoch 662: Validation loss decreased (310.132141 --> 310.099213).\n",
            "\t Train_Loss: 138.4146 Val_Loss: 310.0992  BEST VAL Loss: 310.0992\n",
            "\n",
            "Epoch 663: Validation loss decreased (310.099213 --> 310.066559).\n",
            "\t Train_Loss: 138.4144 Val_Loss: 310.0666  BEST VAL Loss: 310.0666\n",
            "\n",
            "Epoch 664: Validation loss decreased (310.066559 --> 310.034271).\n",
            "\t Train_Loss: 138.4142 Val_Loss: 310.0343  BEST VAL Loss: 310.0343\n",
            "\n",
            "Epoch 665: Validation loss decreased (310.034271 --> 310.003174).\n",
            "\t Train_Loss: 138.4142 Val_Loss: 310.0032  BEST VAL Loss: 310.0032\n",
            "\n",
            "Epoch 666: Validation loss decreased (310.003174 --> 309.972382).\n",
            "\t Train_Loss: 138.4140 Val_Loss: 309.9724  BEST VAL Loss: 309.9724\n",
            "\n",
            "Epoch 667: Validation loss decreased (309.972382 --> 309.941498).\n",
            "\t Train_Loss: 138.4138 Val_Loss: 309.9415  BEST VAL Loss: 309.9415\n",
            "\n",
            "Epoch 668: Validation loss decreased (309.941498 --> 309.912140).\n",
            "\t Train_Loss: 138.4137 Val_Loss: 309.9121  BEST VAL Loss: 309.9121\n",
            "\n",
            "Epoch 669: Validation loss decreased (309.912140 --> 309.882782).\n",
            "\t Train_Loss: 138.4136 Val_Loss: 309.8828  BEST VAL Loss: 309.8828\n",
            "\n",
            "Epoch 670: Validation loss decreased (309.882782 --> 309.853607).\n",
            "\t Train_Loss: 138.4135 Val_Loss: 309.8536  BEST VAL Loss: 309.8536\n",
            "\n",
            "Epoch 671: Validation loss decreased (309.853607 --> 309.825134).\n",
            "\t Train_Loss: 138.4133 Val_Loss: 309.8251  BEST VAL Loss: 309.8251\n",
            "\n",
            "Epoch 672: Validation loss decreased (309.825134 --> 309.797272).\n",
            "\t Train_Loss: 138.4133 Val_Loss: 309.7973  BEST VAL Loss: 309.7973\n",
            "\n",
            "Epoch 673: Validation loss decreased (309.797272 --> 309.769409).\n",
            "\t Train_Loss: 138.4131 Val_Loss: 309.7694  BEST VAL Loss: 309.7694\n",
            "\n",
            "Epoch 674: Validation loss decreased (309.769409 --> 309.742676).\n",
            "\t Train_Loss: 138.4131 Val_Loss: 309.7427  BEST VAL Loss: 309.7427\n",
            "\n",
            "Epoch 675: Validation loss decreased (309.742676 --> 309.716370).\n",
            "\t Train_Loss: 138.4130 Val_Loss: 309.7164  BEST VAL Loss: 309.7164\n",
            "\n",
            "Epoch 676: Validation loss decreased (309.716370 --> 309.690277).\n",
            "\t Train_Loss: 138.4129 Val_Loss: 309.6903  BEST VAL Loss: 309.6903\n",
            "\n",
            "Epoch 677: Validation loss decreased (309.690277 --> 309.664398).\n",
            "\t Train_Loss: 138.4128 Val_Loss: 309.6644  BEST VAL Loss: 309.6644\n",
            "\n",
            "Epoch 678: Validation loss decreased (309.664398 --> 309.638794).\n",
            "\t Train_Loss: 138.4127 Val_Loss: 309.6388  BEST VAL Loss: 309.6388\n",
            "\n",
            "Epoch 679: Validation loss decreased (309.638794 --> 309.614716).\n",
            "\t Train_Loss: 138.4126 Val_Loss: 309.6147  BEST VAL Loss: 309.6147\n",
            "\n",
            "Epoch 680: Validation loss decreased (309.614716 --> 309.589935).\n",
            "\t Train_Loss: 138.4126 Val_Loss: 309.5899  BEST VAL Loss: 309.5899\n",
            "\n",
            "Epoch 681: Validation loss decreased (309.589935 --> 309.566467).\n",
            "\t Train_Loss: 138.4125 Val_Loss: 309.5665  BEST VAL Loss: 309.5665\n",
            "\n",
            "Epoch 682: Validation loss decreased (309.566467 --> 309.542603).\n",
            "\t Train_Loss: 138.4124 Val_Loss: 309.5426  BEST VAL Loss: 309.5426\n",
            "\n",
            "Epoch 683: Validation loss decreased (309.542603 --> 309.519257).\n",
            "\t Train_Loss: 138.4124 Val_Loss: 309.5193  BEST VAL Loss: 309.5193\n",
            "\n",
            "Epoch 684: Validation loss decreased (309.519257 --> 309.496582).\n",
            "\t Train_Loss: 138.4122 Val_Loss: 309.4966  BEST VAL Loss: 309.4966\n",
            "\n",
            "Epoch 685: Validation loss decreased (309.496582 --> 309.474182).\n",
            "\t Train_Loss: 138.4122 Val_Loss: 309.4742  BEST VAL Loss: 309.4742\n",
            "\n",
            "Epoch 686: Validation loss decreased (309.474182 --> 309.451965).\n",
            "\t Train_Loss: 138.4121 Val_Loss: 309.4520  BEST VAL Loss: 309.4520\n",
            "\n",
            "Epoch 687: Validation loss decreased (309.451965 --> 309.430450).\n",
            "\t Train_Loss: 138.4120 Val_Loss: 309.4305  BEST VAL Loss: 309.4305\n",
            "\n",
            "Epoch 688: Validation loss decreased (309.430450 --> 309.409393).\n",
            "\t Train_Loss: 138.4120 Val_Loss: 309.4094  BEST VAL Loss: 309.4094\n",
            "\n",
            "Epoch 689: Validation loss decreased (309.409393 --> 309.388153).\n",
            "\t Train_Loss: 138.4119 Val_Loss: 309.3882  BEST VAL Loss: 309.3882\n",
            "\n",
            "Epoch 690: Validation loss decreased (309.388153 --> 309.367767).\n",
            "\t Train_Loss: 138.4119 Val_Loss: 309.3678  BEST VAL Loss: 309.3678\n",
            "\n",
            "Epoch 691: Validation loss decreased (309.367767 --> 309.347504).\n",
            "\t Train_Loss: 138.4118 Val_Loss: 309.3475  BEST VAL Loss: 309.3475\n",
            "\n",
            "Epoch 692: Validation loss decreased (309.347504 --> 309.327972).\n",
            "\t Train_Loss: 138.4118 Val_Loss: 309.3280  BEST VAL Loss: 309.3280\n",
            "\n",
            "Epoch 693: Validation loss decreased (309.327972 --> 309.308441).\n",
            "\t Train_Loss: 138.4117 Val_Loss: 309.3084  BEST VAL Loss: 309.3084\n",
            "\n",
            "Epoch 694: Validation loss decreased (309.308441 --> 309.289276).\n",
            "\t Train_Loss: 138.4116 Val_Loss: 309.2893  BEST VAL Loss: 309.2893\n",
            "\n",
            "Epoch 695: Validation loss decreased (309.289276 --> 309.270569).\n",
            "\t Train_Loss: 138.4117 Val_Loss: 309.2706  BEST VAL Loss: 309.2706\n",
            "\n",
            "Epoch 696: Validation loss decreased (309.270569 --> 309.252197).\n",
            "\t Train_Loss: 138.4115 Val_Loss: 309.2522  BEST VAL Loss: 309.2522\n",
            "\n",
            "Epoch 697: Validation loss decreased (309.252197 --> 309.233582).\n",
            "\t Train_Loss: 138.4115 Val_Loss: 309.2336  BEST VAL Loss: 309.2336\n",
            "\n",
            "Epoch 698: Validation loss decreased (309.233582 --> 309.215912).\n",
            "\t Train_Loss: 138.4115 Val_Loss: 309.2159  BEST VAL Loss: 309.2159\n",
            "\n",
            "Epoch 699: Validation loss decreased (309.215912 --> 309.198364).\n",
            "\t Train_Loss: 138.4115 Val_Loss: 309.1984  BEST VAL Loss: 309.1984\n",
            "\n",
            "Epoch 700: Validation loss decreased (309.198364 --> 309.181274).\n",
            "\t Train_Loss: 138.4114 Val_Loss: 309.1813  BEST VAL Loss: 309.1813\n",
            "\n",
            "Epoch 701: Validation loss decreased (309.181274 --> 309.164185).\n",
            "\t Train_Loss: 138.4113 Val_Loss: 309.1642  BEST VAL Loss: 309.1642\n",
            "\n",
            "Epoch 702: Validation loss decreased (309.164185 --> 309.147614).\n",
            "\t Train_Loss: 138.4113 Val_Loss: 309.1476  BEST VAL Loss: 309.1476\n",
            "\n",
            "Epoch 703: Validation loss decreased (309.147614 --> 309.131195).\n",
            "\t Train_Loss: 138.4113 Val_Loss: 309.1312  BEST VAL Loss: 309.1312\n",
            "\n",
            "Epoch 704: Validation loss decreased (309.131195 --> 309.115387).\n",
            "\t Train_Loss: 138.4113 Val_Loss: 309.1154  BEST VAL Loss: 309.1154\n",
            "\n",
            "Epoch 705: Validation loss decreased (309.115387 --> 309.099426).\n",
            "\t Train_Loss: 138.4112 Val_Loss: 309.0994  BEST VAL Loss: 309.0994\n",
            "\n",
            "Epoch 706: Validation loss decreased (309.099426 --> 309.083862).\n",
            "\t Train_Loss: 138.4112 Val_Loss: 309.0839  BEST VAL Loss: 309.0839\n",
            "\n",
            "Epoch 707: Validation loss decreased (309.083862 --> 309.068542).\n",
            "\t Train_Loss: 138.4111 Val_Loss: 309.0685  BEST VAL Loss: 309.0685\n",
            "\n",
            "Epoch 708: Validation loss decreased (309.068542 --> 309.053436).\n",
            "\t Train_Loss: 138.4111 Val_Loss: 309.0534  BEST VAL Loss: 309.0534\n",
            "\n",
            "Epoch 709: Validation loss decreased (309.053436 --> 309.038788).\n",
            "\t Train_Loss: 138.4111 Val_Loss: 309.0388  BEST VAL Loss: 309.0388\n",
            "\n",
            "Epoch 710: Validation loss decreased (309.038788 --> 309.024353).\n",
            "\t Train_Loss: 138.4111 Val_Loss: 309.0244  BEST VAL Loss: 309.0244\n",
            "\n",
            "Epoch 711: Validation loss decreased (309.024353 --> 309.010162).\n",
            "\t Train_Loss: 138.4110 Val_Loss: 309.0102  BEST VAL Loss: 309.0102\n",
            "\n",
            "Epoch 712: Validation loss decreased (309.010162 --> 308.996094).\n",
            "\t Train_Loss: 138.4110 Val_Loss: 308.9961  BEST VAL Loss: 308.9961\n",
            "\n",
            "Epoch 713: Validation loss decreased (308.996094 --> 308.982544).\n",
            "\t Train_Loss: 138.4109 Val_Loss: 308.9825  BEST VAL Loss: 308.9825\n",
            "\n",
            "Epoch 714: Validation loss decreased (308.982544 --> 308.969299).\n",
            "\t Train_Loss: 138.4110 Val_Loss: 308.9693  BEST VAL Loss: 308.9693\n",
            "\n",
            "Epoch 715: Validation loss decreased (308.969299 --> 308.955872).\n",
            "\t Train_Loss: 138.4110 Val_Loss: 308.9559  BEST VAL Loss: 308.9559\n",
            "\n",
            "Epoch 716: Validation loss decreased (308.955872 --> 308.942932).\n",
            "\t Train_Loss: 138.4109 Val_Loss: 308.9429  BEST VAL Loss: 308.9429\n",
            "\n",
            "Epoch 717: Validation loss decreased (308.942932 --> 308.930054).\n",
            "\t Train_Loss: 138.4109 Val_Loss: 308.9301  BEST VAL Loss: 308.9301\n",
            "\n",
            "Epoch 718: Validation loss decreased (308.930054 --> 308.917572).\n",
            "\t Train_Loss: 138.4109 Val_Loss: 308.9176  BEST VAL Loss: 308.9176\n",
            "\n",
            "Epoch 719: Validation loss decreased (308.917572 --> 308.905212).\n",
            "\t Train_Loss: 138.4108 Val_Loss: 308.9052  BEST VAL Loss: 308.9052\n",
            "\n",
            "Epoch 720: Validation loss decreased (308.905212 --> 308.893280).\n",
            "\t Train_Loss: 138.4108 Val_Loss: 308.8933  BEST VAL Loss: 308.8933\n",
            "\n",
            "Epoch 721: Validation loss decreased (308.893280 --> 308.881500).\n",
            "\t Train_Loss: 138.4108 Val_Loss: 308.8815  BEST VAL Loss: 308.8815\n",
            "\n",
            "Epoch 722: Validation loss decreased (308.881500 --> 308.869904).\n",
            "\t Train_Loss: 138.4108 Val_Loss: 308.8699  BEST VAL Loss: 308.8699\n",
            "\n",
            "Epoch 723: Validation loss decreased (308.869904 --> 308.858307).\n",
            "\t Train_Loss: 138.4108 Val_Loss: 308.8583  BEST VAL Loss: 308.8583\n",
            "\n",
            "Epoch 724: Validation loss decreased (308.858307 --> 308.846863).\n",
            "\t Train_Loss: 138.4108 Val_Loss: 308.8469  BEST VAL Loss: 308.8469\n",
            "\n",
            "Epoch 725: Validation loss decreased (308.846863 --> 308.835999).\n",
            "\t Train_Loss: 138.4107 Val_Loss: 308.8360  BEST VAL Loss: 308.8360\n",
            "\n",
            "Epoch 726: Validation loss decreased (308.835999 --> 308.825256).\n",
            "\t Train_Loss: 138.4107 Val_Loss: 308.8253  BEST VAL Loss: 308.8253\n",
            "\n",
            "Epoch 727: Validation loss decreased (308.825256 --> 308.814545).\n",
            "\t Train_Loss: 138.4107 Val_Loss: 308.8145  BEST VAL Loss: 308.8145\n",
            "\n",
            "Epoch 728: Validation loss decreased (308.814545 --> 308.804047).\n",
            "\t Train_Loss: 138.4107 Val_Loss: 308.8040  BEST VAL Loss: 308.8040\n",
            "\n",
            "Epoch 729: Validation loss decreased (308.804047 --> 308.793610).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.7936  BEST VAL Loss: 308.7936\n",
            "\n",
            "Epoch 730: Validation loss decreased (308.793610 --> 308.783508).\n",
            "\t Train_Loss: 138.4107 Val_Loss: 308.7835  BEST VAL Loss: 308.7835\n",
            "\n",
            "Epoch 731: Validation loss decreased (308.783508 --> 308.773651).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.7737  BEST VAL Loss: 308.7737\n",
            "\n",
            "Epoch 732: Validation loss decreased (308.773651 --> 308.763916).\n",
            "\t Train_Loss: 138.4107 Val_Loss: 308.7639  BEST VAL Loss: 308.7639\n",
            "\n",
            "Epoch 733: Validation loss decreased (308.763916 --> 308.754639).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.7546  BEST VAL Loss: 308.7546\n",
            "\n",
            "Epoch 734: Validation loss decreased (308.754639 --> 308.745239).\n",
            "\t Train_Loss: 138.4107 Val_Loss: 308.7452  BEST VAL Loss: 308.7452\n",
            "\n",
            "Epoch 735: Validation loss decreased (308.745239 --> 308.736298).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.7363  BEST VAL Loss: 308.7363\n",
            "\n",
            "Epoch 736: Validation loss decreased (308.736298 --> 308.726868).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.7269  BEST VAL Loss: 308.7269\n",
            "\n",
            "Epoch 737: Validation loss decreased (308.726868 --> 308.717957).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.7180  BEST VAL Loss: 308.7180\n",
            "\n",
            "Epoch 738: Validation loss decreased (308.717957 --> 308.709229).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.7092  BEST VAL Loss: 308.7092\n",
            "\n",
            "Epoch 739: Validation loss decreased (308.709229 --> 308.700867).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.7009  BEST VAL Loss: 308.7009\n",
            "\n",
            "Epoch 740: Validation loss decreased (308.700867 --> 308.692352).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.6924  BEST VAL Loss: 308.6924\n",
            "\n",
            "Epoch 741: Validation loss decreased (308.692352 --> 308.684235).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.6842  BEST VAL Loss: 308.6842\n",
            "\n",
            "Epoch 742: Validation loss decreased (308.684235 --> 308.676147).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.6761  BEST VAL Loss: 308.6761\n",
            "\n",
            "Epoch 743: Validation loss decreased (308.676147 --> 308.668488).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.6685  BEST VAL Loss: 308.6685\n",
            "\n",
            "Epoch 744: Validation loss decreased (308.668488 --> 308.660370).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.6604  BEST VAL Loss: 308.6604\n",
            "\n",
            "Epoch 745: Validation loss decreased (308.660370 --> 308.652557).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.6526  BEST VAL Loss: 308.6526\n",
            "\n",
            "Epoch 746: Validation loss decreased (308.652557 --> 308.645172).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.6452  BEST VAL Loss: 308.6452\n",
            "\n",
            "Epoch 747: Validation loss decreased (308.645172 --> 308.637756).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.6378  BEST VAL Loss: 308.6378\n",
            "\n",
            "Epoch 748: Validation loss decreased (308.637756 --> 308.630463).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.6305  BEST VAL Loss: 308.6305\n",
            "\n",
            "Epoch 749: Validation loss decreased (308.630463 --> 308.623871).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.6239  BEST VAL Loss: 308.6239\n",
            "\n",
            "Epoch 750: Validation loss decreased (308.623871 --> 308.616913).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.6169  BEST VAL Loss: 308.6169\n",
            "\n",
            "Epoch 751: Validation loss decreased (308.616913 --> 308.609955).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.6100  BEST VAL Loss: 308.6100\n",
            "\n",
            "Epoch 752: Validation loss decreased (308.609955 --> 308.603363).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 308.6034  BEST VAL Loss: 308.6034\n",
            "\n",
            "Epoch 753: Validation loss decreased (308.603363 --> 308.596558).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.5966  BEST VAL Loss: 308.5966\n",
            "\n",
            "Epoch 754: Validation loss decreased (308.596558 --> 308.590179).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5902  BEST VAL Loss: 308.5902\n",
            "\n",
            "Epoch 755: Validation loss decreased (308.590179 --> 308.584106).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.5841  BEST VAL Loss: 308.5841\n",
            "\n",
            "Epoch 756: Validation loss decreased (308.584106 --> 308.577789).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5778  BEST VAL Loss: 308.5778\n",
            "\n",
            "Epoch 757: Validation loss decreased (308.577789 --> 308.571625).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5716  BEST VAL Loss: 308.5716\n",
            "\n",
            "Epoch 758: Validation loss decreased (308.571625 --> 308.565979).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5660  BEST VAL Loss: 308.5660\n",
            "\n",
            "Epoch 759: Validation loss decreased (308.565979 --> 308.559845).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.5598  BEST VAL Loss: 308.5598\n",
            "\n",
            "Epoch 760: Validation loss decreased (308.559845 --> 308.554169).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5542  BEST VAL Loss: 308.5542\n",
            "\n",
            "Epoch 761: Validation loss decreased (308.554169 --> 308.548462).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 308.5485  BEST VAL Loss: 308.5485\n",
            "\n",
            "Epoch 762: Validation loss decreased (308.548462 --> 308.543030).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5430  BEST VAL Loss: 308.5430\n",
            "\n",
            "Epoch 763: Validation loss decreased (308.543030 --> 308.537567).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5376  BEST VAL Loss: 308.5376\n",
            "\n",
            "Epoch 764: Validation loss decreased (308.537567 --> 308.532318).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5323  BEST VAL Loss: 308.5323\n",
            "\n",
            "Epoch 765: Validation loss decreased (308.532318 --> 308.526855).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5269  BEST VAL Loss: 308.5269\n",
            "\n",
            "Epoch 766: Validation loss decreased (308.526855 --> 308.522034).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5220  BEST VAL Loss: 308.5220\n",
            "\n",
            "Epoch 767: Validation loss decreased (308.522034 --> 308.516815).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5168  BEST VAL Loss: 308.5168\n",
            "\n",
            "Epoch 768: Validation loss decreased (308.516815 --> 308.511963).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5120  BEST VAL Loss: 308.5120\n",
            "\n",
            "Epoch 769: Validation loss decreased (308.511963 --> 308.506958).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5070  BEST VAL Loss: 308.5070\n",
            "\n",
            "Epoch 770: Validation loss decreased (308.506958 --> 308.502380).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.5024  BEST VAL Loss: 308.5024\n",
            "\n",
            "Epoch 771: Validation loss decreased (308.502380 --> 308.497131).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.4971  BEST VAL Loss: 308.4971\n",
            "\n",
            "Epoch 772: Validation loss decreased (308.497131 --> 308.493164).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4932  BEST VAL Loss: 308.4932\n",
            "\n",
            "Epoch 773: Validation loss decreased (308.493164 --> 308.488617).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4886  BEST VAL Loss: 308.4886\n",
            "\n",
            "Epoch 774: Validation loss decreased (308.488617 --> 308.484009).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4840  BEST VAL Loss: 308.4840\n",
            "\n",
            "Epoch 775: Validation loss decreased (308.484009 --> 308.479889).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4799  BEST VAL Loss: 308.4799\n",
            "\n",
            "Epoch 776: Validation loss decreased (308.479889 --> 308.475494).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4755  BEST VAL Loss: 308.4755\n",
            "\n",
            "Epoch 777: Validation loss decreased (308.475494 --> 308.471100).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4711  BEST VAL Loss: 308.4711\n",
            "\n",
            "Epoch 778: Validation loss decreased (308.471100 --> 308.467377).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4674  BEST VAL Loss: 308.4674\n",
            "\n",
            "Epoch 779: Validation loss decreased (308.467377 --> 308.463470).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4635  BEST VAL Loss: 308.4635\n",
            "\n",
            "Epoch 780: Validation loss decreased (308.463470 --> 308.459778).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4598  BEST VAL Loss: 308.4598\n",
            "\n",
            "Epoch 781: Validation loss decreased (308.459778 --> 308.455627).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4556  BEST VAL Loss: 308.4556\n",
            "\n",
            "Epoch 782: Validation loss decreased (308.455627 --> 308.451660).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4517  BEST VAL Loss: 308.4517\n",
            "\n",
            "Epoch 783: Validation loss decreased (308.451660 --> 308.448395).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4484  BEST VAL Loss: 308.4484\n",
            "\n",
            "Epoch 784: Validation loss decreased (308.448395 --> 308.444672).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4447  BEST VAL Loss: 308.4447\n",
            "\n",
            "Epoch 785: Validation loss decreased (308.444672 --> 308.441162).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4412  BEST VAL Loss: 308.4412\n",
            "\n",
            "Epoch 786: Validation loss decreased (308.441162 --> 308.437714).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4377  BEST VAL Loss: 308.4377\n",
            "\n",
            "Epoch 787: Validation loss decreased (308.437714 --> 308.433990).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.4340  BEST VAL Loss: 308.4340\n",
            "\n",
            "Epoch 788: Validation loss decreased (308.433990 --> 308.430481).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4305  BEST VAL Loss: 308.4305\n",
            "\n",
            "Epoch 789: Validation loss decreased (308.430481 --> 308.427460).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4275  BEST VAL Loss: 308.4275\n",
            "\n",
            "Epoch 790: Validation loss decreased (308.427460 --> 308.424561).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4246  BEST VAL Loss: 308.4246\n",
            "\n",
            "Epoch 791: Validation loss decreased (308.424561 --> 308.421295).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4213  BEST VAL Loss: 308.4213\n",
            "\n",
            "Epoch 792: Validation loss decreased (308.421295 --> 308.418121).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4181  BEST VAL Loss: 308.4181\n",
            "\n",
            "Epoch 793: Validation loss decreased (308.418121 --> 308.415192).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4152  BEST VAL Loss: 308.4152\n",
            "\n",
            "Epoch 794: Validation loss decreased (308.415192 --> 308.411926).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.4119  BEST VAL Loss: 308.4119\n",
            "\n",
            "Epoch 795: Validation loss decreased (308.411926 --> 308.409271).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4093  BEST VAL Loss: 308.4093\n",
            "\n",
            "Epoch 796: Validation loss decreased (308.409271 --> 308.406433).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4064  BEST VAL Loss: 308.4064\n",
            "\n",
            "Epoch 797: Validation loss decreased (308.406433 --> 308.403839).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4038  BEST VAL Loss: 308.4038\n",
            "\n",
            "Epoch 798: Validation loss decreased (308.403839 --> 308.400757).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.4008  BEST VAL Loss: 308.4008\n",
            "\n",
            "Epoch 799: Validation loss decreased (308.400757 --> 308.398346).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3983  BEST VAL Loss: 308.3983\n",
            "\n",
            "Epoch 800: Validation loss decreased (308.398346 --> 308.395508).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3955  BEST VAL Loss: 308.3955\n",
            "\n",
            "Epoch 801: Validation loss decreased (308.395508 --> 308.392914).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3929  BEST VAL Loss: 308.3929\n",
            "\n",
            "Epoch 802: Validation loss decreased (308.392914 --> 308.390289).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3903  BEST VAL Loss: 308.3903\n",
            "\n",
            "Epoch 803: Validation loss decreased (308.390289 --> 308.388092).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3881  BEST VAL Loss: 308.3881\n",
            "\n",
            "Epoch 804: Validation loss decreased (308.388092 --> 308.385468).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3855  BEST VAL Loss: 308.3855\n",
            "\n",
            "Epoch 805: Validation loss decreased (308.385468 --> 308.382874).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3829  BEST VAL Loss: 308.3829\n",
            "\n",
            "Epoch 806: Validation loss decreased (308.382874 --> 308.381104).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3811  BEST VAL Loss: 308.3811\n",
            "\n",
            "Epoch 807: Validation loss decreased (308.381104 --> 308.378479).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3785  BEST VAL Loss: 308.3785\n",
            "\n",
            "Epoch 808: Validation loss decreased (308.378479 --> 308.376495).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3765  BEST VAL Loss: 308.3765\n",
            "\n",
            "Epoch 809: Validation loss decreased (308.376495 --> 308.374115).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3741  BEST VAL Loss: 308.3741\n",
            "\n",
            "Epoch 810: Validation loss decreased (308.374115 --> 308.371704).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3717  BEST VAL Loss: 308.3717\n",
            "\n",
            "Epoch 811: Validation loss decreased (308.371704 --> 308.369720).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3697  BEST VAL Loss: 308.3697\n",
            "\n",
            "Epoch 812: Validation loss decreased (308.369720 --> 308.367584).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3676  BEST VAL Loss: 308.3676\n",
            "\n",
            "Epoch 813: Validation loss decreased (308.367584 --> 308.365814).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3658  BEST VAL Loss: 308.3658\n",
            "\n",
            "Epoch 814: Validation loss decreased (308.365814 --> 308.363495).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3635  BEST VAL Loss: 308.3635\n",
            "\n",
            "Epoch 815: Validation loss decreased (308.363495 --> 308.361877).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3619  BEST VAL Loss: 308.3619\n",
            "\n",
            "Epoch 816: Validation loss decreased (308.361877 --> 308.359711).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3597  BEST VAL Loss: 308.3597\n",
            "\n",
            "Epoch 817: Validation loss decreased (308.359711 --> 308.358154).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3582  BEST VAL Loss: 308.3582\n",
            "\n",
            "Epoch 818: Validation loss decreased (308.358154 --> 308.356201).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3562  BEST VAL Loss: 308.3562\n",
            "\n",
            "Epoch 819: Validation loss decreased (308.356201 --> 308.354462).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3545  BEST VAL Loss: 308.3545\n",
            "\n",
            "Epoch 820: Validation loss decreased (308.354462 --> 308.352722).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3527  BEST VAL Loss: 308.3527\n",
            "\n",
            "Epoch 821: Validation loss decreased (308.352722 --> 308.350983).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3510  BEST VAL Loss: 308.3510\n",
            "\n",
            "Epoch 822: Validation loss decreased (308.350983 --> 308.348969).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3490  BEST VAL Loss: 308.3490\n",
            "\n",
            "Epoch 823: Validation loss decreased (308.348969 --> 308.347656).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3477  BEST VAL Loss: 308.3477\n",
            "\n",
            "Epoch 824: Validation loss decreased (308.347656 --> 308.345947).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3459  BEST VAL Loss: 308.3459\n",
            "\n",
            "Epoch 825: Validation loss decreased (308.345947 --> 308.344635).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3446  BEST VAL Loss: 308.3446\n",
            "\n",
            "Epoch 826: Validation loss decreased (308.344635 --> 308.343109).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3431  BEST VAL Loss: 308.3431\n",
            "\n",
            "Epoch 827: Validation loss decreased (308.343109 --> 308.341156).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3412  BEST VAL Loss: 308.3412\n",
            "\n",
            "Epoch 828: Validation loss decreased (308.341156 --> 308.339813).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3398  BEST VAL Loss: 308.3398\n",
            "\n",
            "Epoch 829: Validation loss decreased (308.339813 --> 308.338074).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3381  BEST VAL Loss: 308.3381\n",
            "\n",
            "Epoch 830: Validation loss decreased (308.338074 --> 308.336975).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3370  BEST VAL Loss: 308.3370\n",
            "\n",
            "Epoch 831: Validation loss decreased (308.336975 --> 308.335876).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3359  BEST VAL Loss: 308.3359\n",
            "\n",
            "Epoch 832: Validation loss decreased (308.335876 --> 308.333923).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3339  BEST VAL Loss: 308.3339\n",
            "\n",
            "Epoch 833: Validation loss decreased (308.333923 --> 308.333038).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3330  BEST VAL Loss: 308.3330\n",
            "\n",
            "Epoch 834: Validation loss decreased (308.333038 --> 308.331512).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3315  BEST VAL Loss: 308.3315\n",
            "\n",
            "Epoch 835: Validation loss decreased (308.331512 --> 308.330414).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3304  BEST VAL Loss: 308.3304\n",
            "\n",
            "Epoch 836: Validation loss decreased (308.330414 --> 308.329315).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3293  BEST VAL Loss: 308.3293\n",
            "\n",
            "Epoch 837: Validation loss decreased (308.329315 --> 308.328033).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3280  BEST VAL Loss: 308.3280\n",
            "\n",
            "Epoch 838: Validation loss decreased (308.328033 --> 308.326477).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3265  BEST VAL Loss: 308.3265\n",
            "\n",
            "Epoch 839: Validation loss decreased (308.326477 --> 308.325592).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3256  BEST VAL Loss: 308.3256\n",
            "\n",
            "Epoch 840: Validation loss decreased (308.325592 --> 308.324310).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3243  BEST VAL Loss: 308.3243\n",
            "\n",
            "Epoch 841: Validation loss decreased (308.324310 --> 308.322876).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3229  BEST VAL Loss: 308.3229\n",
            "\n",
            "Epoch 842: Validation loss decreased (308.322876 --> 308.322113).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3221  BEST VAL Loss: 308.3221\n",
            "\n",
            "Epoch 843: Validation loss decreased (308.322113 --> 308.321442).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3214  BEST VAL Loss: 308.3214\n",
            "\n",
            "Epoch 844: Validation loss decreased (308.321442 --> 308.319946).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3199  BEST VAL Loss: 308.3199\n",
            "\n",
            "Epoch 845: Validation loss decreased (308.319946 --> 308.319061).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3191  BEST VAL Loss: 308.3191\n",
            "\n",
            "Epoch 846: Validation loss decreased (308.319061 --> 308.318024).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3180  BEST VAL Loss: 308.3180\n",
            "\n",
            "Epoch 847: Validation loss decreased (308.318024 --> 308.316986).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3170  BEST VAL Loss: 308.3170\n",
            "\n",
            "Epoch 848: Validation loss decreased (308.316986 --> 308.315582).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3156  BEST VAL Loss: 308.3156\n",
            "\n",
            "Epoch 849: Validation loss decreased (308.315582 --> 308.314911).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3149  BEST VAL Loss: 308.3149\n",
            "\n",
            "Epoch 850: Validation loss decreased (308.314911 --> 308.314240).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3142  BEST VAL Loss: 308.3142\n",
            "\n",
            "Epoch 851: Validation loss decreased (308.314240 --> 308.313416).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3134  BEST VAL Loss: 308.3134\n",
            "\n",
            "Epoch 852: Validation loss decreased (308.313416 --> 308.312317).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3123  BEST VAL Loss: 308.3123\n",
            "\n",
            "Epoch 853: Validation loss decreased (308.312317 --> 308.311462).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3115  BEST VAL Loss: 308.3115\n",
            "\n",
            "Epoch 854: Validation loss decreased (308.311462 --> 308.310760).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3108  BEST VAL Loss: 308.3108\n",
            "\n",
            "Epoch 855: Validation loss decreased (308.310760 --> 308.310028).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3100  BEST VAL Loss: 308.3100\n",
            "\n",
            "Epoch 856: Validation loss decreased (308.310028 --> 308.308807).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3088  BEST VAL Loss: 308.3088\n",
            "\n",
            "Epoch 857: Validation loss decreased (308.308807 --> 308.307922).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3079  BEST VAL Loss: 308.3079\n",
            "\n",
            "Epoch 858: Validation loss decreased (308.307922 --> 308.307037).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3070  BEST VAL Loss: 308.3070\n",
            "\n",
            "Epoch 859: Validation loss decreased (308.307037 --> 308.306610).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3066  BEST VAL Loss: 308.3066\n",
            "\n",
            "Epoch 860: Validation loss decreased (308.306610 --> 308.305573).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3056  BEST VAL Loss: 308.3056\n",
            "\n",
            "Epoch 861: Validation loss decreased (308.305573 --> 308.304871).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3049  BEST VAL Loss: 308.3049\n",
            "\n",
            "Epoch 862: Validation loss decreased (308.304871 --> 308.304199).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3042  BEST VAL Loss: 308.3042\n",
            "\n",
            "Epoch 863: Validation loss decreased (308.304199 --> 308.303528).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3035  BEST VAL Loss: 308.3035\n",
            "\n",
            "Epoch 864: Validation loss decreased (308.303528 --> 308.303070).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3031  BEST VAL Loss: 308.3031\n",
            "\n",
            "Epoch 865: Validation loss decreased (308.303070 --> 308.302460).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3025  BEST VAL Loss: 308.3025\n",
            "\n",
            "Epoch 866: Validation loss decreased (308.302460 --> 308.301178).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3012  BEST VAL Loss: 308.3012\n",
            "\n",
            "Epoch 867: Validation loss decreased (308.301178 --> 308.300507).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.3005  BEST VAL Loss: 308.3005\n",
            "\n",
            "Epoch 868: Validation loss decreased (308.300507 --> 308.300079).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.3001  BEST VAL Loss: 308.3001\n",
            "\n",
            "Epoch 869: Validation loss decreased (308.300079 --> 308.299652).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2997  BEST VAL Loss: 308.2997\n",
            "\n",
            "Epoch 870: Validation loss decreased (308.299652 --> 308.299194).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.2992  BEST VAL Loss: 308.2992\n",
            "\n",
            "Epoch 871: Validation loss decreased (308.299194 --> 308.298553).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.2986  BEST VAL Loss: 308.2986\n",
            "\n",
            "Epoch 872: Validation loss decreased (308.298553 --> 308.297852).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2979  BEST VAL Loss: 308.2979\n",
            "\n",
            "Epoch 873: Validation loss decreased (308.297852 --> 308.297455).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2975  BEST VAL Loss: 308.2975\n",
            "\n",
            "Epoch 874: Validation loss decreased (308.297455 --> 308.296783).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.2968  BEST VAL Loss: 308.2968\n",
            "\n",
            "Epoch 875: Validation loss decreased (308.296783 --> 308.296112).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.2961  BEST VAL Loss: 308.2961\n",
            "\n",
            "Epoch 876: Validation loss decreased (308.296112 --> 308.295685).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2957  BEST VAL Loss: 308.2957\n",
            "\n",
            "Epoch 877: Validation loss decreased (308.295685 --> 308.295258).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2953  BEST VAL Loss: 308.2953\n",
            "\n",
            "Epoch 878: Validation loss decreased (308.295258 --> 308.294800).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2948  BEST VAL Loss: 308.2948\n",
            "\n",
            "Epoch 879: Validation loss decreased (308.294800 --> 308.293945).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2939  BEST VAL Loss: 308.2939\n",
            "\n",
            "Epoch 880: Validation loss decreased (308.293945 --> 308.293304).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.2933  BEST VAL Loss: 308.2933\n",
            "\n",
            "Epoch 881: Validation loss decreased (308.293304 --> 308.293060).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2931  BEST VAL Loss: 308.2931\n",
            "\n",
            "Epoch 882: Validation loss decreased (308.293060 --> 308.292633).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.2926  BEST VAL Loss: 308.2926\n",
            "\n",
            "Epoch 883: Validation loss decreased (308.292633 --> 308.292175).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2922  BEST VAL Loss: 308.2922\n",
            "\n",
            "Epoch 884: Validation loss decreased (308.292175 --> 308.291595).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2916  BEST VAL Loss: 308.2916\n",
            "\n",
            "Epoch 885: Validation loss decreased (308.291595 --> 308.291321).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2913  BEST VAL Loss: 308.2913\n",
            "\n",
            "Epoch 886: Validation loss decreased (308.291321 --> 308.290894).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2909  BEST VAL Loss: 308.2909\n",
            "\n",
            "Epoch 887: Validation loss decreased (308.290894 --> 308.290253).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2903  BEST VAL Loss: 308.2903\n",
            "\n",
            "Epoch 888: Validation loss decreased (308.290253 --> 308.290009).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2900  BEST VAL Loss: 308.2900\n",
            "\n",
            "Epoch 889: Validation loss decreased (308.290009 --> 308.289795).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2898  BEST VAL Loss: 308.2898\n",
            "\n",
            "Epoch 890: Validation loss decreased (308.289795 --> 308.289368).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.2894  BEST VAL Loss: 308.2894\n",
            "\n",
            "Epoch 891: Validation loss decreased (308.289368 --> 308.289154).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2892  BEST VAL Loss: 308.2892\n",
            "\n",
            "Epoch 892: Validation loss decreased (308.289154 --> 308.288696).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2887  BEST VAL Loss: 308.2887\n",
            "\n",
            "Epoch 893: Validation loss decreased (308.288696 --> 308.288483).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2885  BEST VAL Loss: 308.2885\n",
            "\n",
            "Epoch 894: Validation loss decreased (308.288483 --> 308.288269).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2883  BEST VAL Loss: 308.2883\n",
            "\n",
            "Epoch 895: Validation loss decreased (308.288269 --> 308.287811).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2878  BEST VAL Loss: 308.2878\n",
            "\n",
            "Epoch 896: Validation loss decreased (308.287811 --> 308.287598).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2876  BEST VAL Loss: 308.2876\n",
            "\n",
            "Epoch 897: Validation loss decreased (308.287598 --> 308.286957).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2870  BEST VAL Loss: 308.2870\n",
            "\n",
            "Epoch 898: Validation loss decreased (308.286957 --> 308.286499).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2865  BEST VAL Loss: 308.2865\n",
            "\n",
            "Epoch 899: Validation loss decreased (308.286499 --> 308.286285).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2863  BEST VAL Loss: 308.2863\n",
            "\n",
            "Epoch 900: Validation loss decreased (308.286285 --> 308.285858).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2859  BEST VAL Loss: 308.2859\n",
            "\n",
            "Epoch 901: Validation loss decreased (308.285858 --> 308.285614).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2856  BEST VAL Loss: 308.2856\n",
            "\n",
            "Epoch 902: Validation loss decreased (308.285614 --> 308.284973).\n",
            "\t Train_Loss: 138.4104 Val_Loss: 308.2850  BEST VAL Loss: 308.2850\n",
            "\n",
            "Epoch 903: Validation loss decreased (308.284973 --> 308.284760).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2848  BEST VAL Loss: 308.2848\n",
            "\n",
            "Epoch 904: Validation loss decreased (308.284760 --> 308.284515).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2845  BEST VAL Loss: 308.2845\n",
            "\n",
            "Epoch 905: Validation loss decreased (308.284515 --> 308.284332).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2843  BEST VAL Loss: 308.2843\n",
            "\n",
            "Epoch 906: Validation loss decreased (308.284332 --> 308.283875).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2839  BEST VAL Loss: 308.2839\n",
            "\n",
            "Epoch 907: Validation loss decreased (308.283875 --> 308.283325).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2833  BEST VAL Loss: 308.2833\n",
            "\n",
            "Epoch 908: Validation loss decreased (308.283325 --> 308.283264).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2833  BEST VAL Loss: 308.2833\n",
            "\n",
            "Epoch 909: Validation loss decreased (308.283264 --> 308.282806).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2828  BEST VAL Loss: 308.2828\n",
            "\n",
            "Epoch 910: Validation loss decreased (308.282806 --> 308.282806).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2828  BEST VAL Loss: 308.2828\n",
            "\n",
            "Epoch 911: Validation loss decreased (308.282806 --> 308.282471).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2825  BEST VAL Loss: 308.2825\n",
            "\n",
            "Epoch 912: Validation loss decreased (308.282471 --> 308.282410).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2824  BEST VAL Loss: 308.2824\n",
            "\n",
            "Epoch 913: Validation loss decreased (308.282410 --> 308.282166).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2822  BEST VAL Loss: 308.2822\n",
            "\n",
            "Epoch 914: Validation loss decreased (308.282166 --> 308.282166).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2822  BEST VAL Loss: 308.2822\n",
            "\n",
            "Epoch 915: Validation loss decreased (308.282166 --> 308.281830).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2818  BEST VAL Loss: 308.2818\n",
            "\n",
            "Epoch 916: Validation loss decreased (308.281830 --> 308.281738).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2817  BEST VAL Loss: 308.2817\n",
            "\n",
            "Epoch 917: Validation loss decreased (308.281738 --> 308.281708).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2817  BEST VAL Loss: 308.2817\n",
            "\n",
            "Epoch 918: Validation loss decreased (308.281708 --> 308.281494).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2815  BEST VAL Loss: 308.2815\n",
            "\n",
            "Epoch 919: Validation loss decreased (308.281494 --> 308.281494).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2815  BEST VAL Loss: 308.2815\n",
            "\n",
            "Epoch 920: Validation loss decreased (308.281494 --> 308.281250).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2812  BEST VAL Loss: 308.2812\n",
            "\n",
            "Epoch 921: Validation loss decreased (308.281250 --> 308.281067).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2811  BEST VAL Loss: 308.2811\n",
            "\n",
            "Epoch 922: Validation loss decreased (308.281067 --> 308.280853).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2809  BEST VAL Loss: 308.2809\n",
            "\n",
            "Epoch 923: Validation loss decreased (308.280853 --> 308.280823).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2808  BEST VAL Loss: 308.2808\n",
            "\n",
            "Epoch 924: Validation loss decreased (308.280823 --> 308.280670).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2807  BEST VAL Loss: 308.2807\n",
            "\n",
            "Epoch 925: Validation loss decreased (308.280670 --> 308.280609).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2806  BEST VAL Loss: 308.2806\n",
            "\n",
            "Epoch 926: Validation loss decreased (308.280609 --> 308.280243).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2802  BEST VAL Loss: 308.2802\n",
            "\n",
            "Epoch 927: Validation loss decreased (308.280243 --> 308.280182).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2802  BEST VAL Loss: 308.2802\n",
            "\n",
            "Epoch 928: Validation loss decreased (308.280182 --> 308.279999).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2800  BEST VAL Loss: 308.2800\n",
            "\n",
            "Epoch 929: Validation loss decreased (308.279999 --> 308.279327).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2793  BEST VAL Loss: 308.2793\n",
            "\n",
            "Epoch 930: Validation loss decreased (308.279327 --> 308.279327).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2793  BEST VAL Loss: 308.2793\n",
            "\n",
            "Epoch 931: Validation loss decreased (308.279327 --> 308.279114).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2791  BEST VAL Loss: 308.2791\n",
            "\n",
            "Epoch 932: Validation loss decreased (308.279114 --> 308.278870).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2789  BEST VAL Loss: 308.2789\n",
            "\n",
            "Epoch 933: Validation loss decreased (308.278870 --> 308.278870).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2789  BEST VAL Loss: 308.2789\n",
            "\n",
            "Epoch 934: Validation loss decreased (308.278870 --> 308.278656).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2787  BEST VAL Loss: 308.2787\n",
            "\n",
            "Epoch 935: Validation loss decreased (308.278656 --> 308.278656).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2787  BEST VAL Loss: 308.2787\n",
            "\n",
            "Epoch 936: Validation loss decreased (308.278656 --> 308.278229).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2782  BEST VAL Loss: 308.2782\n",
            "\n",
            "Epoch 937: Validation loss decreased (308.278229 --> 308.278229).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2782  BEST VAL Loss: 308.2782\n",
            "\n",
            "Epoch 938: Validation loss decreased (308.278229 --> 308.278229).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2782  BEST VAL Loss: 308.2782\n",
            "\n",
            "Epoch 939: Validation loss decreased (308.278229 --> 308.278015).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2780  BEST VAL Loss: 308.2780\n",
            "\n",
            "Epoch 940: Validation loss decreased (308.278015 --> 308.277802).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2778  BEST VAL Loss: 308.2778\n",
            "\n",
            "Epoch 941: Validation loss decreased (308.277802 --> 308.277557).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2776  BEST VAL Loss: 308.2776\n",
            "\n",
            "Epoch 942: Validation loss decreased (308.277557 --> 308.277405).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2774  BEST VAL Loss: 308.2774\n",
            "\n",
            "Epoch 943: Validation loss decreased (308.277405 --> 308.277252).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2773  BEST VAL Loss: 308.2773\n",
            "\n",
            "Epoch 944: Validation loss decreased (308.277252 --> 308.277161).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2772  BEST VAL Loss: 308.2772\n",
            "\n",
            "Epoch 945: Validation loss decreased (308.277161 --> 308.277100).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2771  BEST VAL Loss: 308.2771\n",
            "\n",
            "Epoch 946: Validation loss decreased (308.277100 --> 308.276917).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2769  BEST VAL Loss: 308.2769\n",
            "\n",
            "Epoch 947: Validation loss decreased (308.276917 --> 308.276703).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2767  BEST VAL Loss: 308.2767\n",
            "\n",
            "Epoch 948: Validation loss decreased (308.276703 --> 308.276459).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2765  BEST VAL Loss: 308.2765\n",
            "\n",
            "Epoch 949: Validation loss decreased (308.276459 --> 308.276459).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2765  BEST VAL Loss: 308.2765\n",
            "\n",
            "Epoch 950: Validation loss decreased (308.276459 --> 308.276245).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2762  BEST VAL Loss: 308.2762\n",
            "\n",
            "Epoch 951: Validation loss decreased (308.276245 --> 308.276031).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 308.2760  BEST VAL Loss: 308.2760\n",
            "\n",
            "Epoch 952: Validation loss decreased (308.276031 --> 308.276031).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2760  BEST VAL Loss: 308.2760\n",
            "\n",
            "Epoch 953: Validation loss decreased (308.276031 --> 308.276031).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2760  BEST VAL Loss: 308.2760\n",
            "\n",
            "Epoch 954: Validation loss decreased (308.276031 --> 308.276031).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2760  BEST VAL Loss: 308.2760\n",
            "\n",
            "Epoch 955: Validation loss decreased (308.276031 --> 308.275848).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 956: Validation loss decreased (308.275848 --> 308.275757).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 957: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 958: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 959: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 960: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 961: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 962: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 963: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 964: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 965: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 966: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 967: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 968: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 969: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 970: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 971: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 972: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 973: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 974: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 975: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 976: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 977: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 978: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 979: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 980: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 981: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 982: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 983: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 984: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 985: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 986: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 987: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 988: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 989: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 990: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 991: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 992: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 993: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 994: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 995: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 996: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 997: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 998: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 999: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1000: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1001: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1002: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1003: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1004: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1005: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1006: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1007: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1008: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1009: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1010: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1011: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1012: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1013: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1014: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1015: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1016: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1017: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1018: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1019: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1020: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1021: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1022: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1023: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1024: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1025: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1026: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1027: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1028: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1029: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1030: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1031: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4102 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1032: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1033: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1034: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1035: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1036: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1037: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1038: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1039: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1040: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1041: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1042: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2758  BEST VAL Loss: 308.2758\n",
            "\n",
            "Epoch 1043: Validation loss decreased (308.275757 --> 308.275726).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1044: Validation loss decreased (308.275726 --> 308.275726).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1045: Validation loss decreased (308.275726 --> 308.275726).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1046: Validation loss decreased (308.275726 --> 308.275726).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1047: Validation loss decreased (308.275726 --> 308.275696).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1048: Validation loss decreased (308.275696 --> 308.275696).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1049: Validation loss decreased (308.275696 --> 308.275696).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1050: Validation loss decreased (308.275696 --> 308.275696).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1051: Validation loss decreased (308.275696 --> 308.275696).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1052: Validation loss decreased (308.275696 --> 308.275696).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1053: Validation loss decreased (308.275696 --> 308.275696).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1054: Validation loss decreased (308.275696 --> 308.275665).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1055: Validation loss decreased (308.275665 --> 308.275665).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2757  BEST VAL Loss: 308.2757\n",
            "\n",
            "Epoch 1056: Validation loss decreased (308.275665 --> 308.275635).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1057: Validation loss decreased (308.275635 --> 308.275635).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1058: Validation loss decreased (308.275635 --> 308.275635).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1059: Validation loss decreased (308.275635 --> 308.275635).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1060: Validation loss decreased (308.275635 --> 308.275635).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1061: Validation loss decreased (308.275635 --> 308.275635).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1062: Validation loss decreased (308.275635 --> 308.275635).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1063: Validation loss decreased (308.275635 --> 308.275635).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1064: Validation loss decreased (308.275635 --> 308.275635).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1065: Validation loss decreased (308.275635 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1066: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1067: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1068: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1069: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1070: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1071: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1072: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1073: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1074: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1075: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1076: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1077: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1078: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1079: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1080: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4099 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1081: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4099 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1082: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4099 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1083: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4099 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1084: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4099 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1085: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4099 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1086: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4099 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1087: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4099 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1088: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4098 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1089: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4098 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1090: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4098 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1091: Validation loss decreased (308.275604 --> 308.275604).\n",
            "\t Train_Loss: 138.4098 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1092: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4098 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1093: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4098 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1094: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4098 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1095: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4097 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1096: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4097 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1097: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4097 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1098: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4097 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1099: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1100: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1101: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.2756  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1102: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1103: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1104: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1105: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1106: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1107: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1108: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1109: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1110: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1111: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1112: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4090 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1113: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4089 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1114: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4088 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1115: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4087 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1116: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4086 Val_Loss: 308.2757  BEST VAL Loss: 308.2756\n",
            "\n",
            "Epoch 1117: Validation loss decreased (308.275604 --> 308.275391).\n",
            "\t Train_Loss: 138.4084 Val_Loss: 308.2754  BEST VAL Loss: 308.2754\n",
            "\n",
            "Epoch 1118: Validation loss decreased (308.275391 --> 308.275146).\n",
            "\t Train_Loss: 138.4082 Val_Loss: 308.2751  BEST VAL Loss: 308.2751\n",
            "\n",
            "Epoch 1119: Validation loss decreased (308.275146 --> 308.275146).\n",
            "\t Train_Loss: 138.4081 Val_Loss: 308.2751  BEST VAL Loss: 308.2751\n",
            "\n",
            "Epoch 1120: Validation loss decreased (308.275146 --> 308.274994).\n",
            "\t Train_Loss: 138.4078 Val_Loss: 308.2750  BEST VAL Loss: 308.2750\n",
            "\n",
            "Epoch 1121: Validation loss decreased (308.274994 --> 308.274811).\n",
            "\t Train_Loss: 138.4075 Val_Loss: 308.2748  BEST VAL Loss: 308.2748\n",
            "\n",
            "Epoch 1122: Validation loss decreased (308.274811 --> 308.274719).\n",
            "\t Train_Loss: 138.4072 Val_Loss: 308.2747  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1123: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4067 Val_Loss: 308.2747  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1124: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4061 Val_Loss: 308.2749  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1125: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4053 Val_Loss: 308.2747  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1126: Validation loss decreased (308.274719 --> 308.274658).\n",
            "\t Train_Loss: 138.4043 Val_Loss: 308.2747  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1127: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4029 Val_Loss: 308.2749  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1128: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4009 Val_Loss: 308.2751  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1129: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3977 Val_Loss: 308.2760  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1130: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3924 Val_Loss: 308.2781  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1131: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3823 Val_Loss: 308.2832  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1132: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3587 Val_Loss: 308.3036  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1133: Validation loss did not decrease\n",
            "\t Train_Loss: 138.2820 Val_Loss: 308.5028  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1134: Validation loss did not decrease\n",
            "\t Train_Loss: 137.7452 Val_Loss: 338.2008  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1135: Validation loss did not decrease\n",
            "\t Train_Loss: 131.5338 Val_Loss: 308.6199  BEST VAL Loss: 308.2747\n",
            "\n",
            "Epoch 1136: Validation loss decreased (308.274658 --> 308.122650).\n",
            "\t Train_Loss: 137.0865 Val_Loss: 308.1227  BEST VAL Loss: 308.1227\n",
            "\n",
            "Epoch 1137: Validation loss decreased (308.122650 --> 307.977264).\n",
            "\t Train_Loss: 138.0184 Val_Loss: 307.9773  BEST VAL Loss: 307.9773\n",
            "\n",
            "Epoch 1138: Validation loss decreased (307.977264 --> 307.877655).\n",
            "\t Train_Loss: 138.1933 Val_Loss: 307.8777  BEST VAL Loss: 307.8777\n",
            "\n",
            "Epoch 1139: Validation loss decreased (307.877655 --> 307.795624).\n",
            "\t Train_Loss: 138.2380 Val_Loss: 307.7956  BEST VAL Loss: 307.7956\n",
            "\n",
            "Epoch 1140: Validation loss decreased (307.795624 --> 307.727386).\n",
            "\t Train_Loss: 138.2315 Val_Loss: 307.7274  BEST VAL Loss: 307.7274\n",
            "\n",
            "Epoch 1141: Validation loss decreased (307.727386 --> 307.675842).\n",
            "\t Train_Loss: 138.1722 Val_Loss: 307.6758  BEST VAL Loss: 307.6758\n",
            "\n",
            "Epoch 1142: Validation loss did not decrease\n",
            "\t Train_Loss: 137.9823 Val_Loss: 307.6775  BEST VAL Loss: 307.6758\n",
            "\n",
            "Epoch 1143: Validation loss did not decrease\n",
            "\t Train_Loss: 137.1634 Val_Loss: 308.7970  BEST VAL Loss: 307.6758\n",
            "\n",
            "Epoch 1144: Validation loss did not decrease\n",
            "\t Train_Loss: 130.1794 Val_Loss: 384.1909  BEST VAL Loss: 307.6758\n",
            "\n",
            "Epoch 1145: Validation loss did not decrease\n",
            "\t Train_Loss: 141.3831 Val_Loss: 346.7339  BEST VAL Loss: 307.6758\n",
            "\n",
            "Epoch 1146: Validation loss did not decrease\n",
            "\t Train_Loss: 129.6976 Val_Loss: 307.6994  BEST VAL Loss: 307.6758\n",
            "\n",
            "Epoch 1147: Validation loss decreased (307.675842 --> 306.503754).\n",
            "\t Train_Loss: 130.0551 Val_Loss: 306.5038  BEST VAL Loss: 306.5038\n",
            "\n",
            "Epoch 1148: Validation loss decreased (306.503754 --> 306.076874).\n",
            "\t Train_Loss: 135.5932 Val_Loss: 306.0769  BEST VAL Loss: 306.0769\n",
            "\n",
            "Epoch 1149: Validation loss decreased (306.076874 --> 305.749023).\n",
            "\t Train_Loss: 136.6791 Val_Loss: 305.7490  BEST VAL Loss: 305.7490\n",
            "\n",
            "Epoch 1150: Validation loss decreased (305.749023 --> 305.493652).\n",
            "\t Train_Loss: 136.6339 Val_Loss: 305.4937  BEST VAL Loss: 305.4937\n",
            "\n",
            "Epoch 1151: Validation loss decreased (305.493652 --> 305.479706).\n",
            "\t Train_Loss: 135.5678 Val_Loss: 305.4797  BEST VAL Loss: 305.4797\n",
            "\n",
            "Epoch 1152: Validation loss did not decrease\n",
            "\t Train_Loss: 131.6972 Val_Loss: 309.0186  BEST VAL Loss: 305.4797\n",
            "\n",
            "Epoch 1153: Validation loss did not decrease\n",
            "\t Train_Loss: 122.2134 Val_Loss: 339.6280  BEST VAL Loss: 305.4797\n",
            "\n",
            "Epoch 1154: Validation loss did not decrease\n",
            "\t Train_Loss: 127.2844 Val_Loss: 346.7319  BEST VAL Loss: 305.4797\n",
            "\n",
            "Epoch 1155: Validation loss did not decrease\n",
            "\t Train_Loss: 129.8679 Val_Loss: 320.2405  BEST VAL Loss: 305.4797\n",
            "\n",
            "Epoch 1156: Validation loss did not decrease\n",
            "\t Train_Loss: 121.0159 Val_Loss: 305.8942  BEST VAL Loss: 305.4797\n",
            "\n",
            "Epoch 1157: Validation loss decreased (305.479706 --> 303.373505).\n",
            "\t Train_Loss: 122.0560 Val_Loss: 303.3735  BEST VAL Loss: 303.3735\n",
            "\n",
            "Epoch 1158: Validation loss decreased (303.373505 --> 302.969971).\n",
            "\t Train_Loss: 125.7133 Val_Loss: 302.9700  BEST VAL Loss: 302.9700\n",
            "\n",
            "Epoch 1159: Validation loss did not decrease\n",
            "\t Train_Loss: 124.7119 Val_Loss: 304.8374  BEST VAL Loss: 302.9700\n",
            "\n",
            "Epoch 1160: Validation loss did not decrease\n",
            "\t Train_Loss: 120.3359 Val_Loss: 313.3496  BEST VAL Loss: 302.9700\n",
            "\n",
            "Epoch 1161: Validation loss did not decrease\n",
            "\t Train_Loss: 118.9778 Val_Loss: 323.8639  BEST VAL Loss: 302.9700\n",
            "\n",
            "Epoch 1162: Validation loss did not decrease\n",
            "\t Train_Loss: 122.1991 Val_Loss: 320.3432  BEST VAL Loss: 302.9700\n",
            "\n",
            "Epoch 1163: Validation loss did not decrease\n",
            "\t Train_Loss: 121.0367 Val_Loss: 308.7654  BEST VAL Loss: 302.9700\n",
            "\n",
            "Epoch 1164: Validation loss decreased (302.969971 --> 301.853729).\n",
            "\t Train_Loss: 117.7994 Val_Loss: 301.8537  BEST VAL Loss: 301.8537\n",
            "\n",
            "Epoch 1165: Validation loss decreased (301.853729 --> 299.428650).\n",
            "\t Train_Loss: 118.3545 Val_Loss: 299.4286  BEST VAL Loss: 299.4286\n",
            "\n",
            "Epoch 1166: Validation loss did not decrease\n",
            "\t Train_Loss: 119.4392 Val_Loss: 316.8930  BEST VAL Loss: 299.4286\n",
            "\n",
            "Epoch 1167: Validation loss did not decrease\n",
            "\t Train_Loss: 111.6373 Val_Loss: 299.5813  BEST VAL Loss: 299.4286\n",
            "\n",
            "Epoch 1168: Validation loss did not decrease\n",
            "\t Train_Loss: 116.9265 Val_Loss: 302.3106  BEST VAL Loss: 299.4286\n",
            "\n",
            "Epoch 1169: Validation loss did not decrease\n",
            "\t Train_Loss: 116.3129 Val_Loss: 306.2573  BEST VAL Loss: 299.4286\n",
            "\n",
            "Epoch 1170: Validation loss did not decrease\n",
            "\t Train_Loss: 116.9520 Val_Loss: 306.9669  BEST VAL Loss: 299.4286\n",
            "\n",
            "Epoch 1171: Validation loss did not decrease\n",
            "\t Train_Loss: 117.2385 Val_Loss: 302.9842  BEST VAL Loss: 299.4286\n",
            "\n",
            "Epoch 1172: Validation loss decreased (299.428650 --> 298.023254).\n",
            "\t Train_Loss: 116.1592 Val_Loss: 298.0233  BEST VAL Loss: 298.0233\n",
            "\n",
            "Epoch 1173: Validation loss decreased (298.023254 --> 294.755585).\n",
            "\t Train_Loss: 115.4388 Val_Loss: 294.7556  BEST VAL Loss: 294.7556\n",
            "\n",
            "Epoch 1174: Validation loss decreased (294.755585 --> 293.248199).\n",
            "\t Train_Loss: 115.7473 Val_Loss: 293.2482  BEST VAL Loss: 293.2482\n",
            "\n",
            "Epoch 1175: Validation loss decreased (293.248199 --> 293.083344).\n",
            "\t Train_Loss: 115.9106 Val_Loss: 293.0833  BEST VAL Loss: 293.0833\n",
            "\n",
            "Epoch 1176: Validation loss did not decrease\n",
            "\t Train_Loss: 115.3429 Val_Loss: 294.1628  BEST VAL Loss: 293.0833\n",
            "\n",
            "Epoch 1177: Validation loss did not decrease\n",
            "\t Train_Loss: 114.7066 Val_Loss: 295.9258  BEST VAL Loss: 293.0833\n",
            "\n",
            "Epoch 1178: Validation loss did not decrease\n",
            "\t Train_Loss: 114.6999 Val_Loss: 296.7209  BEST VAL Loss: 293.0833\n",
            "\n",
            "Epoch 1179: Validation loss did not decrease\n",
            "\t Train_Loss: 114.8913 Val_Loss: 295.2973  BEST VAL Loss: 293.0833\n",
            "\n",
            "Epoch 1180: Validation loss decreased (293.083344 --> 292.422455).\n",
            "\t Train_Loss: 114.6003 Val_Loss: 292.4225  BEST VAL Loss: 292.4225\n",
            "\n",
            "Epoch 1181: Validation loss decreased (292.422455 --> 289.700287).\n",
            "\t Train_Loss: 114.1146 Val_Loss: 289.7003  BEST VAL Loss: 289.7003\n",
            "\n",
            "Epoch 1182: Validation loss decreased (289.700287 --> 287.922852).\n",
            "\t Train_Loss: 113.9888 Val_Loss: 287.9229  BEST VAL Loss: 287.9229\n",
            "\n",
            "Epoch 1183: Validation loss decreased (287.922852 --> 287.111938).\n",
            "\t Train_Loss: 114.0848 Val_Loss: 287.1119  BEST VAL Loss: 287.1119\n",
            "\n",
            "Epoch 1184: Validation loss did not decrease\n",
            "\t Train_Loss: 113.9797 Val_Loss: 287.1380  BEST VAL Loss: 287.1119\n",
            "\n",
            "Epoch 1185: Validation loss did not decrease\n",
            "\t Train_Loss: 113.6566 Val_Loss: 287.7860  BEST VAL Loss: 287.1119\n",
            "\n",
            "Epoch 1186: Validation loss did not decrease\n",
            "\t Train_Loss: 113.4347 Val_Loss: 288.4665  BEST VAL Loss: 287.1119\n",
            "\n",
            "Epoch 1187: Validation loss did not decrease\n",
            "\t Train_Loss: 113.4360 Val_Loss: 288.3547  BEST VAL Loss: 287.1119\n",
            "\n",
            "Epoch 1188: Validation loss did not decrease\n",
            "\t Train_Loss: 113.4211 Val_Loss: 287.1335  BEST VAL Loss: 287.1119\n",
            "\n",
            "Epoch 1189: Validation loss decreased (287.111938 --> 285.297424).\n",
            "\t Train_Loss: 113.2277 Val_Loss: 285.2974  BEST VAL Loss: 285.2974\n",
            "\n",
            "Epoch 1190: Validation loss decreased (285.297424 --> 283.579193).\n",
            "\t Train_Loss: 113.0126 Val_Loss: 283.5792  BEST VAL Loss: 283.5792\n",
            "\n",
            "Epoch 1191: Validation loss decreased (283.579193 --> 282.383728).\n",
            "\t Train_Loss: 112.9372 Val_Loss: 282.3837  BEST VAL Loss: 282.3837\n",
            "\n",
            "Epoch 1192: Validation loss decreased (282.383728 --> 281.783234).\n",
            "\t Train_Loss: 112.9231 Val_Loss: 281.7832  BEST VAL Loss: 281.7832\n",
            "\n",
            "Epoch 1193: Validation loss decreased (281.783234 --> 281.703888).\n",
            "\t Train_Loss: 112.8269 Val_Loss: 281.7039  BEST VAL Loss: 281.7039\n",
            "\n",
            "Epoch 1194: Validation loss did not decrease\n",
            "\t Train_Loss: 112.6581 Val_Loss: 281.9529  BEST VAL Loss: 281.7039\n",
            "\n",
            "Epoch 1195: Validation loss did not decrease\n",
            "\t Train_Loss: 112.5329 Val_Loss: 282.1645  BEST VAL Loss: 281.7039\n",
            "\n",
            "Epoch 1196: Validation loss did not decrease\n",
            "\t Train_Loss: 112.4870 Val_Loss: 281.9370  BEST VAL Loss: 281.7039\n",
            "\n",
            "Epoch 1197: Validation loss decreased (281.703888 --> 281.127411).\n",
            "\t Train_Loss: 112.4360 Val_Loss: 281.1274  BEST VAL Loss: 281.1274\n",
            "\n",
            "Epoch 1198: Validation loss decreased (281.127411 --> 279.954590).\n",
            "\t Train_Loss: 112.3220 Val_Loss: 279.9546  BEST VAL Loss: 279.9546\n",
            "\n",
            "Epoch 1199: Validation loss decreased (279.954590 --> 278.784332).\n",
            "\t Train_Loss: 112.1952 Val_Loss: 278.7843  BEST VAL Loss: 278.7843\n",
            "\n",
            "Epoch 1200: Validation loss decreased (278.784332 --> 277.880707).\n",
            "\t Train_Loss: 112.1161 Val_Loss: 277.8807  BEST VAL Loss: 277.8807\n",
            "\n",
            "Epoch 1201: Validation loss decreased (277.880707 --> 277.338287).\n",
            "\t Train_Loss: 112.0663 Val_Loss: 277.3383  BEST VAL Loss: 277.3383\n",
            "\n",
            "Epoch 1202: Validation loss decreased (277.338287 --> 277.130585).\n",
            "\t Train_Loss: 111.9927 Val_Loss: 277.1306  BEST VAL Loss: 277.1306\n",
            "\n",
            "Epoch 1203: Validation loss did not decrease\n",
            "\t Train_Loss: 111.8885 Val_Loss: 277.1376  BEST VAL Loss: 277.1306\n",
            "\n",
            "Epoch 1204: Validation loss did not decrease\n",
            "\t Train_Loss: 111.7925 Val_Loss: 277.1570  BEST VAL Loss: 277.1306\n",
            "\n",
            "Epoch 1205: Validation loss decreased (277.130585 --> 276.973480).\n",
            "\t Train_Loss: 111.7266 Val_Loss: 276.9735  BEST VAL Loss: 276.9735\n",
            "\n",
            "Epoch 1206: Validation loss decreased (276.973480 --> 276.478058).\n",
            "\t Train_Loss: 111.6678 Val_Loss: 276.4781  BEST VAL Loss: 276.4781\n",
            "\n",
            "Epoch 1207: Validation loss decreased (276.478058 --> 275.735077).\n",
            "\t Train_Loss: 111.5891 Val_Loss: 275.7351  BEST VAL Loss: 275.7351\n",
            "\n",
            "Epoch 1208: Validation loss decreased (275.735077 --> 274.920532).\n",
            "\t Train_Loss: 111.4983 Val_Loss: 274.9205  BEST VAL Loss: 274.9205\n",
            "\n",
            "Epoch 1209: Validation loss decreased (274.920532 --> 274.206146).\n",
            "\t Train_Loss: 111.4198 Val_Loss: 274.2061  BEST VAL Loss: 274.2061\n",
            "\n",
            "Epoch 1210: Validation loss decreased (274.206146 --> 273.689545).\n",
            "\t Train_Loss: 111.3572 Val_Loss: 273.6895  BEST VAL Loss: 273.6895\n",
            "\n",
            "Epoch 1211: Validation loss decreased (273.689545 --> 273.387054).\n",
            "\t Train_Loss: 111.2928 Val_Loss: 273.3871  BEST VAL Loss: 273.3871\n",
            "\n",
            "Epoch 1212: Validation loss decreased (273.387054 --> 273.246094).\n",
            "\t Train_Loss: 111.2163 Val_Loss: 273.2461  BEST VAL Loss: 273.2461\n",
            "\n",
            "Epoch 1213: Validation loss decreased (273.246094 --> 273.162689).\n",
            "\t Train_Loss: 111.1366 Val_Loss: 273.1627  BEST VAL Loss: 273.1627\n",
            "\n",
            "Epoch 1214: Validation loss decreased (273.162689 --> 273.015350).\n",
            "\t Train_Loss: 111.0662 Val_Loss: 273.0154  BEST VAL Loss: 273.0154\n",
            "\n",
            "Epoch 1215: Validation loss decreased (273.015350 --> 272.716248).\n",
            "\t Train_Loss: 111.0036 Val_Loss: 272.7162  BEST VAL Loss: 272.7162\n",
            "\n",
            "Epoch 1216: Validation loss decreased (272.716248 --> 272.255219).\n",
            "\t Train_Loss: 110.9381 Val_Loss: 272.2552  BEST VAL Loss: 272.2552\n",
            "\n",
            "Epoch 1217: Validation loss decreased (272.255219 --> 271.697723).\n",
            "\t Train_Loss: 110.8658 Val_Loss: 271.6977  BEST VAL Loss: 271.6977\n",
            "\n",
            "Epoch 1218: Validation loss decreased (271.697723 --> 271.141083).\n",
            "\t Train_Loss: 110.7937 Val_Loss: 271.1411  BEST VAL Loss: 271.1411\n",
            "\n",
            "Epoch 1219: Validation loss decreased (271.141083 --> 270.666992).\n",
            "\t Train_Loss: 110.7277 Val_Loss: 270.6670  BEST VAL Loss: 270.6670\n",
            "\n",
            "Epoch 1220: Validation loss decreased (270.666992 --> 270.316162).\n",
            "\t Train_Loss: 110.6655 Val_Loss: 270.3162  BEST VAL Loss: 270.3162\n",
            "\n",
            "Epoch 1221: Validation loss decreased (270.316162 --> 270.082489).\n",
            "\t Train_Loss: 110.6009 Val_Loss: 270.0825  BEST VAL Loss: 270.0825\n",
            "\n",
            "Epoch 1222: Validation loss decreased (270.082489 --> 269.923492).\n",
            "\t Train_Loss: 110.5329 Val_Loss: 269.9235  BEST VAL Loss: 269.9235\n",
            "\n",
            "Epoch 1223: Validation loss decreased (269.923492 --> 269.774139).\n",
            "\t Train_Loss: 110.4658 Val_Loss: 269.7741  BEST VAL Loss: 269.7741\n",
            "\n",
            "Epoch 1224: Validation loss decreased (269.774139 --> 269.572754).\n",
            "\t Train_Loss: 110.4026 Val_Loss: 269.5728  BEST VAL Loss: 269.5728\n",
            "\n",
            "Epoch 1225: Validation loss decreased (269.572754 --> 269.283905).\n",
            "\t Train_Loss: 110.3413 Val_Loss: 269.2839  BEST VAL Loss: 269.2839\n",
            "\n",
            "Epoch 1226: Validation loss decreased (269.283905 --> 268.913361).\n",
            "\t Train_Loss: 110.2786 Val_Loss: 268.9134  BEST VAL Loss: 268.9134\n",
            "\n",
            "Epoch 1227: Validation loss decreased (268.913361 --> 268.501068).\n",
            "\t Train_Loss: 110.2142 Val_Loss: 268.5011  BEST VAL Loss: 268.5011\n",
            "\n",
            "Epoch 1228: Validation loss decreased (268.501068 --> 268.098877).\n",
            "\t Train_Loss: 110.1507 Val_Loss: 268.0989  BEST VAL Loss: 268.0989\n",
            "\n",
            "Epoch 1229: Validation loss decreased (268.098877 --> 267.748352).\n",
            "\t Train_Loss: 110.0896 Val_Loss: 267.7484  BEST VAL Loss: 267.7484\n",
            "\n",
            "Epoch 1230: Validation loss decreased (267.748352 --> 267.469086).\n",
            "\t Train_Loss: 110.0297 Val_Loss: 267.4691  BEST VAL Loss: 267.4691\n",
            "\n",
            "Epoch 1231: Validation loss decreased (267.469086 --> 267.255524).\n",
            "\t Train_Loss: 109.9690 Val_Loss: 267.2555  BEST VAL Loss: 267.2555\n",
            "\n",
            "Epoch 1232: Validation loss decreased (267.255524 --> 267.080841).\n",
            "\t Train_Loss: 109.9076 Val_Loss: 267.0808  BEST VAL Loss: 267.0808\n",
            "\n",
            "Epoch 1233: Validation loss decreased (267.080841 --> 266.908905).\n",
            "\t Train_Loss: 109.8468 Val_Loss: 266.9089  BEST VAL Loss: 266.9089\n",
            "\n",
            "Epoch 1234: Validation loss decreased (266.908905 --> 266.706909).\n",
            "\t Train_Loss: 109.7875 Val_Loss: 266.7069  BEST VAL Loss: 266.7069\n",
            "\n",
            "Epoch 1235: Validation loss decreased (266.706909 --> 266.457855).\n",
            "\t Train_Loss: 109.7290 Val_Loss: 266.4579  BEST VAL Loss: 266.4579\n",
            "\n",
            "Epoch 1236: Validation loss decreased (266.457855 --> 266.165619).\n",
            "\t Train_Loss: 109.6703 Val_Loss: 266.1656  BEST VAL Loss: 266.1656\n",
            "\n",
            "Epoch 1237: Validation loss decreased (266.165619 --> 265.851318).\n",
            "\t Train_Loss: 109.6111 Val_Loss: 265.8513  BEST VAL Loss: 265.8513\n",
            "\n",
            "Epoch 1238: Validation loss decreased (265.851318 --> 265.542511).\n",
            "\t Train_Loss: 109.5525 Val_Loss: 265.5425  BEST VAL Loss: 265.5425\n",
            "\n",
            "Epoch 1239: Validation loss decreased (265.542511 --> 265.262939).\n",
            "\t Train_Loss: 109.4948 Val_Loss: 265.2629  BEST VAL Loss: 265.2629\n",
            "\n",
            "Epoch 1240: Validation loss decreased (265.262939 --> 265.024170).\n",
            "\t Train_Loss: 109.4376 Val_Loss: 265.0242  BEST VAL Loss: 265.0242\n",
            "\n",
            "Epoch 1241: Validation loss decreased (265.024170 --> 264.823486).\n",
            "\t Train_Loss: 109.3805 Val_Loss: 264.8235  BEST VAL Loss: 264.8235\n",
            "\n",
            "Epoch 1242: Validation loss decreased (264.823486 --> 264.646484).\n",
            "\t Train_Loss: 109.3232 Val_Loss: 264.6465  BEST VAL Loss: 264.6465\n",
            "\n",
            "Epoch 1243: Validation loss decreased (264.646484 --> 264.474487).\n",
            "\t Train_Loss: 109.2662 Val_Loss: 264.4745  BEST VAL Loss: 264.4745\n",
            "\n",
            "Epoch 1244: Validation loss decreased (264.474487 --> 264.288727).\n",
            "\t Train_Loss: 109.2098 Val_Loss: 264.2887  BEST VAL Loss: 264.2887\n",
            "\n",
            "Epoch 1245: Validation loss decreased (264.288727 --> 264.078857).\n",
            "\t Train_Loss: 109.1539 Val_Loss: 264.0789  BEST VAL Loss: 264.0789\n",
            "\n",
            "Epoch 1246: Validation loss decreased (264.078857 --> 263.844208).\n",
            "\t Train_Loss: 109.0981 Val_Loss: 263.8442  BEST VAL Loss: 263.8442\n",
            "\n",
            "Epoch 1247: Validation loss decreased (263.844208 --> 263.596893).\n",
            "\t Train_Loss: 109.0422 Val_Loss: 263.5969  BEST VAL Loss: 263.5969\n",
            "\n",
            "Epoch 1248: Validation loss decreased (263.596893 --> 263.349915).\n",
            "\t Train_Loss: 108.9865 Val_Loss: 263.3499  BEST VAL Loss: 263.3499\n",
            "\n",
            "Epoch 1249: Validation loss decreased (263.349915 --> 263.117950).\n",
            "\t Train_Loss: 108.9313 Val_Loss: 263.1180  BEST VAL Loss: 263.1180\n",
            "\n",
            "Epoch 1250: Validation loss decreased (263.117950 --> 262.908417).\n",
            "\t Train_Loss: 108.8764 Val_Loss: 262.9084  BEST VAL Loss: 262.9084\n",
            "\n",
            "Epoch 1251: Validation loss decreased (262.908417 --> 262.721985).\n",
            "\t Train_Loss: 108.8217 Val_Loss: 262.7220  BEST VAL Loss: 262.7220\n",
            "\n",
            "Epoch 1252: Validation loss decreased (262.721985 --> 262.552948).\n",
            "\t Train_Loss: 108.7669 Val_Loss: 262.5529  BEST VAL Loss: 262.5529\n",
            "\n",
            "Epoch 1253: Validation loss decreased (262.552948 --> 262.389801).\n",
            "\t Train_Loss: 108.7123 Val_Loss: 262.3898  BEST VAL Loss: 262.3898\n",
            "\n",
            "Epoch 1254: Validation loss decreased (262.389801 --> 262.222473).\n",
            "\t Train_Loss: 108.6581 Val_Loss: 262.2225  BEST VAL Loss: 262.2225\n",
            "\n",
            "Epoch 1255: Validation loss decreased (262.222473 --> 262.043701).\n",
            "\t Train_Loss: 108.6041 Val_Loss: 262.0437  BEST VAL Loss: 262.0437\n",
            "\n",
            "Epoch 1256: Validation loss decreased (262.043701 --> 261.851624).\n",
            "\t Train_Loss: 108.5502 Val_Loss: 261.8516  BEST VAL Loss: 261.8516\n",
            "\n",
            "Epoch 1257: Validation loss decreased (261.851624 --> 261.650696).\n",
            "\t Train_Loss: 108.4965 Val_Loss: 261.6507  BEST VAL Loss: 261.6507\n",
            "\n",
            "Epoch 1258: Validation loss decreased (261.650696 --> 261.447754).\n",
            "\t Train_Loss: 108.4428 Val_Loss: 261.4478  BEST VAL Loss: 261.4478\n",
            "\n",
            "Epoch 1259: Validation loss decreased (261.447754 --> 261.251740).\n",
            "\t Train_Loss: 108.3894 Val_Loss: 261.2517  BEST VAL Loss: 261.2517\n",
            "\n",
            "Epoch 1260: Validation loss decreased (261.251740 --> 261.067719).\n",
            "\t Train_Loss: 108.3362 Val_Loss: 261.0677  BEST VAL Loss: 261.0677\n",
            "\n",
            "Epoch 1261: Validation loss decreased (261.067719 --> 260.897949).\n",
            "\t Train_Loss: 108.2831 Val_Loss: 260.8979  BEST VAL Loss: 260.8979\n",
            "\n",
            "Epoch 1262: Validation loss decreased (260.897949 --> 260.740204).\n",
            "\t Train_Loss: 108.2302 Val_Loss: 260.7402  BEST VAL Loss: 260.7402\n",
            "\n",
            "Epoch 1263: Validation loss decreased (260.740204 --> 260.588806).\n",
            "\t Train_Loss: 108.1773 Val_Loss: 260.5888  BEST VAL Loss: 260.5888\n",
            "\n",
            "Epoch 1264: Validation loss decreased (260.588806 --> 260.438202).\n",
            "\t Train_Loss: 108.1247 Val_Loss: 260.4382  BEST VAL Loss: 260.4382\n",
            "\n",
            "Epoch 1265: Validation loss decreased (260.438202 --> 260.283234).\n",
            "\t Train_Loss: 108.0722 Val_Loss: 260.2832  BEST VAL Loss: 260.2832\n",
            "\n",
            "Epoch 1266: Validation loss decreased (260.283234 --> 260.121429).\n",
            "\t Train_Loss: 108.0198 Val_Loss: 260.1214  BEST VAL Loss: 260.1214\n",
            "\n",
            "Epoch 1267: Validation loss decreased (260.121429 --> 259.954498).\n",
            "\t Train_Loss: 107.9675 Val_Loss: 259.9545  BEST VAL Loss: 259.9545\n",
            "\n",
            "Epoch 1268: Validation loss decreased (259.954498 --> 259.784576).\n",
            "\t Train_Loss: 107.9153 Val_Loss: 259.7846  BEST VAL Loss: 259.7846\n",
            "\n",
            "Epoch 1269: Validation loss decreased (259.784576 --> 259.616974).\n",
            "\t Train_Loss: 107.8633 Val_Loss: 259.6170  BEST VAL Loss: 259.6170\n",
            "\n",
            "Epoch 1270: Validation loss decreased (259.616974 --> 259.455780).\n",
            "\t Train_Loss: 107.8115 Val_Loss: 259.4558  BEST VAL Loss: 259.4558\n",
            "\n",
            "Epoch 1271: Validation loss decreased (259.455780 --> 259.302704).\n",
            "\t Train_Loss: 107.7597 Val_Loss: 259.3027  BEST VAL Loss: 259.3027\n",
            "\n",
            "Epoch 1272: Validation loss decreased (259.302704 --> 259.157684).\n",
            "\t Train_Loss: 107.7081 Val_Loss: 259.1577  BEST VAL Loss: 259.1577\n",
            "\n",
            "Epoch 1273: Validation loss decreased (259.157684 --> 259.018646).\n",
            "\t Train_Loss: 107.6565 Val_Loss: 259.0186  BEST VAL Loss: 259.0186\n",
            "\n",
            "Epoch 1274: Validation loss decreased (259.018646 --> 258.882233).\n",
            "\t Train_Loss: 107.6051 Val_Loss: 258.8822  BEST VAL Loss: 258.8822\n",
            "\n",
            "Epoch 1275: Validation loss decreased (258.882233 --> 258.745056).\n",
            "\t Train_Loss: 107.5538 Val_Loss: 258.7451  BEST VAL Loss: 258.7451\n",
            "\n",
            "Epoch 1276: Validation loss decreased (258.745056 --> 258.605286).\n",
            "\t Train_Loss: 107.5026 Val_Loss: 258.6053  BEST VAL Loss: 258.6053\n",
            "\n",
            "Epoch 1277: Validation loss decreased (258.605286 --> 258.462311).\n",
            "\t Train_Loss: 107.4515 Val_Loss: 258.4623  BEST VAL Loss: 258.4623\n",
            "\n",
            "Epoch 1278: Validation loss decreased (258.462311 --> 258.317444).\n",
            "\t Train_Loss: 107.4006 Val_Loss: 258.3174  BEST VAL Loss: 258.3174\n",
            "\n",
            "Epoch 1279: Validation loss decreased (258.317444 --> 258.172821).\n",
            "\t Train_Loss: 107.3497 Val_Loss: 258.1728  BEST VAL Loss: 258.1728\n",
            "\n",
            "Epoch 1280: Validation loss decreased (258.172821 --> 258.031006).\n",
            "\t Train_Loss: 107.2989 Val_Loss: 258.0310  BEST VAL Loss: 258.0310\n",
            "\n",
            "Epoch 1281: Validation loss decreased (258.031006 --> 257.893646).\n",
            "\t Train_Loss: 107.2482 Val_Loss: 257.8936  BEST VAL Loss: 257.8936\n",
            "\n",
            "Epoch 1282: Validation loss decreased (257.893646 --> 257.761017).\n",
            "\t Train_Loss: 107.1976 Val_Loss: 257.7610  BEST VAL Loss: 257.7610\n",
            "\n",
            "Epoch 1283: Validation loss decreased (257.761017 --> 257.633240).\n",
            "\t Train_Loss: 107.1471 Val_Loss: 257.6332  BEST VAL Loss: 257.6332\n",
            "\n",
            "Epoch 1284: Validation loss decreased (257.633240 --> 257.508789).\n",
            "\t Train_Loss: 107.0967 Val_Loss: 257.5088  BEST VAL Loss: 257.5088\n",
            "\n",
            "Epoch 1285: Validation loss decreased (257.508789 --> 257.384827).\n",
            "\t Train_Loss: 107.0464 Val_Loss: 257.3848  BEST VAL Loss: 257.3848\n",
            "\n",
            "Epoch 1286: Validation loss decreased (257.384827 --> 257.261200).\n",
            "\t Train_Loss: 106.9962 Val_Loss: 257.2612  BEST VAL Loss: 257.2612\n",
            "\n",
            "Epoch 1287: Validation loss decreased (257.261200 --> 257.135468).\n",
            "\t Train_Loss: 106.9461 Val_Loss: 257.1355  BEST VAL Loss: 257.1355\n",
            "\n",
            "Epoch 1288: Validation loss decreased (257.135468 --> 257.009003).\n",
            "\t Train_Loss: 106.8960 Val_Loss: 257.0090  BEST VAL Loss: 257.0090\n",
            "\n",
            "Epoch 1289: Validation loss decreased (257.009003 --> 256.882111).\n",
            "\t Train_Loss: 106.8460 Val_Loss: 256.8821  BEST VAL Loss: 256.8821\n",
            "\n",
            "Epoch 1290: Validation loss decreased (256.882111 --> 256.755707).\n",
            "\t Train_Loss: 106.7962 Val_Loss: 256.7557  BEST VAL Loss: 256.7557\n",
            "\n",
            "Epoch 1291: Validation loss decreased (256.755707 --> 256.631866).\n",
            "\t Train_Loss: 106.7464 Val_Loss: 256.6319  BEST VAL Loss: 256.6319\n",
            "\n",
            "Epoch 1292: Validation loss decreased (256.631866 --> 256.511322).\n",
            "\t Train_Loss: 106.6966 Val_Loss: 256.5113  BEST VAL Loss: 256.5113\n",
            "\n",
            "Epoch 1293: Validation loss decreased (256.511322 --> 256.393646).\n",
            "\t Train_Loss: 106.6470 Val_Loss: 256.3936  BEST VAL Loss: 256.3936\n",
            "\n",
            "Epoch 1294: Validation loss decreased (256.393646 --> 256.278625).\n",
            "\t Train_Loss: 106.5974 Val_Loss: 256.2786  BEST VAL Loss: 256.2786\n",
            "\n",
            "Epoch 1295: Validation loss decreased (256.278625 --> 256.164825).\n",
            "\t Train_Loss: 106.5480 Val_Loss: 256.1648  BEST VAL Loss: 256.1648\n",
            "\n",
            "Epoch 1296: Validation loss decreased (256.164825 --> 256.052307).\n",
            "\t Train_Loss: 106.4985 Val_Loss: 256.0523  BEST VAL Loss: 256.0523\n",
            "\n",
            "Epoch 1297: Validation loss decreased (256.052307 --> 255.939194).\n",
            "\t Train_Loss: 106.4492 Val_Loss: 255.9392  BEST VAL Loss: 255.9392\n",
            "\n",
            "Epoch 1298: Validation loss decreased (255.939194 --> 255.826218).\n",
            "\t Train_Loss: 106.3999 Val_Loss: 255.8262  BEST VAL Loss: 255.8262\n",
            "\n",
            "Epoch 1299: Validation loss decreased (255.826218 --> 255.711945).\n",
            "\t Train_Loss: 106.3507 Val_Loss: 255.7119  BEST VAL Loss: 255.7119\n",
            "\n",
            "Epoch 1300: Validation loss decreased (255.711945 --> 255.598343).\n",
            "\t Train_Loss: 106.3016 Val_Loss: 255.5983  BEST VAL Loss: 255.5983\n",
            "\n",
            "Epoch 1301: Validation loss decreased (255.598343 --> 255.486069).\n",
            "\t Train_Loss: 106.2525 Val_Loss: 255.4861  BEST VAL Loss: 255.4861\n",
            "\n",
            "Epoch 1302: Validation loss decreased (255.486069 --> 255.374832).\n",
            "\t Train_Loss: 106.2035 Val_Loss: 255.3748  BEST VAL Loss: 255.3748\n",
            "\n",
            "Epoch 1303: Validation loss decreased (255.374832 --> 255.265854).\n",
            "\t Train_Loss: 106.1546 Val_Loss: 255.2659  BEST VAL Loss: 255.2659\n",
            "\n",
            "Epoch 1304: Validation loss decreased (255.265854 --> 255.158478).\n",
            "\t Train_Loss: 106.1057 Val_Loss: 255.1585  BEST VAL Loss: 255.1585\n",
            "\n",
            "Epoch 1305: Validation loss decreased (255.158478 --> 255.052979).\n",
            "\t Train_Loss: 106.0570 Val_Loss: 255.0530  BEST VAL Loss: 255.0530\n",
            "\n",
            "Epoch 1306: Validation loss decreased (255.052979 --> 254.948471).\n",
            "\t Train_Loss: 106.0082 Val_Loss: 254.9485  BEST VAL Loss: 254.9485\n",
            "\n",
            "Epoch 1307: Validation loss decreased (254.948471 --> 254.844223).\n",
            "\t Train_Loss: 105.9596 Val_Loss: 254.8442  BEST VAL Loss: 254.8442\n",
            "\n",
            "Epoch 1308: Validation loss decreased (254.844223 --> 254.740097).\n",
            "\t Train_Loss: 105.9110 Val_Loss: 254.7401  BEST VAL Loss: 254.7401\n",
            "\n",
            "Epoch 1309: Validation loss decreased (254.740097 --> 254.635788).\n",
            "\t Train_Loss: 105.8624 Val_Loss: 254.6358  BEST VAL Loss: 254.6358\n",
            "\n",
            "Epoch 1310: Validation loss decreased (254.635788 --> 254.531403).\n",
            "\t Train_Loss: 105.8140 Val_Loss: 254.5314  BEST VAL Loss: 254.5314\n",
            "\n",
            "Epoch 1311: Validation loss decreased (254.531403 --> 254.427658).\n",
            "\t Train_Loss: 105.7655 Val_Loss: 254.4277  BEST VAL Loss: 254.4277\n",
            "\n",
            "Epoch 1312: Validation loss decreased (254.427658 --> 254.324585).\n",
            "\t Train_Loss: 105.7172 Val_Loss: 254.3246  BEST VAL Loss: 254.3246\n",
            "\n",
            "Epoch 1313: Validation loss decreased (254.324585 --> 254.222565).\n",
            "\t Train_Loss: 105.6689 Val_Loss: 254.2226  BEST VAL Loss: 254.2226\n",
            "\n",
            "Epoch 1314: Validation loss decreased (254.222565 --> 254.121857).\n",
            "\t Train_Loss: 105.6206 Val_Loss: 254.1219  BEST VAL Loss: 254.1219\n",
            "\n",
            "Epoch 1315: Validation loss decreased (254.121857 --> 254.022247).\n",
            "\t Train_Loss: 105.5724 Val_Loss: 254.0222  BEST VAL Loss: 254.0222\n",
            "\n",
            "Epoch 1316: Validation loss decreased (254.022247 --> 253.923782).\n",
            "\t Train_Loss: 105.5243 Val_Loss: 253.9238  BEST VAL Loss: 253.9238\n",
            "\n",
            "Epoch 1317: Validation loss decreased (253.923782 --> 253.825638).\n",
            "\t Train_Loss: 105.4762 Val_Loss: 253.8256  BEST VAL Loss: 253.8256\n",
            "\n",
            "Epoch 1318: Validation loss decreased (253.825638 --> 253.727905).\n",
            "\t Train_Loss: 105.4281 Val_Loss: 253.7279  BEST VAL Loss: 253.7279\n",
            "\n",
            "Epoch 1319: Validation loss decreased (253.727905 --> 253.630569).\n",
            "\t Train_Loss: 105.3802 Val_Loss: 253.6306  BEST VAL Loss: 253.6306\n",
            "\n",
            "Epoch 1320: Validation loss decreased (253.630569 --> 253.533356).\n",
            "\t Train_Loss: 105.3323 Val_Loss: 253.5334  BEST VAL Loss: 253.5334\n",
            "\n",
            "Epoch 1321: Validation loss decreased (253.533356 --> 253.436188).\n",
            "\t Train_Loss: 105.2844 Val_Loss: 253.4362  BEST VAL Loss: 253.4362\n",
            "\n",
            "Epoch 1322: Validation loss decreased (253.436188 --> 253.339233).\n",
            "\t Train_Loss: 105.2366 Val_Loss: 253.3392  BEST VAL Loss: 253.3392\n",
            "\n",
            "Epoch 1323: Validation loss decreased (253.339233 --> 253.242599).\n",
            "\t Train_Loss: 105.1888 Val_Loss: 253.2426  BEST VAL Loss: 253.2426\n",
            "\n",
            "Epoch 1324: Validation loss decreased (253.242599 --> 253.147324).\n",
            "\t Train_Loss: 105.1411 Val_Loss: 253.1473  BEST VAL Loss: 253.1473\n",
            "\n",
            "Epoch 1325: Validation loss decreased (253.147324 --> 253.052643).\n",
            "\t Train_Loss: 105.0935 Val_Loss: 253.0526  BEST VAL Loss: 253.0526\n",
            "\n",
            "Epoch 1326: Validation loss decreased (253.052643 --> 252.958694).\n",
            "\t Train_Loss: 105.0459 Val_Loss: 252.9587  BEST VAL Loss: 252.9587\n",
            "\n",
            "Epoch 1327: Validation loss decreased (252.958694 --> 252.865479).\n",
            "\t Train_Loss: 104.9983 Val_Loss: 252.8655  BEST VAL Loss: 252.8655\n",
            "\n",
            "Epoch 1328: Validation loss decreased (252.865479 --> 252.772507).\n",
            "\t Train_Loss: 104.9508 Val_Loss: 252.7725  BEST VAL Loss: 252.7725\n",
            "\n",
            "Epoch 1329: Validation loss decreased (252.772507 --> 252.679977).\n",
            "\t Train_Loss: 104.9033 Val_Loss: 252.6800  BEST VAL Loss: 252.6800\n",
            "\n",
            "Epoch 1330: Validation loss decreased (252.679977 --> 252.587601).\n",
            "\t Train_Loss: 104.8559 Val_Loss: 252.5876  BEST VAL Loss: 252.5876\n",
            "\n",
            "Epoch 1331: Validation loss decreased (252.587601 --> 252.495239).\n",
            "\t Train_Loss: 104.8085 Val_Loss: 252.4952  BEST VAL Loss: 252.4952\n",
            "\n",
            "Epoch 1332: Validation loss decreased (252.495239 --> 252.403244).\n",
            "\t Train_Loss: 104.7612 Val_Loss: 252.4032  BEST VAL Loss: 252.4032\n",
            "\n",
            "Epoch 1333: Validation loss decreased (252.403244 --> 252.311279).\n",
            "\t Train_Loss: 104.7140 Val_Loss: 252.3113  BEST VAL Loss: 252.3113\n",
            "\n",
            "Epoch 1334: Validation loss decreased (252.311279 --> 252.219879).\n",
            "\t Train_Loss: 104.6667 Val_Loss: 252.2199  BEST VAL Loss: 252.2199\n",
            "\n",
            "Epoch 1335: Validation loss decreased (252.219879 --> 252.129257).\n",
            "\t Train_Loss: 104.6195 Val_Loss: 252.1293  BEST VAL Loss: 252.1293\n",
            "\n",
            "Epoch 1336: Validation loss decreased (252.129257 --> 252.038818).\n",
            "\t Train_Loss: 104.5724 Val_Loss: 252.0388  BEST VAL Loss: 252.0388\n",
            "\n",
            "Epoch 1337: Validation loss decreased (252.038818 --> 251.949265).\n",
            "\t Train_Loss: 104.5253 Val_Loss: 251.9493  BEST VAL Loss: 251.9493\n",
            "\n",
            "Epoch 1338: Validation loss decreased (251.949265 --> 251.859818).\n",
            "\t Train_Loss: 104.4782 Val_Loss: 251.8598  BEST VAL Loss: 251.8598\n",
            "\n",
            "Epoch 1339: Validation loss decreased (251.859818 --> 251.770859).\n",
            "\t Train_Loss: 104.4312 Val_Loss: 251.7709  BEST VAL Loss: 251.7709\n",
            "\n",
            "Epoch 1340: Validation loss decreased (251.770859 --> 251.681931).\n",
            "\t Train_Loss: 104.3843 Val_Loss: 251.6819  BEST VAL Loss: 251.6819\n",
            "\n",
            "Epoch 1341: Validation loss decreased (251.681931 --> 251.593552).\n",
            "\t Train_Loss: 104.3373 Val_Loss: 251.5936  BEST VAL Loss: 251.5936\n",
            "\n",
            "Epoch 1342: Validation loss decreased (251.593552 --> 251.505081).\n",
            "\t Train_Loss: 104.2904 Val_Loss: 251.5051  BEST VAL Loss: 251.5051\n",
            "\n",
            "Epoch 1343: Validation loss decreased (251.505081 --> 251.416656).\n",
            "\t Train_Loss: 104.2436 Val_Loss: 251.4167  BEST VAL Loss: 251.4167\n",
            "\n",
            "Epoch 1344: Validation loss decreased (251.416656 --> 251.328735).\n",
            "\t Train_Loss: 104.1968 Val_Loss: 251.3287  BEST VAL Loss: 251.3287\n",
            "\n",
            "Epoch 1345: Validation loss decreased (251.328735 --> 251.240997).\n",
            "\t Train_Loss: 104.1500 Val_Loss: 251.2410  BEST VAL Loss: 251.2410\n",
            "\n",
            "Epoch 1346: Validation loss decreased (251.240997 --> 251.153809).\n",
            "\t Train_Loss: 104.1033 Val_Loss: 251.1538  BEST VAL Loss: 251.1538\n",
            "\n",
            "Epoch 1347: Validation loss decreased (251.153809 --> 251.066757).\n",
            "\t Train_Loss: 104.0566 Val_Loss: 251.0668  BEST VAL Loss: 251.0668\n",
            "\n",
            "Epoch 1348: Validation loss decreased (251.066757 --> 250.980347).\n",
            "\t Train_Loss: 104.0100 Val_Loss: 250.9803  BEST VAL Loss: 250.9803\n",
            "\n",
            "Epoch 1349: Validation loss decreased (250.980347 --> 250.893845).\n",
            "\t Train_Loss: 103.9634 Val_Loss: 250.8938  BEST VAL Loss: 250.8938\n",
            "\n",
            "Epoch 1350: Validation loss decreased (250.893845 --> 250.807526).\n",
            "\t Train_Loss: 103.9168 Val_Loss: 250.8075  BEST VAL Loss: 250.8075\n",
            "\n",
            "Epoch 1351: Validation loss decreased (250.807526 --> 250.721680).\n",
            "\t Train_Loss: 103.8703 Val_Loss: 250.7217  BEST VAL Loss: 250.7217\n",
            "\n",
            "Epoch 1352: Validation loss decreased (250.721680 --> 250.635971).\n",
            "\t Train_Loss: 103.8238 Val_Loss: 250.6360  BEST VAL Loss: 250.6360\n",
            "\n",
            "Epoch 1353: Validation loss decreased (250.635971 --> 250.550690).\n",
            "\t Train_Loss: 103.7774 Val_Loss: 250.5507  BEST VAL Loss: 250.5507\n",
            "\n",
            "Epoch 1354: Validation loss decreased (250.550690 --> 250.465118).\n",
            "\t Train_Loss: 103.7309 Val_Loss: 250.4651  BEST VAL Loss: 250.4651\n",
            "\n",
            "Epoch 1355: Validation loss decreased (250.465118 --> 250.379868).\n",
            "\t Train_Loss: 103.6846 Val_Loss: 250.3799  BEST VAL Loss: 250.3799\n",
            "\n",
            "Epoch 1356: Validation loss decreased (250.379868 --> 250.294785).\n",
            "\t Train_Loss: 103.6382 Val_Loss: 250.2948  BEST VAL Loss: 250.2948\n",
            "\n",
            "Epoch 1357: Validation loss decreased (250.294785 --> 250.210159).\n",
            "\t Train_Loss: 103.5919 Val_Loss: 250.2102  BEST VAL Loss: 250.2102\n",
            "\n",
            "Epoch 1358: Validation loss decreased (250.210159 --> 250.125778).\n",
            "\t Train_Loss: 103.5456 Val_Loss: 250.1258  BEST VAL Loss: 250.1258\n",
            "\n",
            "Epoch 1359: Validation loss decreased (250.125778 --> 250.041534).\n",
            "\t Train_Loss: 103.4993 Val_Loss: 250.0415  BEST VAL Loss: 250.0415\n",
            "\n",
            "Epoch 1360: Validation loss decreased (250.041534 --> 249.957474).\n",
            "\t Train_Loss: 103.4531 Val_Loss: 249.9575  BEST VAL Loss: 249.9575\n",
            "\n",
            "Epoch 1361: Validation loss decreased (249.957474 --> 249.873642).\n",
            "\t Train_Loss: 103.4069 Val_Loss: 249.8736  BEST VAL Loss: 249.8736\n",
            "\n",
            "Epoch 1362: Validation loss decreased (249.873642 --> 249.789749).\n",
            "\t Train_Loss: 103.3608 Val_Loss: 249.7897  BEST VAL Loss: 249.7897\n",
            "\n",
            "Epoch 1363: Validation loss decreased (249.789749 --> 249.706161).\n",
            "\t Train_Loss: 103.3146 Val_Loss: 249.7062  BEST VAL Loss: 249.7062\n",
            "\n",
            "Epoch 1364: Validation loss decreased (249.706161 --> 249.622910).\n",
            "\t Train_Loss: 103.2685 Val_Loss: 249.6229  BEST VAL Loss: 249.6229\n",
            "\n",
            "Epoch 1365: Validation loss decreased (249.622910 --> 249.539658).\n",
            "\t Train_Loss: 103.2224 Val_Loss: 249.5397  BEST VAL Loss: 249.5397\n",
            "\n",
            "Epoch 1366: Validation loss decreased (249.539658 --> 249.456329).\n",
            "\t Train_Loss: 103.1763 Val_Loss: 249.4563  BEST VAL Loss: 249.4563\n",
            "\n",
            "Epoch 1367: Validation loss decreased (249.456329 --> 249.372925).\n",
            "\t Train_Loss: 103.1302 Val_Loss: 249.3729  BEST VAL Loss: 249.3729\n",
            "\n",
            "Epoch 1368: Validation loss decreased (249.372925 --> 249.290512).\n",
            "\t Train_Loss: 103.0840 Val_Loss: 249.2905  BEST VAL Loss: 249.2905\n",
            "\n",
            "Epoch 1369: Validation loss decreased (249.290512 --> 249.207718).\n",
            "\t Train_Loss: 103.0379 Val_Loss: 249.2077  BEST VAL Loss: 249.2077\n",
            "\n",
            "Epoch 1370: Validation loss decreased (249.207718 --> 249.125488).\n",
            "\t Train_Loss: 102.9916 Val_Loss: 249.1255  BEST VAL Loss: 249.1255\n",
            "\n",
            "Epoch 1371: Validation loss decreased (249.125488 --> 249.043167).\n",
            "\t Train_Loss: 102.9453 Val_Loss: 249.0432  BEST VAL Loss: 249.0432\n",
            "\n",
            "Epoch 1372: Validation loss decreased (249.043167 --> 248.961288).\n",
            "\t Train_Loss: 102.8987 Val_Loss: 248.9613  BEST VAL Loss: 248.9613\n",
            "\n",
            "Epoch 1373: Validation loss decreased (248.961288 --> 248.879150).\n",
            "\t Train_Loss: 102.8517 Val_Loss: 248.8792  BEST VAL Loss: 248.8792\n",
            "\n",
            "Epoch 1374: Validation loss decreased (248.879150 --> 248.797714).\n",
            "\t Train_Loss: 102.8039 Val_Loss: 248.7977  BEST VAL Loss: 248.7977\n",
            "\n",
            "Epoch 1375: Validation loss decreased (248.797714 --> 248.716171).\n",
            "\t Train_Loss: 102.7545 Val_Loss: 248.7162  BEST VAL Loss: 248.7162\n",
            "\n",
            "Epoch 1376: Validation loss decreased (248.716171 --> 248.637253).\n",
            "\t Train_Loss: 102.7009 Val_Loss: 248.6373  BEST VAL Loss: 248.6373\n",
            "\n",
            "Epoch 1377: Validation loss decreased (248.637253 --> 248.569901).\n",
            "\t Train_Loss: 102.6331 Val_Loss: 248.5699  BEST VAL Loss: 248.5699\n",
            "\n",
            "Epoch 1378: Validation loss did not decrease\n",
            "\t Train_Loss: 102.4798 Val_Loss: 249.0703  BEST VAL Loss: 248.5699\n",
            "\n",
            "Epoch 1379: Validation loss did not decrease\n",
            "\t Train_Loss: 100.1842 Val_Loss: 330.9984  BEST VAL Loss: 248.5699\n",
            "\n",
            "Epoch 1380: Validation loss did not decrease\n",
            "\t Train_Loss: 109.7756 Val_Loss: 264.5602  BEST VAL Loss: 248.5699\n",
            "\n",
            "Epoch 1381: Validation loss decreased (248.569901 --> 246.616013).\n",
            "\t Train_Loss: 93.7470 Val_Loss: 246.6160  BEST VAL Loss: 246.6160\n",
            "\n",
            "Epoch 1382: Validation loss decreased (246.616013 --> 245.916794).\n",
            "\t Train_Loss: 101.2629 Val_Loss: 245.9168  BEST VAL Loss: 245.9168\n",
            "\n",
            "Epoch 1383: Validation loss decreased (245.916794 --> 245.785934).\n",
            "\t Train_Loss: 102.4620 Val_Loss: 245.7859  BEST VAL Loss: 245.7859\n",
            "\n",
            "Epoch 1384: Validation loss did not decrease\n",
            "\t Train_Loss: 102.5045 Val_Loss: 246.0402  BEST VAL Loss: 245.7859\n",
            "\n",
            "Epoch 1385: Validation loss did not decrease\n",
            "\t Train_Loss: 102.3771 Val_Loss: 246.5697  BEST VAL Loss: 245.7859\n",
            "\n",
            "Epoch 1386: Validation loss did not decrease\n",
            "\t Train_Loss: 102.2695 Val_Loss: 247.1598  BEST VAL Loss: 245.7859\n",
            "\n",
            "Epoch 1387: Validation loss did not decrease\n",
            "\t Train_Loss: 102.2510 Val_Loss: 247.5242  BEST VAL Loss: 245.7859\n",
            "\n",
            "Epoch 1388: Validation loss did not decrease\n",
            "\t Train_Loss: 102.2754 Val_Loss: 247.4532  BEST VAL Loss: 245.7859\n",
            "\n",
            "Epoch 1389: Validation loss did not decrease\n",
            "\t Train_Loss: 102.2489 Val_Loss: 246.9415  BEST VAL Loss: 245.7859\n",
            "\n",
            "Epoch 1390: Validation loss did not decrease\n",
            "\t Train_Loss: 102.1525 Val_Loss: 246.1773  BEST VAL Loss: 245.7859\n",
            "\n",
            "Epoch 1391: Validation loss decreased (245.785934 --> 245.412109).\n",
            "\t Train_Loss: 102.0531 Val_Loss: 245.4121  BEST VAL Loss: 245.4121\n",
            "\n",
            "Epoch 1392: Validation loss decreased (245.412109 --> 244.831665).\n",
            "\t Train_Loss: 102.0057 Val_Loss: 244.8317  BEST VAL Loss: 244.8317\n",
            "\n",
            "Epoch 1393: Validation loss decreased (244.831665 --> 244.520187).\n",
            "\t Train_Loss: 101.9957 Val_Loss: 244.5202  BEST VAL Loss: 244.5202\n",
            "\n",
            "Epoch 1394: Validation loss decreased (244.520187 --> 244.483551).\n",
            "\t Train_Loss: 101.9756 Val_Loss: 244.4836  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1395: Validation loss did not decrease\n",
            "\t Train_Loss: 101.9200 Val_Loss: 244.6821  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1396: Validation loss did not decrease\n",
            "\t Train_Loss: 101.8425 Val_Loss: 245.0260  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1397: Validation loss did not decrease\n",
            "\t Train_Loss: 101.7766 Val_Loss: 245.3821  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1398: Validation loss did not decrease\n",
            "\t Train_Loss: 101.7399 Val_Loss: 245.6000  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1399: Validation loss did not decrease\n",
            "\t Train_Loss: 101.7165 Val_Loss: 245.5823  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1400: Validation loss did not decrease\n",
            "\t Train_Loss: 101.6776 Val_Loss: 245.3274  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1401: Validation loss did not decrease\n",
            "\t Train_Loss: 101.6171 Val_Loss: 244.9264  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1402: Validation loss did not decrease\n",
            "\t Train_Loss: 101.5549 Val_Loss: 244.5074  BEST VAL Loss: 244.4836\n",
            "\n",
            "Epoch 1403: Validation loss decreased (244.483551 --> 244.178329).\n",
            "\t Train_Loss: 101.5084 Val_Loss: 244.1783  BEST VAL Loss: 244.1783\n",
            "\n",
            "Epoch 1404: Validation loss decreased (244.178329 --> 244.000839).\n",
            "\t Train_Loss: 101.4744 Val_Loss: 244.0008  BEST VAL Loss: 244.0008\n",
            "\n",
            "Epoch 1405: Validation loss decreased (244.000839 --> 243.986359).\n",
            "\t Train_Loss: 101.4378 Val_Loss: 243.9864  BEST VAL Loss: 243.9864\n",
            "\n",
            "Epoch 1406: Validation loss did not decrease\n",
            "\t Train_Loss: 101.3896 Val_Loss: 244.1099  BEST VAL Loss: 243.9864\n",
            "\n",
            "Epoch 1407: Validation loss did not decrease\n",
            "\t Train_Loss: 101.3344 Val_Loss: 244.3101  BEST VAL Loss: 243.9864\n",
            "\n",
            "Epoch 1408: Validation loss did not decrease\n",
            "\t Train_Loss: 101.2832 Val_Loss: 244.5068  BEST VAL Loss: 243.9864\n",
            "\n",
            "Epoch 1409: Validation loss did not decrease\n",
            "\t Train_Loss: 101.2413 Val_Loss: 244.6208  BEST VAL Loss: 243.9864\n",
            "\n",
            "Epoch 1410: Validation loss did not decrease\n",
            "\t Train_Loss: 101.2033 Val_Loss: 244.6035  BEST VAL Loss: 243.9864\n",
            "\n",
            "Epoch 1411: Validation loss did not decrease\n",
            "\t Train_Loss: 101.1603 Val_Loss: 244.4567  BEST VAL Loss: 243.9864\n",
            "\n",
            "Epoch 1412: Validation loss did not decrease\n",
            "\t Train_Loss: 101.1108 Val_Loss: 244.2269  BEST VAL Loss: 243.9864\n",
            "\n",
            "Epoch 1413: Validation loss decreased (243.986359 --> 243.981644).\n",
            "\t Train_Loss: 101.0608 Val_Loss: 243.9816  BEST VAL Loss: 243.9816\n",
            "\n",
            "Epoch 1414: Validation loss decreased (243.981644 --> 243.783447).\n",
            "\t Train_Loss: 101.0156 Val_Loss: 243.7834  BEST VAL Loss: 243.7834\n",
            "\n",
            "Epoch 1415: Validation loss decreased (243.783447 --> 243.669800).\n",
            "\t Train_Loss: 100.9744 Val_Loss: 243.6698  BEST VAL Loss: 243.6698\n",
            "\n",
            "Epoch 1416: Validation loss decreased (243.669800 --> 243.650543).\n",
            "\t Train_Loss: 100.9325 Val_Loss: 243.6505  BEST VAL Loss: 243.6505\n",
            "\n",
            "Epoch 1417: Validation loss did not decrease\n",
            "\t Train_Loss: 100.8869 Val_Loss: 243.7089  BEST VAL Loss: 243.6505\n",
            "\n",
            "Epoch 1418: Validation loss did not decrease\n",
            "\t Train_Loss: 100.8392 Val_Loss: 243.8091  BEST VAL Loss: 243.6505\n",
            "\n",
            "Epoch 1419: Validation loss did not decrease\n",
            "\t Train_Loss: 100.7928 Val_Loss: 243.9030  BEST VAL Loss: 243.6505\n",
            "\n",
            "Epoch 1420: Validation loss did not decrease\n",
            "\t Train_Loss: 100.7493 Val_Loss: 243.9479  BEST VAL Loss: 243.6505\n",
            "\n",
            "Epoch 1421: Validation loss did not decrease\n",
            "\t Train_Loss: 100.7070 Val_Loss: 243.9193  BEST VAL Loss: 243.6505\n",
            "\n",
            "Epoch 1422: Validation loss did not decrease\n",
            "\t Train_Loss: 100.6631 Val_Loss: 243.8186  BEST VAL Loss: 243.6505\n",
            "\n",
            "Epoch 1423: Validation loss did not decrease\n",
            "\t Train_Loss: 100.6173 Val_Loss: 243.6708  BEST VAL Loss: 243.6505\n",
            "\n",
            "Epoch 1424: Validation loss decreased (243.650543 --> 243.512695).\n",
            "\t Train_Loss: 100.5713 Val_Loss: 243.5127  BEST VAL Loss: 243.5127\n",
            "\n",
            "Epoch 1425: Validation loss decreased (243.512695 --> 243.379593).\n",
            "\t Train_Loss: 100.5269 Val_Loss: 243.3796  BEST VAL Loss: 243.3796\n",
            "\n",
            "Epoch 1426: Validation loss decreased (243.379593 --> 243.293503).\n",
            "\t Train_Loss: 100.4837 Val_Loss: 243.2935  BEST VAL Loss: 243.2935\n",
            "\n",
            "Epoch 1427: Validation loss decreased (243.293503 --> 243.261383).\n",
            "\t Train_Loss: 100.4404 Val_Loss: 243.2614  BEST VAL Loss: 243.2614\n",
            "\n",
            "Epoch 1428: Validation loss did not decrease\n",
            "\t Train_Loss: 100.3958 Val_Loss: 243.2733  BEST VAL Loss: 243.2614\n",
            "\n",
            "Epoch 1429: Validation loss did not decrease\n",
            "\t Train_Loss: 100.3507 Val_Loss: 243.3072  BEST VAL Loss: 243.2614\n",
            "\n",
            "Epoch 1430: Validation loss did not decrease\n",
            "\t Train_Loss: 100.3059 Val_Loss: 243.3365  BEST VAL Loss: 243.2614\n",
            "\n",
            "Epoch 1431: Validation loss did not decrease\n",
            "\t Train_Loss: 100.2622 Val_Loss: 243.3373  BEST VAL Loss: 243.2614\n",
            "\n",
            "Epoch 1432: Validation loss did not decrease\n",
            "\t Train_Loss: 100.2188 Val_Loss: 243.2965  BEST VAL Loss: 243.2614\n",
            "\n",
            "Epoch 1433: Validation loss decreased (243.261383 --> 243.215729).\n",
            "\t Train_Loss: 100.1749 Val_Loss: 243.2157  BEST VAL Loss: 243.2157\n",
            "\n",
            "Epoch 1434: Validation loss decreased (243.215729 --> 243.107620).\n",
            "\t Train_Loss: 100.1305 Val_Loss: 243.1076  BEST VAL Loss: 243.1076\n",
            "\n",
            "Epoch 1435: Validation loss decreased (243.107620 --> 242.993484).\n",
            "\t Train_Loss: 100.0860 Val_Loss: 242.9935  BEST VAL Loss: 242.9935\n",
            "\n",
            "Epoch 1436: Validation loss decreased (242.993484 --> 242.892334).\n",
            "\t Train_Loss: 100.0420 Val_Loss: 242.8923  BEST VAL Loss: 242.8923\n",
            "\n",
            "Epoch 1437: Validation loss decreased (242.892334 --> 242.817535).\n",
            "\t Train_Loss: 99.9985 Val_Loss: 242.8175  BEST VAL Loss: 242.8175\n",
            "\n",
            "Epoch 1438: Validation loss decreased (242.817535 --> 242.773270).\n",
            "\t Train_Loss: 99.9548 Val_Loss: 242.7733  BEST VAL Loss: 242.7733\n",
            "\n",
            "Epoch 1439: Validation loss decreased (242.773270 --> 242.753647).\n",
            "\t Train_Loss: 99.9109 Val_Loss: 242.7536  BEST VAL Loss: 242.7536\n",
            "\n",
            "Epoch 1440: Validation loss decreased (242.753647 --> 242.746124).\n",
            "\t Train_Loss: 99.8668 Val_Loss: 242.7461  BEST VAL Loss: 242.7461\n",
            "\n",
            "Epoch 1441: Validation loss decreased (242.746124 --> 242.735550).\n",
            "\t Train_Loss: 99.8228 Val_Loss: 242.7355  BEST VAL Loss: 242.7355\n",
            "\n",
            "Epoch 1442: Validation loss decreased (242.735550 --> 242.708984).\n",
            "\t Train_Loss: 99.7792 Val_Loss: 242.7090  BEST VAL Loss: 242.7090\n",
            "\n",
            "Epoch 1443: Validation loss decreased (242.708984 --> 242.658600).\n",
            "\t Train_Loss: 99.7356 Val_Loss: 242.6586  BEST VAL Loss: 242.6586\n",
            "\n",
            "Epoch 1444: Validation loss decreased (242.658600 --> 242.585251).\n",
            "\t Train_Loss: 99.6920 Val_Loss: 242.5853  BEST VAL Loss: 242.5853\n",
            "\n",
            "Epoch 1445: Validation loss decreased (242.585251 --> 242.496811).\n",
            "\t Train_Loss: 99.6482 Val_Loss: 242.4968  BEST VAL Loss: 242.4968\n",
            "\n",
            "Epoch 1446: Validation loss decreased (242.496811 --> 242.404221).\n",
            "\t Train_Loss: 99.6044 Val_Loss: 242.4042  BEST VAL Loss: 242.4042\n",
            "\n",
            "Epoch 1447: Validation loss decreased (242.404221 --> 242.318848).\n",
            "\t Train_Loss: 99.5607 Val_Loss: 242.3188  BEST VAL Loss: 242.3188\n",
            "\n",
            "Epoch 1448: Validation loss decreased (242.318848 --> 242.248047).\n",
            "\t Train_Loss: 99.5173 Val_Loss: 242.2480  BEST VAL Loss: 242.2480\n",
            "\n",
            "Epoch 1449: Validation loss decreased (242.248047 --> 242.194565).\n",
            "\t Train_Loss: 99.4738 Val_Loss: 242.1946  BEST VAL Loss: 242.1946\n",
            "\n",
            "Epoch 1450: Validation loss decreased (242.194565 --> 242.154831).\n",
            "\t Train_Loss: 99.4303 Val_Loss: 242.1548  BEST VAL Loss: 242.1548\n",
            "\n",
            "Epoch 1451: Validation loss decreased (242.154831 --> 242.121902).\n",
            "\t Train_Loss: 99.3866 Val_Loss: 242.1219  BEST VAL Loss: 242.1219\n",
            "\n",
            "Epoch 1452: Validation loss decreased (242.121902 --> 242.087112).\n",
            "\t Train_Loss: 99.3431 Val_Loss: 242.0871  BEST VAL Loss: 242.0871\n",
            "\n",
            "Epoch 1453: Validation loss decreased (242.087112 --> 242.042770).\n",
            "\t Train_Loss: 99.2997 Val_Loss: 242.0428  BEST VAL Loss: 242.0428\n",
            "\n",
            "Epoch 1454: Validation loss decreased (242.042770 --> 241.985229).\n",
            "\t Train_Loss: 99.2563 Val_Loss: 241.9852  BEST VAL Loss: 241.9852\n",
            "\n",
            "Epoch 1455: Validation loss decreased (241.985229 --> 241.914658).\n",
            "\t Train_Loss: 99.2130 Val_Loss: 241.9147  BEST VAL Loss: 241.9147\n",
            "\n",
            "Epoch 1456: Validation loss decreased (241.914658 --> 241.835068).\n",
            "\t Train_Loss: 99.1695 Val_Loss: 241.8351  BEST VAL Loss: 241.8351\n",
            "\n",
            "Epoch 1457: Validation loss decreased (241.835068 --> 241.753464).\n",
            "\t Train_Loss: 99.1262 Val_Loss: 241.7535  BEST VAL Loss: 241.7535\n",
            "\n",
            "Epoch 1458: Validation loss decreased (241.753464 --> 241.675125).\n",
            "\t Train_Loss: 99.0828 Val_Loss: 241.6751  BEST VAL Loss: 241.6751\n",
            "\n",
            "Epoch 1459: Validation loss decreased (241.675125 --> 241.605515).\n",
            "\t Train_Loss: 99.0396 Val_Loss: 241.6055  BEST VAL Loss: 241.6055\n",
            "\n",
            "Epoch 1460: Validation loss decreased (241.605515 --> 241.545578).\n",
            "\t Train_Loss: 98.9963 Val_Loss: 241.5456  BEST VAL Loss: 241.5456\n",
            "\n",
            "Epoch 1461: Validation loss decreased (241.545578 --> 241.492828).\n",
            "\t Train_Loss: 98.9531 Val_Loss: 241.4928  BEST VAL Loss: 241.4928\n",
            "\n",
            "Epoch 1462: Validation loss decreased (241.492828 --> 241.443924).\n",
            "\t Train_Loss: 98.9098 Val_Loss: 241.4439  BEST VAL Loss: 241.4439\n",
            "\n",
            "Epoch 1463: Validation loss decreased (241.443924 --> 241.394043).\n",
            "\t Train_Loss: 98.8666 Val_Loss: 241.3940  BEST VAL Loss: 241.3940\n",
            "\n",
            "Epoch 1464: Validation loss decreased (241.394043 --> 241.338943).\n",
            "\t Train_Loss: 98.8234 Val_Loss: 241.3389  BEST VAL Loss: 241.3389\n",
            "\n",
            "Epoch 1465: Validation loss decreased (241.338943 --> 241.275681).\n",
            "\t Train_Loss: 98.7803 Val_Loss: 241.2757  BEST VAL Loss: 241.2757\n",
            "\n",
            "Epoch 1466: Validation loss decreased (241.275681 --> 241.205322).\n",
            "\t Train_Loss: 98.7372 Val_Loss: 241.2053  BEST VAL Loss: 241.2053\n",
            "\n",
            "Epoch 1467: Validation loss decreased (241.205322 --> 241.129837).\n",
            "\t Train_Loss: 98.6941 Val_Loss: 241.1298  BEST VAL Loss: 241.1298\n",
            "\n",
            "Epoch 1468: Validation loss decreased (241.129837 --> 241.052902).\n",
            "\t Train_Loss: 98.6510 Val_Loss: 241.0529  BEST VAL Loss: 241.0529\n",
            "\n",
            "Epoch 1469: Validation loss decreased (241.052902 --> 240.977936).\n",
            "\t Train_Loss: 98.6080 Val_Loss: 240.9779  BEST VAL Loss: 240.9779\n",
            "\n",
            "Epoch 1470: Validation loss decreased (240.977936 --> 240.907471).\n",
            "\t Train_Loss: 98.5650 Val_Loss: 240.9075  BEST VAL Loss: 240.9075\n",
            "\n",
            "Epoch 1471: Validation loss decreased (240.907471 --> 240.842636).\n",
            "\t Train_Loss: 98.5220 Val_Loss: 240.8426  BEST VAL Loss: 240.8426\n",
            "\n",
            "Epoch 1472: Validation loss decreased (240.842636 --> 240.781891).\n",
            "\t Train_Loss: 98.4790 Val_Loss: 240.7819  BEST VAL Loss: 240.7819\n",
            "\n",
            "Epoch 1473: Validation loss decreased (240.781891 --> 240.723343).\n",
            "\t Train_Loss: 98.4360 Val_Loss: 240.7233  BEST VAL Loss: 240.7233\n",
            "\n",
            "Epoch 1474: Validation loss decreased (240.723343 --> 240.664261).\n",
            "\t Train_Loss: 98.3931 Val_Loss: 240.6643  BEST VAL Loss: 240.6643\n",
            "\n",
            "Epoch 1475: Validation loss decreased (240.664261 --> 240.601364).\n",
            "\t Train_Loss: 98.3502 Val_Loss: 240.6014  BEST VAL Loss: 240.6014\n",
            "\n",
            "Epoch 1476: Validation loss decreased (240.601364 --> 240.534714).\n",
            "\t Train_Loss: 98.3073 Val_Loss: 240.5347  BEST VAL Loss: 240.5347\n",
            "\n",
            "Epoch 1477: Validation loss decreased (240.534714 --> 240.463287).\n",
            "\t Train_Loss: 98.2645 Val_Loss: 240.4633  BEST VAL Loss: 240.4633\n",
            "\n",
            "Epoch 1478: Validation loss decreased (240.463287 --> 240.388992).\n",
            "\t Train_Loss: 98.2217 Val_Loss: 240.3890  BEST VAL Loss: 240.3890\n",
            "\n",
            "Epoch 1479: Validation loss decreased (240.388992 --> 240.314011).\n",
            "\t Train_Loss: 98.1788 Val_Loss: 240.3140  BEST VAL Loss: 240.3140\n",
            "\n",
            "Epoch 1480: Validation loss decreased (240.314011 --> 240.240189).\n",
            "\t Train_Loss: 98.1361 Val_Loss: 240.2402  BEST VAL Loss: 240.2402\n",
            "\n",
            "Epoch 1481: Validation loss decreased (240.240189 --> 240.168808).\n",
            "\t Train_Loss: 98.0933 Val_Loss: 240.1688  BEST VAL Loss: 240.1688\n",
            "\n",
            "Epoch 1482: Validation loss decreased (240.168808 --> 240.100540).\n",
            "\t Train_Loss: 98.0505 Val_Loss: 240.1005  BEST VAL Loss: 240.1005\n",
            "\n",
            "Epoch 1483: Validation loss decreased (240.100540 --> 240.034714).\n",
            "\t Train_Loss: 98.0078 Val_Loss: 240.0347  BEST VAL Loss: 240.0347\n",
            "\n",
            "Epoch 1484: Validation loss decreased (240.034714 --> 239.969772).\n",
            "\t Train_Loss: 97.9651 Val_Loss: 239.9698  BEST VAL Loss: 239.9698\n",
            "\n",
            "Epoch 1485: Validation loss decreased (239.969772 --> 239.904160).\n",
            "\t Train_Loss: 97.9225 Val_Loss: 239.9042  BEST VAL Loss: 239.9042\n",
            "\n",
            "Epoch 1486: Validation loss decreased (239.904160 --> 239.837189).\n",
            "\t Train_Loss: 97.8798 Val_Loss: 239.8372  BEST VAL Loss: 239.8372\n",
            "\n",
            "Epoch 1487: Validation loss decreased (239.837189 --> 239.767441).\n",
            "\t Train_Loss: 97.8372 Val_Loss: 239.7674  BEST VAL Loss: 239.7674\n",
            "\n",
            "Epoch 1488: Validation loss decreased (239.767441 --> 239.695175).\n",
            "\t Train_Loss: 97.7946 Val_Loss: 239.6952  BEST VAL Loss: 239.6952\n",
            "\n",
            "Epoch 1489: Validation loss decreased (239.695175 --> 239.621002).\n",
            "\t Train_Loss: 97.7520 Val_Loss: 239.6210  BEST VAL Loss: 239.6210\n",
            "\n",
            "Epoch 1490: Validation loss decreased (239.621002 --> 239.546829).\n",
            "\t Train_Loss: 97.7094 Val_Loss: 239.5468  BEST VAL Loss: 239.5468\n",
            "\n",
            "Epoch 1491: Validation loss decreased (239.546829 --> 239.473022).\n",
            "\t Train_Loss: 97.6669 Val_Loss: 239.4730  BEST VAL Loss: 239.4730\n",
            "\n",
            "Epoch 1492: Validation loss decreased (239.473022 --> 239.400787).\n",
            "\t Train_Loss: 97.6244 Val_Loss: 239.4008  BEST VAL Loss: 239.4008\n",
            "\n",
            "Epoch 1493: Validation loss decreased (239.400787 --> 239.329941).\n",
            "\t Train_Loss: 97.5819 Val_Loss: 239.3299  BEST VAL Loss: 239.3299\n",
            "\n",
            "Epoch 1494: Validation loss decreased (239.329941 --> 239.260605).\n",
            "\t Train_Loss: 97.5394 Val_Loss: 239.2606  BEST VAL Loss: 239.2606\n",
            "\n",
            "Epoch 1495: Validation loss decreased (239.260605 --> 239.191818).\n",
            "\t Train_Loss: 97.4969 Val_Loss: 239.1918  BEST VAL Loss: 239.1918\n",
            "\n",
            "Epoch 1496: Validation loss decreased (239.191818 --> 239.122665).\n",
            "\t Train_Loss: 97.4545 Val_Loss: 239.1227  BEST VAL Loss: 239.1227\n",
            "\n",
            "Epoch 1497: Validation loss decreased (239.122665 --> 239.052399).\n",
            "\t Train_Loss: 97.4121 Val_Loss: 239.0524  BEST VAL Loss: 239.0524\n",
            "\n",
            "Epoch 1498: Validation loss decreased (239.052399 --> 238.980942).\n",
            "\t Train_Loss: 97.3697 Val_Loss: 238.9809  BEST VAL Loss: 238.9809\n",
            "\n",
            "Epoch 1499: Validation loss decreased (238.980942 --> 238.907837).\n",
            "\t Train_Loss: 97.3273 Val_Loss: 238.9078  BEST VAL Loss: 238.9078\n",
            "\n",
            "Epoch 1500: Validation loss decreased (238.907837 --> 238.833847).\n",
            "\t Train_Loss: 97.2849 Val_Loss: 238.8338  BEST VAL Loss: 238.8338\n",
            "\n",
            "Epoch 1501: Validation loss decreased (238.833847 --> 238.759277).\n",
            "\t Train_Loss: 97.2426 Val_Loss: 238.7593  BEST VAL Loss: 238.7593\n",
            "\n",
            "Epoch 1502: Validation loss decreased (238.759277 --> 238.685455).\n",
            "\t Train_Loss: 97.2002 Val_Loss: 238.6855  BEST VAL Loss: 238.6855\n",
            "\n",
            "Epoch 1503: Validation loss decreased (238.685455 --> 238.612259).\n",
            "\t Train_Loss: 97.1579 Val_Loss: 238.6123  BEST VAL Loss: 238.6123\n",
            "\n",
            "Epoch 1504: Validation loss decreased (238.612259 --> 238.540283).\n",
            "\t Train_Loss: 97.1156 Val_Loss: 238.5403  BEST VAL Loss: 238.5403\n",
            "\n",
            "Epoch 1505: Validation loss decreased (238.540283 --> 238.468918).\n",
            "\t Train_Loss: 97.0733 Val_Loss: 238.4689  BEST VAL Loss: 238.4689\n",
            "\n",
            "Epoch 1506: Validation loss decreased (238.468918 --> 238.397461).\n",
            "\t Train_Loss: 97.0310 Val_Loss: 238.3975  BEST VAL Loss: 238.3975\n",
            "\n",
            "Epoch 1507: Validation loss decreased (238.397461 --> 238.325974).\n",
            "\t Train_Loss: 96.9886 Val_Loss: 238.3260  BEST VAL Loss: 238.3260\n",
            "\n",
            "Epoch 1508: Validation loss decreased (238.325974 --> 238.253860).\n",
            "\t Train_Loss: 96.9463 Val_Loss: 238.2539  BEST VAL Loss: 238.2539\n",
            "\n",
            "Epoch 1509: Validation loss decreased (238.253860 --> 238.180710).\n",
            "\t Train_Loss: 96.9040 Val_Loss: 238.1807  BEST VAL Loss: 238.1807\n",
            "\n",
            "Epoch 1510: Validation loss decreased (238.180710 --> 238.107224).\n",
            "\t Train_Loss: 96.8616 Val_Loss: 238.1072  BEST VAL Loss: 238.1072\n",
            "\n",
            "Epoch 1511: Validation loss decreased (238.107224 --> 238.032959).\n",
            "\t Train_Loss: 96.8192 Val_Loss: 238.0330  BEST VAL Loss: 238.0330\n",
            "\n",
            "Epoch 1512: Validation loss decreased (238.032959 --> 237.958527).\n",
            "\t Train_Loss: 96.7767 Val_Loss: 237.9585  BEST VAL Loss: 237.9585\n",
            "\n",
            "Epoch 1513: Validation loss decreased (237.958527 --> 237.884384).\n",
            "\t Train_Loss: 96.7341 Val_Loss: 237.8844  BEST VAL Loss: 237.8844\n",
            "\n",
            "Epoch 1514: Validation loss decreased (237.884384 --> 237.810547).\n",
            "\t Train_Loss: 96.6913 Val_Loss: 237.8105  BEST VAL Loss: 237.8105\n",
            "\n",
            "Epoch 1515: Validation loss decreased (237.810547 --> 237.737946).\n",
            "\t Train_Loss: 96.6481 Val_Loss: 237.7379  BEST VAL Loss: 237.7379\n",
            "\n",
            "Epoch 1516: Validation loss decreased (237.737946 --> 237.665695).\n",
            "\t Train_Loss: 96.6045 Val_Loss: 237.6657  BEST VAL Loss: 237.6657\n",
            "\n",
            "Epoch 1517: Validation loss decreased (237.665695 --> 237.593704).\n",
            "\t Train_Loss: 96.5600 Val_Loss: 237.5937  BEST VAL Loss: 237.5937\n",
            "\n",
            "Epoch 1518: Validation loss decreased (237.593704 --> 237.522171).\n",
            "\t Train_Loss: 96.5135 Val_Loss: 237.5222  BEST VAL Loss: 237.5222\n",
            "\n",
            "Epoch 1519: Validation loss decreased (237.522171 --> 237.453323).\n",
            "\t Train_Loss: 96.4628 Val_Loss: 237.4533  BEST VAL Loss: 237.4533\n",
            "\n",
            "Epoch 1520: Validation loss decreased (237.453323 --> 237.397415).\n",
            "\t Train_Loss: 96.3995 Val_Loss: 237.3974  BEST VAL Loss: 237.3974\n",
            "\n",
            "Epoch 1521: Validation loss did not decrease\n",
            "\t Train_Loss: 96.2809 Val_Loss: 237.5560  BEST VAL Loss: 237.3974\n",
            "\n",
            "Epoch 1522: Validation loss did not decrease\n",
            "\t Train_Loss: 95.5495 Val_Loss: 290.2652  BEST VAL Loss: 237.3974\n",
            "\n",
            "Epoch 1523: Validation loss decreased (237.397415 --> 236.897537).\n",
            "\t Train_Loss: 96.7948 Val_Loss: 236.8975  BEST VAL Loss: 236.8975\n",
            "\n",
            "Epoch 1524: Validation loss decreased (236.897537 --> 235.881454).\n",
            "\t Train_Loss: 94.7448 Val_Loss: 235.8815  BEST VAL Loss: 235.8815\n",
            "\n",
            "Epoch 1525: Validation loss decreased (235.881454 --> 235.624008).\n",
            "\t Train_Loss: 96.1894 Val_Loss: 235.6240  BEST VAL Loss: 235.6240\n",
            "\n",
            "Epoch 1526: Validation loss did not decrease\n",
            "\t Train_Loss: 96.2439 Val_Loss: 235.6557  BEST VAL Loss: 235.6240\n",
            "\n",
            "Epoch 1527: Validation loss did not decrease\n",
            "\t Train_Loss: 96.1863 Val_Loss: 235.8954  BEST VAL Loss: 235.6240\n",
            "\n",
            "Epoch 1528: Validation loss did not decrease\n",
            "\t Train_Loss: 96.1184 Val_Loss: 236.2200  BEST VAL Loss: 235.6240\n",
            "\n",
            "Epoch 1529: Validation loss did not decrease\n",
            "\t Train_Loss: 96.0722 Val_Loss: 236.4694  BEST VAL Loss: 235.6240\n",
            "\n",
            "Epoch 1530: Validation loss did not decrease\n",
            "\t Train_Loss: 96.0484 Val_Loss: 236.5083  BEST VAL Loss: 235.6240\n",
            "\n",
            "Epoch 1531: Validation loss did not decrease\n",
            "\t Train_Loss: 96.0211 Val_Loss: 236.2896  BEST VAL Loss: 235.6240\n",
            "\n",
            "Epoch 1532: Validation loss did not decrease\n",
            "\t Train_Loss: 95.9723 Val_Loss: 235.8721  BEST VAL Loss: 235.6240\n",
            "\n",
            "Epoch 1533: Validation loss decreased (235.624008 --> 235.383621).\n",
            "\t Train_Loss: 95.9128 Val_Loss: 235.3836  BEST VAL Loss: 235.3836\n",
            "\n",
            "Epoch 1534: Validation loss decreased (235.383621 --> 234.954834).\n",
            "\t Train_Loss: 95.8640 Val_Loss: 234.9548  BEST VAL Loss: 234.9548\n",
            "\n",
            "Epoch 1535: Validation loss decreased (234.954834 --> 234.672928).\n",
            "\t Train_Loss: 95.8299 Val_Loss: 234.6729  BEST VAL Loss: 234.6729\n",
            "\n",
            "Epoch 1536: Validation loss decreased (234.672928 --> 234.569138).\n",
            "\t Train_Loss: 95.7983 Val_Loss: 234.5691  BEST VAL Loss: 234.5691\n",
            "\n",
            "Epoch 1537: Validation loss did not decrease\n",
            "\t Train_Loss: 95.7575 Val_Loss: 234.6291  BEST VAL Loss: 234.5691\n",
            "\n",
            "Epoch 1538: Validation loss did not decrease\n",
            "\t Train_Loss: 95.7075 Val_Loss: 234.7988  BEST VAL Loss: 234.5691\n",
            "\n",
            "Epoch 1539: Validation loss did not decrease\n",
            "\t Train_Loss: 95.6580 Val_Loss: 234.9945  BEST VAL Loss: 234.5691\n",
            "\n",
            "Epoch 1540: Validation loss did not decrease\n",
            "\t Train_Loss: 95.6167 Val_Loss: 235.1258  BEST VAL Loss: 234.5691\n",
            "\n",
            "Epoch 1541: Validation loss did not decrease\n",
            "\t Train_Loss: 95.5814 Val_Loss: 235.1275  BEST VAL Loss: 234.5691\n",
            "\n",
            "Epoch 1542: Validation loss did not decrease\n",
            "\t Train_Loss: 95.5434 Val_Loss: 234.9855  BEST VAL Loss: 234.5691\n",
            "\n",
            "Epoch 1543: Validation loss did not decrease\n",
            "\t Train_Loss: 95.4985 Val_Loss: 234.7397  BEST VAL Loss: 234.5691\n",
            "\n",
            "Epoch 1544: Validation loss decreased (234.569138 --> 234.461960).\n",
            "\t Train_Loss: 95.4515 Val_Loss: 234.4620  BEST VAL Loss: 234.4620\n",
            "\n",
            "Epoch 1545: Validation loss decreased (234.461960 --> 234.224167).\n",
            "\t Train_Loss: 95.4086 Val_Loss: 234.2242  BEST VAL Loss: 234.2242\n",
            "\n",
            "Epoch 1546: Validation loss decreased (234.224167 --> 234.074814).\n",
            "\t Train_Loss: 95.3701 Val_Loss: 234.0748  BEST VAL Loss: 234.0748\n",
            "\n",
            "Epoch 1547: Validation loss decreased (234.074814 --> 234.030182).\n",
            "\t Train_Loss: 95.3316 Val_Loss: 234.0302  BEST VAL Loss: 234.0302\n",
            "\n",
            "Epoch 1548: Validation loss did not decrease\n",
            "\t Train_Loss: 95.2898 Val_Loss: 234.0768  BEST VAL Loss: 234.0302\n",
            "\n",
            "Epoch 1549: Validation loss did not decrease\n",
            "\t Train_Loss: 95.2454 Val_Loss: 234.1750  BEST VAL Loss: 234.0302\n",
            "\n",
            "Epoch 1550: Validation loss did not decrease\n",
            "\t Train_Loss: 95.2020 Val_Loss: 234.2724  BEST VAL Loss: 234.0302\n",
            "\n",
            "Epoch 1551: Validation loss did not decrease\n",
            "\t Train_Loss: 95.1615 Val_Loss: 234.3201  BEST VAL Loss: 234.0302\n",
            "\n",
            "Epoch 1552: Validation loss did not decrease\n",
            "\t Train_Loss: 95.1223 Val_Loss: 234.2875  BEST VAL Loss: 234.0302\n",
            "\n",
            "Epoch 1553: Validation loss did not decrease\n",
            "\t Train_Loss: 95.0817 Val_Loss: 234.1754  BEST VAL Loss: 234.0302\n",
            "\n",
            "Epoch 1554: Validation loss decreased (234.030182 --> 234.010910).\n",
            "\t Train_Loss: 95.0391 Val_Loss: 234.0109  BEST VAL Loss: 234.0109\n",
            "\n",
            "Epoch 1555: Validation loss decreased (234.010910 --> 233.836655).\n",
            "\t Train_Loss: 94.9963 Val_Loss: 233.8367  BEST VAL Loss: 233.8367\n",
            "\n",
            "Epoch 1556: Validation loss decreased (233.836655 --> 233.692047).\n",
            "\t Train_Loss: 94.9550 Val_Loss: 233.6920  BEST VAL Loss: 233.6920\n",
            "\n",
            "Epoch 1557: Validation loss decreased (233.692047 --> 233.601761).\n",
            "\t Train_Loss: 94.9150 Val_Loss: 233.6018  BEST VAL Loss: 233.6018\n",
            "\n",
            "Epoch 1558: Validation loss decreased (233.601761 --> 233.572128).\n",
            "\t Train_Loss: 94.8747 Val_Loss: 233.5721  BEST VAL Loss: 233.5721\n",
            "\n",
            "Epoch 1559: Validation loss did not decrease\n",
            "\t Train_Loss: 94.8332 Val_Loss: 233.5896  BEST VAL Loss: 233.5721\n",
            "\n",
            "Epoch 1560: Validation loss did not decrease\n",
            "\t Train_Loss: 94.7911 Val_Loss: 233.6283  BEST VAL Loss: 233.5721\n",
            "\n",
            "Epoch 1561: Validation loss did not decrease\n",
            "\t Train_Loss: 94.7497 Val_Loss: 233.6562  BEST VAL Loss: 233.5721\n",
            "\n",
            "Epoch 1562: Validation loss did not decrease\n",
            "\t Train_Loss: 94.7092 Val_Loss: 233.6481  BEST VAL Loss: 233.5721\n",
            "\n",
            "Epoch 1563: Validation loss did not decrease\n",
            "\t Train_Loss: 94.6688 Val_Loss: 233.5917  BEST VAL Loss: 233.5721\n",
            "\n",
            "Epoch 1564: Validation loss decreased (233.572128 --> 233.492477).\n",
            "\t Train_Loss: 94.6279 Val_Loss: 233.4925  BEST VAL Loss: 233.4925\n",
            "\n",
            "Epoch 1565: Validation loss decreased (233.492477 --> 233.369141).\n",
            "\t Train_Loss: 94.5864 Val_Loss: 233.3691  BEST VAL Loss: 233.3691\n",
            "\n",
            "Epoch 1566: Validation loss decreased (233.369141 --> 233.246445).\n",
            "\t Train_Loss: 94.5451 Val_Loss: 233.2464  BEST VAL Loss: 233.2464\n",
            "\n",
            "Epoch 1567: Validation loss decreased (233.246445 --> 233.145660).\n",
            "\t Train_Loss: 94.5044 Val_Loss: 233.1457  BEST VAL Loss: 233.1457\n",
            "\n",
            "Epoch 1568: Validation loss decreased (233.145660 --> 233.078201).\n",
            "\t Train_Loss: 94.4639 Val_Loss: 233.0782  BEST VAL Loss: 233.0782\n",
            "\n",
            "Epoch 1569: Validation loss decreased (233.078201 --> 233.044632).\n",
            "\t Train_Loss: 94.4232 Val_Loss: 233.0446  BEST VAL Loss: 233.0446\n",
            "\n",
            "Epoch 1570: Validation loss decreased (233.044632 --> 233.033936).\n",
            "\t Train_Loss: 94.3822 Val_Loss: 233.0339  BEST VAL Loss: 233.0339\n",
            "\n",
            "Epoch 1571: Validation loss decreased (233.033936 --> 233.029099).\n",
            "\t Train_Loss: 94.3411 Val_Loss: 233.0291  BEST VAL Loss: 233.0291\n",
            "\n",
            "Epoch 1572: Validation loss decreased (233.029099 --> 233.013214).\n",
            "\t Train_Loss: 94.3003 Val_Loss: 233.0132  BEST VAL Loss: 233.0132\n",
            "\n",
            "Epoch 1573: Validation loss decreased (233.013214 --> 232.972092).\n",
            "\t Train_Loss: 94.2598 Val_Loss: 232.9721  BEST VAL Loss: 232.9721\n",
            "\n",
            "Epoch 1574: Validation loss decreased (232.972092 --> 232.903610).\n",
            "\t Train_Loss: 94.2193 Val_Loss: 232.9036  BEST VAL Loss: 232.9036\n",
            "\n",
            "Epoch 1575: Validation loss decreased (232.903610 --> 232.812698).\n",
            "\t Train_Loss: 94.1785 Val_Loss: 232.8127  BEST VAL Loss: 232.8127\n",
            "\n",
            "Epoch 1576: Validation loss decreased (232.812698 --> 232.712753).\n",
            "\t Train_Loss: 94.1376 Val_Loss: 232.7128  BEST VAL Loss: 232.7128\n",
            "\n",
            "Epoch 1577: Validation loss decreased (232.712753 --> 232.616684).\n",
            "\t Train_Loss: 94.0970 Val_Loss: 232.6167  BEST VAL Loss: 232.6167\n",
            "\n",
            "Epoch 1578: Validation loss decreased (232.616684 --> 232.536087).\n",
            "\t Train_Loss: 94.0564 Val_Loss: 232.5361  BEST VAL Loss: 232.5361\n",
            "\n",
            "Epoch 1579: Validation loss decreased (232.536087 --> 232.475983).\n",
            "\t Train_Loss: 94.0160 Val_Loss: 232.4760  BEST VAL Loss: 232.4760\n",
            "\n",
            "Epoch 1580: Validation loss decreased (232.475983 --> 232.433578).\n",
            "\t Train_Loss: 93.9754 Val_Loss: 232.4336  BEST VAL Loss: 232.4336\n",
            "\n",
            "Epoch 1581: Validation loss decreased (232.433578 --> 232.401413).\n",
            "\t Train_Loss: 93.9348 Val_Loss: 232.4014  BEST VAL Loss: 232.4014\n",
            "\n",
            "Epoch 1582: Validation loss decreased (232.401413 --> 232.369125).\n",
            "\t Train_Loss: 93.8942 Val_Loss: 232.3691  BEST VAL Loss: 232.3691\n",
            "\n",
            "Epoch 1583: Validation loss decreased (232.369125 --> 232.327057).\n",
            "\t Train_Loss: 93.8538 Val_Loss: 232.3271  BEST VAL Loss: 232.3271\n",
            "\n",
            "Epoch 1584: Validation loss decreased (232.327057 --> 232.269730).\n",
            "\t Train_Loss: 93.8134 Val_Loss: 232.2697  BEST VAL Loss: 232.2697\n",
            "\n",
            "Epoch 1585: Validation loss decreased (232.269730 --> 232.196945).\n",
            "\t Train_Loss: 93.7730 Val_Loss: 232.1969  BEST VAL Loss: 232.1969\n",
            "\n",
            "Epoch 1586: Validation loss decreased (232.196945 --> 232.113281).\n",
            "\t Train_Loss: 93.7325 Val_Loss: 232.1133  BEST VAL Loss: 232.1133\n",
            "\n",
            "Epoch 1587: Validation loss decreased (232.113281 --> 232.026855).\n",
            "\t Train_Loss: 93.6920 Val_Loss: 232.0269  BEST VAL Loss: 232.0269\n",
            "\n",
            "Epoch 1588: Validation loss decreased (232.026855 --> 231.945221).\n",
            "\t Train_Loss: 93.6517 Val_Loss: 231.9452  BEST VAL Loss: 231.9452\n",
            "\n",
            "Epoch 1589: Validation loss decreased (231.945221 --> 231.873932).\n",
            "\t Train_Loss: 93.6114 Val_Loss: 231.8739  BEST VAL Loss: 231.8739\n",
            "\n",
            "Epoch 1590: Validation loss decreased (231.873932 --> 231.814453).\n",
            "\t Train_Loss: 93.5711 Val_Loss: 231.8145  BEST VAL Loss: 231.8145\n",
            "\n",
            "Epoch 1591: Validation loss decreased (231.814453 --> 231.763840).\n",
            "\t Train_Loss: 93.5308 Val_Loss: 231.7638  BEST VAL Loss: 231.7638\n",
            "\n",
            "Epoch 1592: Validation loss decreased (231.763840 --> 231.717331).\n",
            "\t Train_Loss: 93.4905 Val_Loss: 231.7173  BEST VAL Loss: 231.7173\n",
            "\n",
            "Epoch 1593: Validation loss decreased (231.717331 --> 231.668503).\n",
            "\t Train_Loss: 93.4502 Val_Loss: 231.6685  BEST VAL Loss: 231.6685\n",
            "\n",
            "Epoch 1594: Validation loss decreased (231.668503 --> 231.612473).\n",
            "\t Train_Loss: 93.4100 Val_Loss: 231.6125  BEST VAL Loss: 231.6125\n",
            "\n",
            "Epoch 1595: Validation loss decreased (231.612473 --> 231.546875).\n",
            "\t Train_Loss: 93.3698 Val_Loss: 231.5469  BEST VAL Loss: 231.5469\n",
            "\n",
            "Epoch 1596: Validation loss decreased (231.546875 --> 231.473389).\n",
            "\t Train_Loss: 93.3296 Val_Loss: 231.4734  BEST VAL Loss: 231.4734\n",
            "\n",
            "Epoch 1597: Validation loss decreased (231.473389 --> 231.394775).\n",
            "\t Train_Loss: 93.2894 Val_Loss: 231.3948  BEST VAL Loss: 231.3948\n",
            "\n",
            "Epoch 1598: Validation loss decreased (231.394775 --> 231.316452).\n",
            "\t Train_Loss: 93.2492 Val_Loss: 231.3165  BEST VAL Loss: 231.3165\n",
            "\n",
            "Epoch 1599: Validation loss decreased (231.316452 --> 231.241989).\n",
            "\t Train_Loss: 93.2091 Val_Loss: 231.2420  BEST VAL Loss: 231.2420\n",
            "\n",
            "Epoch 1600: Validation loss decreased (231.241989 --> 231.174103).\n",
            "\t Train_Loss: 93.1690 Val_Loss: 231.1741  BEST VAL Loss: 231.1741\n",
            "\n",
            "Epoch 1601: Validation loss decreased (231.174103 --> 231.112900).\n",
            "\t Train_Loss: 93.1290 Val_Loss: 231.1129  BEST VAL Loss: 231.1129\n",
            "\n",
            "Epoch 1602: Validation loss decreased (231.112900 --> 231.055695).\n",
            "\t Train_Loss: 93.0889 Val_Loss: 231.0557  BEST VAL Loss: 231.0557\n",
            "\n",
            "Epoch 1603: Validation loss decreased (231.055695 --> 230.999512).\n",
            "\t Train_Loss: 93.0489 Val_Loss: 230.9995  BEST VAL Loss: 230.9995\n",
            "\n",
            "Epoch 1604: Validation loss decreased (230.999512 --> 230.940430).\n",
            "\t Train_Loss: 93.0089 Val_Loss: 230.9404  BEST VAL Loss: 230.9404\n",
            "\n",
            "Epoch 1605: Validation loss decreased (230.940430 --> 230.876663).\n",
            "\t Train_Loss: 92.9689 Val_Loss: 230.8767  BEST VAL Loss: 230.8767\n",
            "\n",
            "Epoch 1606: Validation loss decreased (230.876663 --> 230.807281).\n",
            "\t Train_Loss: 92.9289 Val_Loss: 230.8073  BEST VAL Loss: 230.8073\n",
            "\n",
            "Epoch 1607: Validation loss decreased (230.807281 --> 230.733597).\n",
            "\t Train_Loss: 92.8890 Val_Loss: 230.7336  BEST VAL Loss: 230.7336\n",
            "\n",
            "Epoch 1608: Validation loss decreased (230.733597 --> 230.658203).\n",
            "\t Train_Loss: 92.8491 Val_Loss: 230.6582  BEST VAL Loss: 230.6582\n",
            "\n",
            "Epoch 1609: Validation loss decreased (230.658203 --> 230.583740).\n",
            "\t Train_Loss: 92.8091 Val_Loss: 230.5837  BEST VAL Loss: 230.5837\n",
            "\n",
            "Epoch 1610: Validation loss decreased (230.583740 --> 230.512741).\n",
            "\t Train_Loss: 92.7693 Val_Loss: 230.5127  BEST VAL Loss: 230.5127\n",
            "\n",
            "Epoch 1611: Validation loss decreased (230.512741 --> 230.445557).\n",
            "\t Train_Loss: 92.7294 Val_Loss: 230.4456  BEST VAL Loss: 230.4456\n",
            "\n",
            "Epoch 1612: Validation loss decreased (230.445557 --> 230.381378).\n",
            "\t Train_Loss: 92.6896 Val_Loss: 230.3814  BEST VAL Loss: 230.3814\n",
            "\n",
            "Epoch 1613: Validation loss decreased (230.381378 --> 230.318893).\n",
            "\t Train_Loss: 92.6498 Val_Loss: 230.3189  BEST VAL Loss: 230.3189\n",
            "\n",
            "Epoch 1614: Validation loss decreased (230.318893 --> 230.256104).\n",
            "\t Train_Loss: 92.6100 Val_Loss: 230.2561  BEST VAL Loss: 230.2561\n",
            "\n",
            "Epoch 1615: Validation loss decreased (230.256104 --> 230.190872).\n",
            "\t Train_Loss: 92.5702 Val_Loss: 230.1909  BEST VAL Loss: 230.1909\n",
            "\n",
            "Epoch 1616: Validation loss decreased (230.190872 --> 230.122177).\n",
            "\t Train_Loss: 92.5305 Val_Loss: 230.1222  BEST VAL Loss: 230.1222\n",
            "\n",
            "Epoch 1617: Validation loss decreased (230.122177 --> 230.051010).\n",
            "\t Train_Loss: 92.4908 Val_Loss: 230.0510  BEST VAL Loss: 230.0510\n",
            "\n",
            "Epoch 1618: Validation loss decreased (230.051010 --> 229.977829).\n",
            "\t Train_Loss: 92.4511 Val_Loss: 229.9778  BEST VAL Loss: 229.9778\n",
            "\n",
            "Epoch 1619: Validation loss decreased (229.977829 --> 229.904053).\n",
            "\t Train_Loss: 92.4114 Val_Loss: 229.9041  BEST VAL Loss: 229.9041\n",
            "\n",
            "Epoch 1620: Validation loss decreased (229.904053 --> 229.831863).\n",
            "\t Train_Loss: 92.3717 Val_Loss: 229.8319  BEST VAL Loss: 229.8319\n",
            "\n",
            "Epoch 1621: Validation loss decreased (229.831863 --> 229.761139).\n",
            "\t Train_Loss: 92.3321 Val_Loss: 229.7611  BEST VAL Loss: 229.7611\n",
            "\n",
            "Epoch 1622: Validation loss decreased (229.761139 --> 229.693024).\n",
            "\t Train_Loss: 92.2925 Val_Loss: 229.6930  BEST VAL Loss: 229.6930\n",
            "\n",
            "Epoch 1623: Validation loss decreased (229.693024 --> 229.626129).\n",
            "\t Train_Loss: 92.2529 Val_Loss: 229.6261  BEST VAL Loss: 229.6261\n",
            "\n",
            "Epoch 1624: Validation loss decreased (229.626129 --> 229.559723).\n",
            "\t Train_Loss: 92.2133 Val_Loss: 229.5597  BEST VAL Loss: 229.5597\n",
            "\n",
            "Epoch 1625: Validation loss decreased (229.559723 --> 229.492706).\n",
            "\t Train_Loss: 92.1738 Val_Loss: 229.4927  BEST VAL Loss: 229.4927\n",
            "\n",
            "Epoch 1626: Validation loss decreased (229.492706 --> 229.423691).\n",
            "\t Train_Loss: 92.1342 Val_Loss: 229.4237  BEST VAL Loss: 229.4237\n",
            "\n",
            "Epoch 1627: Validation loss decreased (229.423691 --> 229.352905).\n",
            "\t Train_Loss: 92.0947 Val_Loss: 229.3529  BEST VAL Loss: 229.3529\n",
            "\n",
            "Epoch 1628: Validation loss decreased (229.352905 --> 229.280472).\n",
            "\t Train_Loss: 92.0553 Val_Loss: 229.2805  BEST VAL Loss: 229.2805\n",
            "\n",
            "Epoch 1629: Validation loss decreased (229.280472 --> 229.207474).\n",
            "\t Train_Loss: 92.0158 Val_Loss: 229.2075  BEST VAL Loss: 229.2075\n",
            "\n",
            "Epoch 1630: Validation loss decreased (229.207474 --> 229.134445).\n",
            "\t Train_Loss: 91.9764 Val_Loss: 229.1344  BEST VAL Loss: 229.1344\n",
            "\n",
            "Epoch 1631: Validation loss decreased (229.134445 --> 229.062653).\n",
            "\t Train_Loss: 91.9370 Val_Loss: 229.0627  BEST VAL Loss: 229.0627\n",
            "\n",
            "Epoch 1632: Validation loss decreased (229.062653 --> 228.992142).\n",
            "\t Train_Loss: 91.8976 Val_Loss: 228.9921  BEST VAL Loss: 228.9921\n",
            "\n",
            "Epoch 1633: Validation loss decreased (228.992142 --> 228.922516).\n",
            "\t Train_Loss: 91.8582 Val_Loss: 228.9225  BEST VAL Loss: 228.9225\n",
            "\n",
            "Epoch 1634: Validation loss decreased (228.922516 --> 228.853867).\n",
            "\t Train_Loss: 91.8188 Val_Loss: 228.8539  BEST VAL Loss: 228.8539\n",
            "\n",
            "Epoch 1635: Validation loss decreased (228.853867 --> 228.784821).\n",
            "\t Train_Loss: 91.7795 Val_Loss: 228.7848  BEST VAL Loss: 228.7848\n",
            "\n",
            "Epoch 1636: Validation loss decreased (228.784821 --> 228.715088).\n",
            "\t Train_Loss: 91.7402 Val_Loss: 228.7151  BEST VAL Loss: 228.7151\n",
            "\n",
            "Epoch 1637: Validation loss decreased (228.715088 --> 228.644318).\n",
            "\t Train_Loss: 91.7009 Val_Loss: 228.6443  BEST VAL Loss: 228.6443\n",
            "\n",
            "Epoch 1638: Validation loss decreased (228.644318 --> 228.572052).\n",
            "\t Train_Loss: 91.6617 Val_Loss: 228.5721  BEST VAL Loss: 228.5721\n",
            "\n",
            "Epoch 1639: Validation loss decreased (228.572052 --> 228.499557).\n",
            "\t Train_Loss: 91.6224 Val_Loss: 228.4996  BEST VAL Loss: 228.4996\n",
            "\n",
            "Epoch 1640: Validation loss decreased (228.499557 --> 228.426804).\n",
            "\t Train_Loss: 91.5832 Val_Loss: 228.4268  BEST VAL Loss: 228.4268\n",
            "\n",
            "Epoch 1641: Validation loss decreased (228.426804 --> 228.354416).\n",
            "\t Train_Loss: 91.5440 Val_Loss: 228.3544  BEST VAL Loss: 228.3544\n",
            "\n",
            "Epoch 1642: Validation loss decreased (228.354416 --> 228.282623).\n",
            "\t Train_Loss: 91.5048 Val_Loss: 228.2826  BEST VAL Loss: 228.2826\n",
            "\n",
            "Epoch 1643: Validation loss decreased (228.282623 --> 228.212021).\n",
            "\t Train_Loss: 91.4657 Val_Loss: 228.2120  BEST VAL Loss: 228.2120\n",
            "\n",
            "Epoch 1644: Validation loss decreased (228.212021 --> 228.141510).\n",
            "\t Train_Loss: 91.4265 Val_Loss: 228.1415  BEST VAL Loss: 228.1415\n",
            "\n",
            "Epoch 1645: Validation loss decreased (228.141510 --> 228.071198).\n",
            "\t Train_Loss: 91.3874 Val_Loss: 228.0712  BEST VAL Loss: 228.0712\n",
            "\n",
            "Epoch 1646: Validation loss decreased (228.071198 --> 228.000488).\n",
            "\t Train_Loss: 91.3483 Val_Loss: 228.0005  BEST VAL Loss: 228.0005\n",
            "\n",
            "Epoch 1647: Validation loss decreased (228.000488 --> 227.929398).\n",
            "\t Train_Loss: 91.3092 Val_Loss: 227.9294  BEST VAL Loss: 227.9294\n",
            "\n",
            "Epoch 1648: Validation loss decreased (227.929398 --> 227.857529).\n",
            "\t Train_Loss: 91.2702 Val_Loss: 227.8575  BEST VAL Loss: 227.8575\n",
            "\n",
            "Epoch 1649: Validation loss decreased (227.857529 --> 227.785355).\n",
            "\t Train_Loss: 91.2312 Val_Loss: 227.7854  BEST VAL Loss: 227.7854\n",
            "\n",
            "Epoch 1650: Validation loss decreased (227.785355 --> 227.712357).\n",
            "\t Train_Loss: 91.1922 Val_Loss: 227.7124  BEST VAL Loss: 227.7124\n",
            "\n",
            "Epoch 1651: Validation loss decreased (227.712357 --> 227.640182).\n",
            "\t Train_Loss: 91.1532 Val_Loss: 227.6402  BEST VAL Loss: 227.6402\n",
            "\n",
            "Epoch 1652: Validation loss decreased (227.640182 --> 227.567871).\n",
            "\t Train_Loss: 91.1143 Val_Loss: 227.5679  BEST VAL Loss: 227.5679\n",
            "\n",
            "Epoch 1653: Validation loss decreased (227.567871 --> 227.496338).\n",
            "\t Train_Loss: 91.0753 Val_Loss: 227.4963  BEST VAL Loss: 227.4963\n",
            "\n",
            "Epoch 1654: Validation loss decreased (227.496338 --> 227.425095).\n",
            "\t Train_Loss: 91.0364 Val_Loss: 227.4251  BEST VAL Loss: 227.4251\n",
            "\n",
            "Epoch 1655: Validation loss decreased (227.425095 --> 227.353989).\n",
            "\t Train_Loss: 90.9975 Val_Loss: 227.3540  BEST VAL Loss: 227.3540\n",
            "\n",
            "Epoch 1656: Validation loss decreased (227.353989 --> 227.282715).\n",
            "\t Train_Loss: 90.9586 Val_Loss: 227.2827  BEST VAL Loss: 227.2827\n",
            "\n",
            "Epoch 1657: Validation loss decreased (227.282715 --> 227.211426).\n",
            "\t Train_Loss: 90.9198 Val_Loss: 227.2114  BEST VAL Loss: 227.2114\n",
            "\n",
            "Epoch 1658: Validation loss decreased (227.211426 --> 227.139603).\n",
            "\t Train_Loss: 90.8809 Val_Loss: 227.1396  BEST VAL Loss: 227.1396\n",
            "\n",
            "Epoch 1659: Validation loss decreased (227.139603 --> 227.067093).\n",
            "\t Train_Loss: 90.8421 Val_Loss: 227.0671  BEST VAL Loss: 227.0671\n",
            "\n",
            "Epoch 1660: Validation loss decreased (227.067093 --> 226.994873).\n",
            "\t Train_Loss: 90.8034 Val_Loss: 226.9949  BEST VAL Loss: 226.9949\n",
            "\n",
            "Epoch 1661: Validation loss decreased (226.994873 --> 226.922409).\n",
            "\t Train_Loss: 90.7646 Val_Loss: 226.9224  BEST VAL Loss: 226.9224\n",
            "\n",
            "Epoch 1662: Validation loss decreased (226.922409 --> 226.850449).\n",
            "\t Train_Loss: 90.7259 Val_Loss: 226.8504  BEST VAL Loss: 226.8504\n",
            "\n",
            "Epoch 1663: Validation loss decreased (226.850449 --> 226.778488).\n",
            "\t Train_Loss: 90.6871 Val_Loss: 226.7785  BEST VAL Loss: 226.7785\n",
            "\n",
            "Epoch 1664: Validation loss decreased (226.778488 --> 226.706589).\n",
            "\t Train_Loss: 90.6484 Val_Loss: 226.7066  BEST VAL Loss: 226.7066\n",
            "\n",
            "Epoch 1665: Validation loss decreased (226.706589 --> 226.635086).\n",
            "\t Train_Loss: 90.6098 Val_Loss: 226.6351  BEST VAL Loss: 226.6351\n",
            "\n",
            "Epoch 1666: Validation loss decreased (226.635086 --> 226.563461).\n",
            "\t Train_Loss: 90.5711 Val_Loss: 226.5635  BEST VAL Loss: 226.5635\n",
            "\n",
            "Epoch 1667: Validation loss decreased (226.563461 --> 226.492142).\n",
            "\t Train_Loss: 90.5325 Val_Loss: 226.4921  BEST VAL Loss: 226.4921\n",
            "\n",
            "Epoch 1668: Validation loss decreased (226.492142 --> 226.420151).\n",
            "\t Train_Loss: 90.4939 Val_Loss: 226.4202  BEST VAL Loss: 226.4202\n",
            "\n",
            "Epoch 1669: Validation loss decreased (226.420151 --> 226.347946).\n",
            "\t Train_Loss: 90.4553 Val_Loss: 226.3479  BEST VAL Loss: 226.3479\n",
            "\n",
            "Epoch 1670: Validation loss decreased (226.347946 --> 226.275925).\n",
            "\t Train_Loss: 90.4167 Val_Loss: 226.2759  BEST VAL Loss: 226.2759\n",
            "\n",
            "Epoch 1671: Validation loss decreased (226.275925 --> 226.203781).\n",
            "\t Train_Loss: 90.3782 Val_Loss: 226.2038  BEST VAL Loss: 226.2038\n",
            "\n",
            "Epoch 1672: Validation loss decreased (226.203781 --> 226.131424).\n",
            "\t Train_Loss: 90.3397 Val_Loss: 226.1314  BEST VAL Loss: 226.1314\n",
            "\n",
            "Epoch 1673: Validation loss decreased (226.131424 --> 226.059525).\n",
            "\t Train_Loss: 90.3011 Val_Loss: 226.0595  BEST VAL Loss: 226.0595\n",
            "\n",
            "Epoch 1674: Validation loss decreased (226.059525 --> 225.987549).\n",
            "\t Train_Loss: 90.2627 Val_Loss: 225.9875  BEST VAL Loss: 225.9875\n",
            "\n",
            "Epoch 1675: Validation loss decreased (225.987549 --> 225.915726).\n",
            "\t Train_Loss: 90.2242 Val_Loss: 225.9157  BEST VAL Loss: 225.9157\n",
            "\n",
            "Epoch 1676: Validation loss decreased (225.915726 --> 225.844162).\n",
            "\t Train_Loss: 90.1857 Val_Loss: 225.8442  BEST VAL Loss: 225.8442\n",
            "\n",
            "Epoch 1677: Validation loss decreased (225.844162 --> 225.772339).\n",
            "\t Train_Loss: 90.1473 Val_Loss: 225.7723  BEST VAL Loss: 225.7723\n",
            "\n",
            "Epoch 1678: Validation loss decreased (225.772339 --> 225.700439).\n",
            "\t Train_Loss: 90.1090 Val_Loss: 225.7004  BEST VAL Loss: 225.7004\n",
            "\n",
            "Epoch 1679: Validation loss decreased (225.700439 --> 225.628662).\n",
            "\t Train_Loss: 90.0706 Val_Loss: 225.6287  BEST VAL Loss: 225.6287\n",
            "\n",
            "Epoch 1680: Validation loss decreased (225.628662 --> 225.556625).\n",
            "\t Train_Loss: 90.0322 Val_Loss: 225.5566  BEST VAL Loss: 225.5566\n",
            "\n",
            "Epoch 1681: Validation loss decreased (225.556625 --> 225.484375).\n",
            "\t Train_Loss: 89.9939 Val_Loss: 225.4844  BEST VAL Loss: 225.4844\n",
            "\n",
            "Epoch 1682: Validation loss decreased (225.484375 --> 225.412521).\n",
            "\t Train_Loss: 89.9556 Val_Loss: 225.4125  BEST VAL Loss: 225.4125\n",
            "\n",
            "Epoch 1683: Validation loss decreased (225.412521 --> 225.340332).\n",
            "\t Train_Loss: 89.9173 Val_Loss: 225.3403  BEST VAL Loss: 225.3403\n",
            "\n",
            "Epoch 1684: Validation loss decreased (225.340332 --> 225.268295).\n",
            "\t Train_Loss: 89.8791 Val_Loss: 225.2683  BEST VAL Loss: 225.2683\n",
            "\n",
            "Epoch 1685: Validation loss decreased (225.268295 --> 225.196701).\n",
            "\t Train_Loss: 89.8408 Val_Loss: 225.1967  BEST VAL Loss: 225.1967\n",
            "\n",
            "Epoch 1686: Validation loss decreased (225.196701 --> 225.124802).\n",
            "\t Train_Loss: 89.8026 Val_Loss: 225.1248  BEST VAL Loss: 225.1248\n",
            "\n",
            "Epoch 1687: Validation loss decreased (225.124802 --> 225.053009).\n",
            "\t Train_Loss: 89.7644 Val_Loss: 225.0530  BEST VAL Loss: 225.0530\n",
            "\n",
            "Epoch 1688: Validation loss decreased (225.053009 --> 224.981033).\n",
            "\t Train_Loss: 89.7262 Val_Loss: 224.9810  BEST VAL Loss: 224.9810\n",
            "\n",
            "Epoch 1689: Validation loss decreased (224.981033 --> 224.909302).\n",
            "\t Train_Loss: 89.6881 Val_Loss: 224.9093  BEST VAL Loss: 224.9093\n",
            "\n",
            "Epoch 1690: Validation loss decreased (224.909302 --> 224.837509).\n",
            "\t Train_Loss: 89.6499 Val_Loss: 224.8375  BEST VAL Loss: 224.8375\n",
            "\n",
            "Epoch 1691: Validation loss decreased (224.837509 --> 224.765793).\n",
            "\t Train_Loss: 89.6118 Val_Loss: 224.7658  BEST VAL Loss: 224.7658\n",
            "\n",
            "Epoch 1692: Validation loss decreased (224.765793 --> 224.693588).\n",
            "\t Train_Loss: 89.5737 Val_Loss: 224.6936  BEST VAL Loss: 224.6936\n",
            "\n",
            "Epoch 1693: Validation loss decreased (224.693588 --> 224.621613).\n",
            "\t Train_Loss: 89.5357 Val_Loss: 224.6216  BEST VAL Loss: 224.6216\n",
            "\n",
            "Epoch 1694: Validation loss decreased (224.621613 --> 224.549759).\n",
            "\t Train_Loss: 89.4976 Val_Loss: 224.5498  BEST VAL Loss: 224.5498\n",
            "\n",
            "Epoch 1695: Validation loss decreased (224.549759 --> 224.478226).\n",
            "\t Train_Loss: 89.4596 Val_Loss: 224.4782  BEST VAL Loss: 224.4782\n",
            "\n",
            "Epoch 1696: Validation loss decreased (224.478226 --> 224.406601).\n",
            "\t Train_Loss: 89.4216 Val_Loss: 224.4066  BEST VAL Loss: 224.4066\n",
            "\n",
            "Epoch 1697: Validation loss decreased (224.406601 --> 224.335114).\n",
            "\t Train_Loss: 89.3836 Val_Loss: 224.3351  BEST VAL Loss: 224.3351\n",
            "\n",
            "Epoch 1698: Validation loss decreased (224.335114 --> 224.263062).\n",
            "\t Train_Loss: 89.3457 Val_Loss: 224.2631  BEST VAL Loss: 224.2631\n",
            "\n",
            "Epoch 1699: Validation loss decreased (224.263062 --> 224.191360).\n",
            "\t Train_Loss: 89.3077 Val_Loss: 224.1914  BEST VAL Loss: 224.1914\n",
            "\n",
            "Epoch 1700: Validation loss decreased (224.191360 --> 224.119614).\n",
            "\t Train_Loss: 89.2698 Val_Loss: 224.1196  BEST VAL Loss: 224.1196\n",
            "\n",
            "Epoch 1701: Validation loss decreased (224.119614 --> 224.047775).\n",
            "\t Train_Loss: 89.2319 Val_Loss: 224.0478  BEST VAL Loss: 224.0478\n",
            "\n",
            "Epoch 1702: Validation loss decreased (224.047775 --> 223.976303).\n",
            "\t Train_Loss: 89.1941 Val_Loss: 223.9763  BEST VAL Loss: 223.9763\n",
            "\n",
            "Epoch 1703: Validation loss decreased (223.976303 --> 223.904495).\n",
            "\t Train_Loss: 89.1562 Val_Loss: 223.9045  BEST VAL Loss: 223.9045\n",
            "\n",
            "Epoch 1704: Validation loss decreased (223.904495 --> 223.832809).\n",
            "\t Train_Loss: 89.1184 Val_Loss: 223.8328  BEST VAL Loss: 223.8328\n",
            "\n",
            "Epoch 1705: Validation loss decreased (223.832809 --> 223.761063).\n",
            "\t Train_Loss: 89.0806 Val_Loss: 223.7611  BEST VAL Loss: 223.7611\n",
            "\n",
            "Epoch 1706: Validation loss decreased (223.761063 --> 223.689728).\n",
            "\t Train_Loss: 89.0428 Val_Loss: 223.6897  BEST VAL Loss: 223.6897\n",
            "\n",
            "Epoch 1707: Validation loss decreased (223.689728 --> 223.618317).\n",
            "\t Train_Loss: 89.0050 Val_Loss: 223.6183  BEST VAL Loss: 223.6183\n",
            "\n",
            "Epoch 1708: Validation loss decreased (223.618317 --> 223.546509).\n",
            "\t Train_Loss: 88.9673 Val_Loss: 223.5465  BEST VAL Loss: 223.5465\n",
            "\n",
            "Epoch 1709: Validation loss decreased (223.546509 --> 223.474716).\n",
            "\t Train_Loss: 88.9296 Val_Loss: 223.4747  BEST VAL Loss: 223.4747\n",
            "\n",
            "Epoch 1710: Validation loss decreased (223.474716 --> 223.403427).\n",
            "\t Train_Loss: 88.8919 Val_Loss: 223.4034  BEST VAL Loss: 223.4034\n",
            "\n",
            "Epoch 1711: Validation loss decreased (223.403427 --> 223.331863).\n",
            "\t Train_Loss: 88.8542 Val_Loss: 223.3319  BEST VAL Loss: 223.3319\n",
            "\n",
            "Epoch 1712: Validation loss decreased (223.331863 --> 223.260284).\n",
            "\t Train_Loss: 88.8166 Val_Loss: 223.2603  BEST VAL Loss: 223.2603\n",
            "\n",
            "Epoch 1713: Validation loss decreased (223.260284 --> 223.188675).\n",
            "\t Train_Loss: 88.7789 Val_Loss: 223.1887  BEST VAL Loss: 223.1887\n",
            "\n",
            "Epoch 1714: Validation loss decreased (223.188675 --> 223.117477).\n",
            "\t Train_Loss: 88.7413 Val_Loss: 223.1175  BEST VAL Loss: 223.1175\n",
            "\n",
            "Epoch 1715: Validation loss decreased (223.117477 --> 223.046005).\n",
            "\t Train_Loss: 88.7038 Val_Loss: 223.0460  BEST VAL Loss: 223.0460\n",
            "\n",
            "Epoch 1716: Validation loss decreased (223.046005 --> 222.974472).\n",
            "\t Train_Loss: 88.6662 Val_Loss: 222.9745  BEST VAL Loss: 222.9745\n",
            "\n",
            "Epoch 1717: Validation loss decreased (222.974472 --> 222.903229).\n",
            "\t Train_Loss: 88.6287 Val_Loss: 222.9032  BEST VAL Loss: 222.9032\n",
            "\n",
            "Epoch 1718: Validation loss decreased (222.903229 --> 222.831833).\n",
            "\t Train_Loss: 88.5911 Val_Loss: 222.8318  BEST VAL Loss: 222.8318\n",
            "\n",
            "Epoch 1719: Validation loss decreased (222.831833 --> 222.760406).\n",
            "\t Train_Loss: 88.5536 Val_Loss: 222.7604  BEST VAL Loss: 222.7604\n",
            "\n",
            "Epoch 1720: Validation loss decreased (222.760406 --> 222.689011).\n",
            "\t Train_Loss: 88.5162 Val_Loss: 222.6890  BEST VAL Loss: 222.6890\n",
            "\n",
            "Epoch 1721: Validation loss decreased (222.689011 --> 222.617828).\n",
            "\t Train_Loss: 88.4787 Val_Loss: 222.6178  BEST VAL Loss: 222.6178\n",
            "\n",
            "Epoch 1722: Validation loss decreased (222.617828 --> 222.546249).\n",
            "\t Train_Loss: 88.4413 Val_Loss: 222.5462  BEST VAL Loss: 222.5462\n",
            "\n",
            "Epoch 1723: Validation loss decreased (222.546249 --> 222.475220).\n",
            "\t Train_Loss: 88.4039 Val_Loss: 222.4752  BEST VAL Loss: 222.4752\n",
            "\n",
            "Epoch 1724: Validation loss decreased (222.475220 --> 222.404037).\n",
            "\t Train_Loss: 88.3665 Val_Loss: 222.4040  BEST VAL Loss: 222.4040\n",
            "\n",
            "Epoch 1725: Validation loss decreased (222.404037 --> 222.332443).\n",
            "\t Train_Loss: 88.3291 Val_Loss: 222.3324  BEST VAL Loss: 222.3324\n",
            "\n",
            "Epoch 1726: Validation loss decreased (222.332443 --> 222.261353).\n",
            "\t Train_Loss: 88.2918 Val_Loss: 222.2614  BEST VAL Loss: 222.2614\n",
            "\n",
            "Epoch 1727: Validation loss decreased (222.261353 --> 222.190430).\n",
            "\t Train_Loss: 88.2545 Val_Loss: 222.1904  BEST VAL Loss: 222.1904\n",
            "\n",
            "Epoch 1728: Validation loss decreased (222.190430 --> 222.118927).\n",
            "\t Train_Loss: 88.2171 Val_Loss: 222.1189  BEST VAL Loss: 222.1189\n",
            "\n",
            "Epoch 1729: Validation loss decreased (222.118927 --> 222.047836).\n",
            "\t Train_Loss: 88.1799 Val_Loss: 222.0478  BEST VAL Loss: 222.0478\n",
            "\n",
            "Epoch 1730: Validation loss decreased (222.047836 --> 221.976883).\n",
            "\t Train_Loss: 88.1426 Val_Loss: 221.9769  BEST VAL Loss: 221.9769\n",
            "\n",
            "Epoch 1731: Validation loss decreased (221.976883 --> 221.905396).\n",
            "\t Train_Loss: 88.1054 Val_Loss: 221.9054  BEST VAL Loss: 221.9054\n",
            "\n",
            "Epoch 1732: Validation loss decreased (221.905396 --> 221.834717).\n",
            "\t Train_Loss: 88.0682 Val_Loss: 221.8347  BEST VAL Loss: 221.8347\n",
            "\n",
            "Epoch 1733: Validation loss decreased (221.834717 --> 221.763626).\n",
            "\t Train_Loss: 88.0310 Val_Loss: 221.7636  BEST VAL Loss: 221.7636\n",
            "\n",
            "Epoch 1734: Validation loss decreased (221.763626 --> 221.692245).\n",
            "\t Train_Loss: 87.9938 Val_Loss: 221.6922  BEST VAL Loss: 221.6922\n",
            "\n",
            "Epoch 1735: Validation loss decreased (221.692245 --> 221.621567).\n",
            "\t Train_Loss: 87.9567 Val_Loss: 221.6216  BEST VAL Loss: 221.6216\n",
            "\n",
            "Epoch 1736: Validation loss decreased (221.621567 --> 221.550583).\n",
            "\t Train_Loss: 87.9195 Val_Loss: 221.5506  BEST VAL Loss: 221.5506\n",
            "\n",
            "Epoch 1737: Validation loss decreased (221.550583 --> 221.479416).\n",
            "\t Train_Loss: 87.8824 Val_Loss: 221.4794  BEST VAL Loss: 221.4794\n",
            "\n",
            "Epoch 1738: Validation loss decreased (221.479416 --> 221.408691).\n",
            "\t Train_Loss: 87.8454 Val_Loss: 221.4087  BEST VAL Loss: 221.4087\n",
            "\n",
            "Epoch 1739: Validation loss decreased (221.408691 --> 221.337601).\n",
            "\t Train_Loss: 87.8083 Val_Loss: 221.3376  BEST VAL Loss: 221.3376\n",
            "\n",
            "Epoch 1740: Validation loss decreased (221.337601 --> 221.266632).\n",
            "\t Train_Loss: 87.7712 Val_Loss: 221.2666  BEST VAL Loss: 221.2666\n",
            "\n",
            "Epoch 1741: Validation loss decreased (221.266632 --> 221.195969).\n",
            "\t Train_Loss: 87.7342 Val_Loss: 221.1960  BEST VAL Loss: 221.1960\n",
            "\n",
            "Epoch 1742: Validation loss decreased (221.195969 --> 221.124710).\n",
            "\t Train_Loss: 87.6972 Val_Loss: 221.1247  BEST VAL Loss: 221.1247\n",
            "\n",
            "Epoch 1743: Validation loss decreased (221.124710 --> 221.054199).\n",
            "\t Train_Loss: 87.6603 Val_Loss: 221.0542  BEST VAL Loss: 221.0542\n",
            "\n",
            "Epoch 1744: Validation loss decreased (221.054199 --> 220.983200).\n",
            "\t Train_Loss: 87.6233 Val_Loss: 220.9832  BEST VAL Loss: 220.9832\n",
            "\n",
            "Epoch 1745: Validation loss decreased (220.983200 --> 220.912460).\n",
            "\t Train_Loss: 87.5864 Val_Loss: 220.9125  BEST VAL Loss: 220.9125\n",
            "\n",
            "Epoch 1746: Validation loss decreased (220.912460 --> 220.841904).\n",
            "\t Train_Loss: 87.5495 Val_Loss: 220.8419  BEST VAL Loss: 220.8419\n",
            "\n",
            "Epoch 1747: Validation loss decreased (220.841904 --> 220.770798).\n",
            "\t Train_Loss: 87.5126 Val_Loss: 220.7708  BEST VAL Loss: 220.7708\n",
            "\n",
            "Epoch 1748: Validation loss decreased (220.770798 --> 220.700439).\n",
            "\t Train_Loss: 87.4757 Val_Loss: 220.7004  BEST VAL Loss: 220.7004\n",
            "\n",
            "Epoch 1749: Validation loss decreased (220.700439 --> 220.629501).\n",
            "\t Train_Loss: 87.4389 Val_Loss: 220.6295  BEST VAL Loss: 220.6295\n",
            "\n",
            "Epoch 1750: Validation loss decreased (220.629501 --> 220.558868).\n",
            "\t Train_Loss: 87.4021 Val_Loss: 220.5589  BEST VAL Loss: 220.5589\n",
            "\n",
            "Epoch 1751: Validation loss decreased (220.558868 --> 220.488571).\n",
            "\t Train_Loss: 87.3653 Val_Loss: 220.4886  BEST VAL Loss: 220.4886\n",
            "\n",
            "Epoch 1752: Validation loss decreased (220.488571 --> 220.417587).\n",
            "\t Train_Loss: 87.3285 Val_Loss: 220.4176  BEST VAL Loss: 220.4176\n",
            "\n",
            "Epoch 1753: Validation loss decreased (220.417587 --> 220.347214).\n",
            "\t Train_Loss: 87.2918 Val_Loss: 220.3472  BEST VAL Loss: 220.3472\n",
            "\n",
            "Epoch 1754: Validation loss decreased (220.347214 --> 220.276230).\n",
            "\t Train_Loss: 87.2550 Val_Loss: 220.2762  BEST VAL Loss: 220.2762\n",
            "\n",
            "Epoch 1755: Validation loss decreased (220.276230 --> 220.206009).\n",
            "\t Train_Loss: 87.2183 Val_Loss: 220.2060  BEST VAL Loss: 220.2060\n",
            "\n",
            "Epoch 1756: Validation loss decreased (220.206009 --> 220.135361).\n",
            "\t Train_Loss: 87.1816 Val_Loss: 220.1354  BEST VAL Loss: 220.1354\n",
            "\n",
            "Epoch 1757: Validation loss decreased (220.135361 --> 220.064987).\n",
            "\t Train_Loss: 87.1450 Val_Loss: 220.0650  BEST VAL Loss: 220.0650\n",
            "\n",
            "Epoch 1758: Validation loss decreased (220.064987 --> 219.994537).\n",
            "\t Train_Loss: 87.1084 Val_Loss: 219.9945  BEST VAL Loss: 219.9945\n",
            "\n",
            "Epoch 1759: Validation loss decreased (219.994537 --> 219.923859).\n",
            "\t Train_Loss: 87.0717 Val_Loss: 219.9239  BEST VAL Loss: 219.9239\n",
            "\n",
            "Epoch 1760: Validation loss decreased (219.923859 --> 219.853683).\n",
            "\t Train_Loss: 87.0351 Val_Loss: 219.8537  BEST VAL Loss: 219.8537\n",
            "\n",
            "Epoch 1761: Validation loss decreased (219.853683 --> 219.783066).\n",
            "\t Train_Loss: 86.9986 Val_Loss: 219.7831  BEST VAL Loss: 219.7831\n",
            "\n",
            "Epoch 1762: Validation loss decreased (219.783066 --> 219.712967).\n",
            "\t Train_Loss: 86.9620 Val_Loss: 219.7130  BEST VAL Loss: 219.7130\n",
            "\n",
            "Epoch 1763: Validation loss decreased (219.712967 --> 219.642456).\n",
            "\t Train_Loss: 86.9254 Val_Loss: 219.6425  BEST VAL Loss: 219.6425\n",
            "\n",
            "Epoch 1764: Validation loss decreased (219.642456 --> 219.572342).\n",
            "\t Train_Loss: 86.8890 Val_Loss: 219.5723  BEST VAL Loss: 219.5723\n",
            "\n",
            "Epoch 1765: Validation loss decreased (219.572342 --> 219.501755).\n",
            "\t Train_Loss: 86.8525 Val_Loss: 219.5018  BEST VAL Loss: 219.5018\n",
            "\n",
            "Epoch 1766: Validation loss decreased (219.501755 --> 219.431564).\n",
            "\t Train_Loss: 86.8160 Val_Loss: 219.4316  BEST VAL Loss: 219.4316\n",
            "\n",
            "Epoch 1767: Validation loss decreased (219.431564 --> 219.361526).\n",
            "\t Train_Loss: 86.7795 Val_Loss: 219.3615  BEST VAL Loss: 219.3615\n",
            "\n",
            "Epoch 1768: Validation loss decreased (219.361526 --> 219.291138).\n",
            "\t Train_Loss: 86.7431 Val_Loss: 219.2911  BEST VAL Loss: 219.2911\n",
            "\n",
            "Epoch 1769: Validation loss decreased (219.291138 --> 219.221054).\n",
            "\t Train_Loss: 86.7067 Val_Loss: 219.2211  BEST VAL Loss: 219.2211\n",
            "\n",
            "Epoch 1770: Validation loss decreased (219.221054 --> 219.150620).\n",
            "\t Train_Loss: 86.6704 Val_Loss: 219.1506  BEST VAL Loss: 219.1506\n",
            "\n",
            "Epoch 1771: Validation loss decreased (219.150620 --> 219.080597).\n",
            "\t Train_Loss: 86.6340 Val_Loss: 219.0806  BEST VAL Loss: 219.0806\n",
            "\n",
            "Epoch 1772: Validation loss decreased (219.080597 --> 219.010361).\n",
            "\t Train_Loss: 86.5976 Val_Loss: 219.0104  BEST VAL Loss: 219.0104\n",
            "\n",
            "Epoch 1773: Validation loss decreased (219.010361 --> 218.940720).\n",
            "\t Train_Loss: 86.5613 Val_Loss: 218.9407  BEST VAL Loss: 218.9407\n",
            "\n",
            "Epoch 1774: Validation loss decreased (218.940720 --> 218.870224).\n",
            "\t Train_Loss: 86.5251 Val_Loss: 218.8702  BEST VAL Loss: 218.8702\n",
            "\n",
            "Epoch 1775: Validation loss decreased (218.870224 --> 218.800461).\n",
            "\t Train_Loss: 86.4888 Val_Loss: 218.8005  BEST VAL Loss: 218.8005\n",
            "\n",
            "Epoch 1776: Validation loss decreased (218.800461 --> 218.730026).\n",
            "\t Train_Loss: 86.4525 Val_Loss: 218.7300  BEST VAL Loss: 218.7300\n",
            "\n",
            "Epoch 1777: Validation loss decreased (218.730026 --> 218.660355).\n",
            "\t Train_Loss: 86.4163 Val_Loss: 218.6604  BEST VAL Loss: 218.6604\n",
            "\n",
            "Epoch 1778: Validation loss decreased (218.660355 --> 218.590134).\n",
            "\t Train_Loss: 86.3801 Val_Loss: 218.5901  BEST VAL Loss: 218.5901\n",
            "\n",
            "Epoch 1779: Validation loss decreased (218.590134 --> 218.520554).\n",
            "\t Train_Loss: 86.3439 Val_Loss: 218.5206  BEST VAL Loss: 218.5206\n",
            "\n",
            "Epoch 1780: Validation loss decreased (218.520554 --> 218.450241).\n",
            "\t Train_Loss: 86.3078 Val_Loss: 218.4502  BEST VAL Loss: 218.4502\n",
            "\n",
            "Epoch 1781: Validation loss decreased (218.450241 --> 218.380661).\n",
            "\t Train_Loss: 86.2716 Val_Loss: 218.3807  BEST VAL Loss: 218.3807\n",
            "\n",
            "Epoch 1782: Validation loss decreased (218.380661 --> 218.310532).\n",
            "\t Train_Loss: 86.2355 Val_Loss: 218.3105  BEST VAL Loss: 218.3105\n",
            "\n",
            "Epoch 1783: Validation loss decreased (218.310532 --> 218.241013).\n",
            "\t Train_Loss: 86.1994 Val_Loss: 218.2410  BEST VAL Loss: 218.2410\n",
            "\n",
            "Epoch 1784: Validation loss decreased (218.241013 --> 218.171051).\n",
            "\t Train_Loss: 86.1634 Val_Loss: 218.1711  BEST VAL Loss: 218.1711\n",
            "\n",
            "Epoch 1785: Validation loss decreased (218.171051 --> 218.101547).\n",
            "\t Train_Loss: 86.1273 Val_Loss: 218.1015  BEST VAL Loss: 218.1015\n",
            "\n",
            "Epoch 1786: Validation loss decreased (218.101547 --> 218.031540).\n",
            "\t Train_Loss: 86.0913 Val_Loss: 218.0315  BEST VAL Loss: 218.0315\n",
            "\n",
            "Epoch 1787: Validation loss decreased (218.031540 --> 217.962082).\n",
            "\t Train_Loss: 86.0553 Val_Loss: 217.9621  BEST VAL Loss: 217.9621\n",
            "\n",
            "Epoch 1788: Validation loss decreased (217.962082 --> 217.891998).\n",
            "\t Train_Loss: 86.0193 Val_Loss: 217.8920  BEST VAL Loss: 217.8920\n",
            "\n",
            "Epoch 1789: Validation loss decreased (217.891998 --> 217.822464).\n",
            "\t Train_Loss: 85.9834 Val_Loss: 217.8225  BEST VAL Loss: 217.8225\n",
            "\n",
            "Epoch 1790: Validation loss decreased (217.822464 --> 217.752731).\n",
            "\t Train_Loss: 85.9474 Val_Loss: 217.7527  BEST VAL Loss: 217.7527\n",
            "\n",
            "Epoch 1791: Validation loss decreased (217.752731 --> 217.683182).\n",
            "\t Train_Loss: 85.9115 Val_Loss: 217.6832  BEST VAL Loss: 217.6832\n",
            "\n",
            "Epoch 1792: Validation loss decreased (217.683182 --> 217.613480).\n",
            "\t Train_Loss: 85.8756 Val_Loss: 217.6135  BEST VAL Loss: 217.6135\n",
            "\n",
            "Epoch 1793: Validation loss decreased (217.613480 --> 217.544052).\n",
            "\t Train_Loss: 85.8397 Val_Loss: 217.5441  BEST VAL Loss: 217.5441\n",
            "\n",
            "Epoch 1794: Validation loss decreased (217.544052 --> 217.474274).\n",
            "\t Train_Loss: 85.8039 Val_Loss: 217.4743  BEST VAL Loss: 217.4743\n",
            "\n",
            "Epoch 1795: Validation loss decreased (217.474274 --> 217.404861).\n",
            "\t Train_Loss: 85.7681 Val_Loss: 217.4049  BEST VAL Loss: 217.4049\n",
            "\n",
            "Epoch 1796: Validation loss decreased (217.404861 --> 217.335281).\n",
            "\t Train_Loss: 85.7323 Val_Loss: 217.3353  BEST VAL Loss: 217.3353\n",
            "\n",
            "Epoch 1797: Validation loss decreased (217.335281 --> 217.266037).\n",
            "\t Train_Loss: 85.6965 Val_Loss: 217.2660  BEST VAL Loss: 217.2660\n",
            "\n",
            "Epoch 1798: Validation loss decreased (217.266037 --> 217.196579).\n",
            "\t Train_Loss: 85.6607 Val_Loss: 217.1966  BEST VAL Loss: 217.1966\n",
            "\n",
            "Epoch 1799: Validation loss decreased (217.196579 --> 217.126938).\n",
            "\t Train_Loss: 85.6250 Val_Loss: 217.1269  BEST VAL Loss: 217.1269\n",
            "\n",
            "Epoch 1800: Validation loss decreased (217.126938 --> 217.057724).\n",
            "\t Train_Loss: 85.5892 Val_Loss: 217.0577  BEST VAL Loss: 217.0577\n",
            "\n",
            "Epoch 1801: Validation loss decreased (217.057724 --> 216.988083).\n",
            "\t Train_Loss: 85.5536 Val_Loss: 216.9881  BEST VAL Loss: 216.9881\n",
            "\n",
            "Epoch 1802: Validation loss decreased (216.988083 --> 216.918900).\n",
            "\t Train_Loss: 85.5179 Val_Loss: 216.9189  BEST VAL Loss: 216.9189\n",
            "\n",
            "Epoch 1803: Validation loss decreased (216.918900 --> 216.849472).\n",
            "\t Train_Loss: 85.4822 Val_Loss: 216.8495  BEST VAL Loss: 216.8495\n",
            "\n",
            "Epoch 1804: Validation loss decreased (216.849472 --> 216.780426).\n",
            "\t Train_Loss: 85.4466 Val_Loss: 216.7804  BEST VAL Loss: 216.7804\n",
            "\n",
            "Epoch 1805: Validation loss decreased (216.780426 --> 216.711136).\n",
            "\t Train_Loss: 85.4110 Val_Loss: 216.7111  BEST VAL Loss: 216.7111\n",
            "\n",
            "Epoch 1806: Validation loss decreased (216.711136 --> 216.642090).\n",
            "\t Train_Loss: 85.3754 Val_Loss: 216.6421  BEST VAL Loss: 216.6421\n",
            "\n",
            "Epoch 1807: Validation loss decreased (216.642090 --> 216.572861).\n",
            "\t Train_Loss: 85.3399 Val_Loss: 216.5729  BEST VAL Loss: 216.5729\n",
            "\n",
            "Epoch 1808: Validation loss decreased (216.572861 --> 216.503708).\n",
            "\t Train_Loss: 85.3043 Val_Loss: 216.5037  BEST VAL Loss: 216.5037\n",
            "\n",
            "Epoch 1809: Validation loss decreased (216.503708 --> 216.434494).\n",
            "\t Train_Loss: 85.2688 Val_Loss: 216.4345  BEST VAL Loss: 216.4345\n",
            "\n",
            "Epoch 1810: Validation loss decreased (216.434494 --> 216.365112).\n",
            "\t Train_Loss: 85.2333 Val_Loss: 216.3651  BEST VAL Loss: 216.3651\n",
            "\n",
            "Epoch 1811: Validation loss decreased (216.365112 --> 216.296097).\n",
            "\t Train_Loss: 85.1978 Val_Loss: 216.2961  BEST VAL Loss: 216.2961\n",
            "\n",
            "Epoch 1812: Validation loss decreased (216.296097 --> 216.227051).\n",
            "\t Train_Loss: 85.1624 Val_Loss: 216.2271  BEST VAL Loss: 216.2271\n",
            "\n",
            "Epoch 1813: Validation loss decreased (216.227051 --> 216.158188).\n",
            "\t Train_Loss: 85.1269 Val_Loss: 216.1582  BEST VAL Loss: 216.1582\n",
            "\n",
            "Epoch 1814: Validation loss decreased (216.158188 --> 216.089020).\n",
            "\t Train_Loss: 85.0915 Val_Loss: 216.0890  BEST VAL Loss: 216.0890\n",
            "\n",
            "Epoch 1815: Validation loss decreased (216.089020 --> 216.019928).\n",
            "\t Train_Loss: 85.0562 Val_Loss: 216.0199  BEST VAL Loss: 216.0199\n",
            "\n",
            "Epoch 1816: Validation loss decreased (216.019928 --> 215.951004).\n",
            "\t Train_Loss: 85.0208 Val_Loss: 215.9510  BEST VAL Loss: 215.9510\n",
            "\n",
            "Epoch 1817: Validation loss decreased (215.951004 --> 215.882080).\n",
            "\t Train_Loss: 84.9854 Val_Loss: 215.8821  BEST VAL Loss: 215.8821\n",
            "\n",
            "Epoch 1818: Validation loss decreased (215.882080 --> 215.813263).\n",
            "\t Train_Loss: 84.9501 Val_Loss: 215.8133  BEST VAL Loss: 215.8133\n",
            "\n",
            "Epoch 1819: Validation loss decreased (215.813263 --> 215.744385).\n",
            "\t Train_Loss: 84.9148 Val_Loss: 215.7444  BEST VAL Loss: 215.7444\n",
            "\n",
            "Epoch 1820: Validation loss decreased (215.744385 --> 215.675583).\n",
            "\t Train_Loss: 84.8795 Val_Loss: 215.6756  BEST VAL Loss: 215.6756\n",
            "\n",
            "Epoch 1821: Validation loss decreased (215.675583 --> 215.606567).\n",
            "\t Train_Loss: 84.8443 Val_Loss: 215.6066  BEST VAL Loss: 215.6066\n",
            "\n",
            "Epoch 1822: Validation loss decreased (215.606567 --> 215.537704).\n",
            "\t Train_Loss: 84.8091 Val_Loss: 215.5377  BEST VAL Loss: 215.5377\n",
            "\n",
            "Epoch 1823: Validation loss decreased (215.537704 --> 215.469040).\n",
            "\t Train_Loss: 84.7739 Val_Loss: 215.4690  BEST VAL Loss: 215.4690\n",
            "\n",
            "Epoch 1824: Validation loss decreased (215.469040 --> 215.400436).\n",
            "\t Train_Loss: 84.7387 Val_Loss: 215.4004  BEST VAL Loss: 215.4004\n",
            "\n",
            "Epoch 1825: Validation loss decreased (215.400436 --> 215.331833).\n",
            "\t Train_Loss: 84.7035 Val_Loss: 215.3318  BEST VAL Loss: 215.3318\n",
            "\n",
            "Epoch 1826: Validation loss decreased (215.331833 --> 215.263016).\n",
            "\t Train_Loss: 84.6684 Val_Loss: 215.2630  BEST VAL Loss: 215.2630\n",
            "\n",
            "Epoch 1827: Validation loss decreased (215.263016 --> 215.194336).\n",
            "\t Train_Loss: 84.6333 Val_Loss: 215.1943  BEST VAL Loss: 215.1943\n",
            "\n",
            "Epoch 1828: Validation loss decreased (215.194336 --> 215.125641).\n",
            "\t Train_Loss: 84.5982 Val_Loss: 215.1256  BEST VAL Loss: 215.1256\n",
            "\n",
            "Epoch 1829: Validation loss decreased (215.125641 --> 215.057281).\n",
            "\t Train_Loss: 84.5631 Val_Loss: 215.0573  BEST VAL Loss: 215.0573\n",
            "\n",
            "Epoch 1830: Validation loss decreased (215.057281 --> 214.988510).\n",
            "\t Train_Loss: 84.5280 Val_Loss: 214.9885  BEST VAL Loss: 214.9885\n",
            "\n",
            "Epoch 1831: Validation loss decreased (214.988510 --> 214.920212).\n",
            "\t Train_Loss: 84.4930 Val_Loss: 214.9202  BEST VAL Loss: 214.9202\n",
            "\n",
            "Epoch 1832: Validation loss decreased (214.920212 --> 214.851761).\n",
            "\t Train_Loss: 84.4580 Val_Loss: 214.8518  BEST VAL Loss: 214.8518\n",
            "\n",
            "Epoch 1833: Validation loss decreased (214.851761 --> 214.783112).\n",
            "\t Train_Loss: 84.4230 Val_Loss: 214.7831  BEST VAL Loss: 214.7831\n",
            "\n",
            "Epoch 1834: Validation loss decreased (214.783112 --> 214.714630).\n",
            "\t Train_Loss: 84.3880 Val_Loss: 214.7146  BEST VAL Loss: 214.7146\n",
            "\n",
            "Epoch 1835: Validation loss decreased (214.714630 --> 214.646118).\n",
            "\t Train_Loss: 84.3531 Val_Loss: 214.6461  BEST VAL Loss: 214.6461\n",
            "\n",
            "Epoch 1836: Validation loss decreased (214.646118 --> 214.577545).\n",
            "\t Train_Loss: 84.3181 Val_Loss: 214.5775  BEST VAL Loss: 214.5775\n",
            "\n",
            "Epoch 1837: Validation loss decreased (214.577545 --> 214.509277).\n",
            "\t Train_Loss: 84.2833 Val_Loss: 214.5093  BEST VAL Loss: 214.5093\n",
            "\n",
            "Epoch 1838: Validation loss decreased (214.509277 --> 214.440796).\n",
            "\t Train_Loss: 84.2484 Val_Loss: 214.4408  BEST VAL Loss: 214.4408\n",
            "\n",
            "Epoch 1839: Validation loss decreased (214.440796 --> 214.372757).\n",
            "\t Train_Loss: 84.2136 Val_Loss: 214.3728  BEST VAL Loss: 214.3728\n",
            "\n",
            "Epoch 1840: Validation loss decreased (214.372757 --> 214.304352).\n",
            "\t Train_Loss: 84.1787 Val_Loss: 214.3044  BEST VAL Loss: 214.3044\n",
            "\n",
            "Epoch 1841: Validation loss decreased (214.304352 --> 214.235840).\n",
            "\t Train_Loss: 84.1439 Val_Loss: 214.2358  BEST VAL Loss: 214.2358\n",
            "\n",
            "Epoch 1842: Validation loss decreased (214.235840 --> 214.167847).\n",
            "\t Train_Loss: 84.1091 Val_Loss: 214.1678  BEST VAL Loss: 214.1678\n",
            "\n",
            "Epoch 1843: Validation loss decreased (214.167847 --> 214.099518).\n",
            "\t Train_Loss: 84.0743 Val_Loss: 214.0995  BEST VAL Loss: 214.0995\n",
            "\n",
            "Epoch 1844: Validation loss decreased (214.099518 --> 214.031235).\n",
            "\t Train_Loss: 84.0396 Val_Loss: 214.0312  BEST VAL Loss: 214.0312\n",
            "\n",
            "Epoch 1845: Validation loss decreased (214.031235 --> 213.963165).\n",
            "\t Train_Loss: 84.0049 Val_Loss: 213.9632  BEST VAL Loss: 213.9632\n",
            "\n",
            "Epoch 1846: Validation loss decreased (213.963165 --> 213.894882).\n",
            "\t Train_Loss: 83.9702 Val_Loss: 213.8949  BEST VAL Loss: 213.8949\n",
            "\n",
            "Epoch 1847: Validation loss decreased (213.894882 --> 213.826859).\n",
            "\t Train_Loss: 83.9355 Val_Loss: 213.8269  BEST VAL Loss: 213.8269\n",
            "\n",
            "Epoch 1848: Validation loss decreased (213.826859 --> 213.759033).\n",
            "\t Train_Loss: 83.9008 Val_Loss: 213.7590  BEST VAL Loss: 213.7590\n",
            "\n",
            "Epoch 1849: Validation loss decreased (213.759033 --> 213.690628).\n",
            "\t Train_Loss: 83.8662 Val_Loss: 213.6906  BEST VAL Loss: 213.6906\n",
            "\n",
            "Epoch 1850: Validation loss decreased (213.690628 --> 213.622955).\n",
            "\t Train_Loss: 83.8316 Val_Loss: 213.6230  BEST VAL Loss: 213.6230\n",
            "\n",
            "Epoch 1851: Validation loss decreased (213.622955 --> 213.554840).\n",
            "\t Train_Loss: 83.7970 Val_Loss: 213.5548  BEST VAL Loss: 213.5548\n",
            "\n",
            "Epoch 1852: Validation loss decreased (213.554840 --> 213.486679).\n",
            "\t Train_Loss: 83.7625 Val_Loss: 213.4867  BEST VAL Loss: 213.4867\n",
            "\n",
            "Epoch 1853: Validation loss decreased (213.486679 --> 213.419098).\n",
            "\t Train_Loss: 83.7279 Val_Loss: 213.4191  BEST VAL Loss: 213.4191\n",
            "\n",
            "Epoch 1854: Validation loss decreased (213.419098 --> 213.350983).\n",
            "\t Train_Loss: 83.6934 Val_Loss: 213.3510  BEST VAL Loss: 213.3510\n",
            "\n",
            "Epoch 1855: Validation loss decreased (213.350983 --> 213.283112).\n",
            "\t Train_Loss: 83.6589 Val_Loss: 213.2831  BEST VAL Loss: 213.2831\n",
            "\n",
            "Epoch 1856: Validation loss decreased (213.283112 --> 213.215576).\n",
            "\t Train_Loss: 83.6244 Val_Loss: 213.2156  BEST VAL Loss: 213.2156\n",
            "\n",
            "Epoch 1857: Validation loss decreased (213.215576 --> 213.147491).\n",
            "\t Train_Loss: 83.5900 Val_Loss: 213.1475  BEST VAL Loss: 213.1475\n",
            "\n",
            "Epoch 1858: Validation loss decreased (213.147491 --> 213.079697).\n",
            "\t Train_Loss: 83.5555 Val_Loss: 213.0797  BEST VAL Loss: 213.0797\n",
            "\n",
            "Epoch 1859: Validation loss decreased (213.079697 --> 213.011963).\n",
            "\t Train_Loss: 83.5211 Val_Loss: 213.0120  BEST VAL Loss: 213.0120\n",
            "\n",
            "Epoch 1860: Validation loss decreased (213.011963 --> 212.944290).\n",
            "\t Train_Loss: 83.4867 Val_Loss: 212.9443  BEST VAL Loss: 212.9443\n",
            "\n",
            "Epoch 1861: Validation loss decreased (212.944290 --> 212.876816).\n",
            "\t Train_Loss: 83.4524 Val_Loss: 212.8768  BEST VAL Loss: 212.8768\n",
            "\n",
            "Epoch 1862: Validation loss decreased (212.876816 --> 212.808868).\n",
            "\t Train_Loss: 83.4180 Val_Loss: 212.8089  BEST VAL Loss: 212.8089\n",
            "\n",
            "Epoch 1863: Validation loss decreased (212.808868 --> 212.741364).\n",
            "\t Train_Loss: 83.3837 Val_Loss: 212.7414  BEST VAL Loss: 212.7414\n",
            "\n",
            "Epoch 1864: Validation loss decreased (212.741364 --> 212.673859).\n",
            "\t Train_Loss: 83.3494 Val_Loss: 212.6739  BEST VAL Loss: 212.6739\n",
            "\n",
            "Epoch 1865: Validation loss decreased (212.673859 --> 212.606033).\n",
            "\t Train_Loss: 83.3151 Val_Loss: 212.6060  BEST VAL Loss: 212.6060\n",
            "\n",
            "Epoch 1866: Validation loss decreased (212.606033 --> 212.538406).\n",
            "\t Train_Loss: 83.2809 Val_Loss: 212.5384  BEST VAL Loss: 212.5384\n",
            "\n",
            "Epoch 1867: Validation loss decreased (212.538406 --> 212.471100).\n",
            "\t Train_Loss: 83.2466 Val_Loss: 212.4711  BEST VAL Loss: 212.4711\n",
            "\n",
            "Epoch 1868: Validation loss decreased (212.471100 --> 212.403427).\n",
            "\t Train_Loss: 83.2124 Val_Loss: 212.4034  BEST VAL Loss: 212.4034\n",
            "\n",
            "Epoch 1869: Validation loss decreased (212.403427 --> 212.335815).\n",
            "\t Train_Loss: 83.1782 Val_Loss: 212.3358  BEST VAL Loss: 212.3358\n",
            "\n",
            "Epoch 1870: Validation loss decreased (212.335815 --> 212.268433).\n",
            "\t Train_Loss: 83.1441 Val_Loss: 212.2684  BEST VAL Loss: 212.2684\n",
            "\n",
            "Epoch 1871: Validation loss decreased (212.268433 --> 212.200974).\n",
            "\t Train_Loss: 83.1099 Val_Loss: 212.2010  BEST VAL Loss: 212.2010\n",
            "\n",
            "Epoch 1872: Validation loss decreased (212.200974 --> 212.133713).\n",
            "\t Train_Loss: 83.0758 Val_Loss: 212.1337  BEST VAL Loss: 212.1337\n",
            "\n",
            "Epoch 1873: Validation loss decreased (212.133713 --> 212.066269).\n",
            "\t Train_Loss: 83.0417 Val_Loss: 212.0663  BEST VAL Loss: 212.0663\n",
            "\n",
            "Epoch 1874: Validation loss decreased (212.066269 --> 211.998947).\n",
            "\t Train_Loss: 83.0076 Val_Loss: 211.9989  BEST VAL Loss: 211.9989\n",
            "\n",
            "Epoch 1875: Validation loss decreased (211.998947 --> 211.931519).\n",
            "\t Train_Loss: 82.9736 Val_Loss: 211.9315  BEST VAL Loss: 211.9315\n",
            "\n",
            "Epoch 1876: Validation loss decreased (211.931519 --> 211.864136).\n",
            "\t Train_Loss: 82.9395 Val_Loss: 211.8641  BEST VAL Loss: 211.8641\n",
            "\n",
            "Epoch 1877: Validation loss decreased (211.864136 --> 211.797028).\n",
            "\t Train_Loss: 82.9055 Val_Loss: 211.7970  BEST VAL Loss: 211.7970\n",
            "\n",
            "Epoch 1878: Validation loss decreased (211.797028 --> 211.729721).\n",
            "\t Train_Loss: 82.8715 Val_Loss: 211.7297  BEST VAL Loss: 211.7297\n",
            "\n",
            "Epoch 1879: Validation loss decreased (211.729721 --> 211.662354).\n",
            "\t Train_Loss: 82.8375 Val_Loss: 211.6624  BEST VAL Loss: 211.6624\n",
            "\n",
            "Epoch 1880: Validation loss decreased (211.662354 --> 211.595413).\n",
            "\t Train_Loss: 82.8036 Val_Loss: 211.5954  BEST VAL Loss: 211.5954\n",
            "\n",
            "Epoch 1881: Validation loss decreased (211.595413 --> 211.528397).\n",
            "\t Train_Loss: 82.7697 Val_Loss: 211.5284  BEST VAL Loss: 211.5284\n",
            "\n",
            "Epoch 1882: Validation loss decreased (211.528397 --> 211.461166).\n",
            "\t Train_Loss: 82.7358 Val_Loss: 211.4612  BEST VAL Loss: 211.4612\n",
            "\n",
            "Epoch 1883: Validation loss decreased (211.461166 --> 211.394211).\n",
            "\t Train_Loss: 82.7019 Val_Loss: 211.3942  BEST VAL Loss: 211.3942\n",
            "\n",
            "Epoch 1884: Validation loss decreased (211.394211 --> 211.327148).\n",
            "\t Train_Loss: 82.6681 Val_Loss: 211.3271  BEST VAL Loss: 211.3271\n",
            "\n",
            "Epoch 1885: Validation loss decreased (211.327148 --> 211.260010).\n",
            "\t Train_Loss: 82.6342 Val_Loss: 211.2600  BEST VAL Loss: 211.2600\n",
            "\n",
            "Epoch 1886: Validation loss decreased (211.260010 --> 211.192978).\n",
            "\t Train_Loss: 82.6004 Val_Loss: 211.1930  BEST VAL Loss: 211.1930\n",
            "\n",
            "Epoch 1887: Validation loss decreased (211.192978 --> 211.126129).\n",
            "\t Train_Loss: 82.5666 Val_Loss: 211.1261  BEST VAL Loss: 211.1261\n",
            "\n",
            "Epoch 1888: Validation loss decreased (211.126129 --> 211.058990).\n",
            "\t Train_Loss: 82.5329 Val_Loss: 211.0590  BEST VAL Loss: 211.0590\n",
            "\n",
            "Epoch 1889: Validation loss decreased (211.058990 --> 210.991852).\n",
            "\t Train_Loss: 82.4991 Val_Loss: 210.9919  BEST VAL Loss: 210.9919\n",
            "\n",
            "Epoch 1890: Validation loss decreased (210.991852 --> 210.925644).\n",
            "\t Train_Loss: 82.4654 Val_Loss: 210.9256  BEST VAL Loss: 210.9256\n",
            "\n",
            "Epoch 1891: Validation loss decreased (210.925644 --> 210.858551).\n",
            "\t Train_Loss: 82.4317 Val_Loss: 210.8586  BEST VAL Loss: 210.8586\n",
            "\n",
            "Epoch 1892: Validation loss decreased (210.858551 --> 210.791504).\n",
            "\t Train_Loss: 82.3980 Val_Loss: 210.7915  BEST VAL Loss: 210.7915\n",
            "\n",
            "Epoch 1893: Validation loss decreased (210.791504 --> 210.725204).\n",
            "\t Train_Loss: 82.3644 Val_Loss: 210.7252  BEST VAL Loss: 210.7252\n",
            "\n",
            "Epoch 1894: Validation loss decreased (210.725204 --> 210.658112).\n",
            "\t Train_Loss: 82.3307 Val_Loss: 210.6581  BEST VAL Loss: 210.6581\n",
            "\n",
            "Epoch 1895: Validation loss decreased (210.658112 --> 210.591187).\n",
            "\t Train_Loss: 82.2971 Val_Loss: 210.5912  BEST VAL Loss: 210.5912\n",
            "\n",
            "Epoch 1896: Validation loss decreased (210.591187 --> 210.524811).\n",
            "\t Train_Loss: 82.2635 Val_Loss: 210.5248  BEST VAL Loss: 210.5248\n",
            "\n",
            "Epoch 1897: Validation loss decreased (210.524811 --> 210.458054).\n",
            "\t Train_Loss: 82.2300 Val_Loss: 210.4581  BEST VAL Loss: 210.4581\n",
            "\n",
            "Epoch 1898: Validation loss decreased (210.458054 --> 210.391312).\n",
            "\t Train_Loss: 82.1964 Val_Loss: 210.3913  BEST VAL Loss: 210.3913\n",
            "\n",
            "Epoch 1899: Validation loss decreased (210.391312 --> 210.325058).\n",
            "\t Train_Loss: 82.1629 Val_Loss: 210.3251  BEST VAL Loss: 210.3251\n",
            "\n",
            "Epoch 1900: Validation loss decreased (210.325058 --> 210.258545).\n",
            "\t Train_Loss: 82.1294 Val_Loss: 210.2585  BEST VAL Loss: 210.2585\n",
            "\n",
            "Epoch 1901: Validation loss decreased (210.258545 --> 210.191727).\n",
            "\t Train_Loss: 82.0959 Val_Loss: 210.1917  BEST VAL Loss: 210.1917\n",
            "\n",
            "Epoch 1902: Validation loss decreased (210.191727 --> 210.125488).\n",
            "\t Train_Loss: 82.0624 Val_Loss: 210.1255  BEST VAL Loss: 210.1255\n",
            "\n",
            "Epoch 1903: Validation loss decreased (210.125488 --> 210.058823).\n",
            "\t Train_Loss: 82.0290 Val_Loss: 210.0588  BEST VAL Loss: 210.0588\n",
            "\n",
            "Epoch 1904: Validation loss decreased (210.058823 --> 209.992340).\n",
            "\t Train_Loss: 81.9956 Val_Loss: 209.9923  BEST VAL Loss: 209.9923\n",
            "\n",
            "Epoch 1905: Validation loss decreased (209.992340 --> 209.925568).\n",
            "\t Train_Loss: 81.9622 Val_Loss: 209.9256  BEST VAL Loss: 209.9256\n",
            "\n",
            "Epoch 1906: Validation loss decreased (209.925568 --> 209.859543).\n",
            "\t Train_Loss: 81.9289 Val_Loss: 209.8595  BEST VAL Loss: 209.8595\n",
            "\n",
            "Epoch 1907: Validation loss decreased (209.859543 --> 209.793289).\n",
            "\t Train_Loss: 81.8955 Val_Loss: 209.7933  BEST VAL Loss: 209.7933\n",
            "\n",
            "Epoch 1908: Validation loss decreased (209.793289 --> 209.726593).\n",
            "\t Train_Loss: 81.8622 Val_Loss: 209.7266  BEST VAL Loss: 209.7266\n",
            "\n",
            "Epoch 1909: Validation loss decreased (209.726593 --> 209.660751).\n",
            "\t Train_Loss: 81.8289 Val_Loss: 209.6608  BEST VAL Loss: 209.6608\n",
            "\n",
            "Epoch 1910: Validation loss decreased (209.660751 --> 209.594513).\n",
            "\t Train_Loss: 81.7956 Val_Loss: 209.5945  BEST VAL Loss: 209.5945\n",
            "\n",
            "Epoch 1911: Validation loss decreased (209.594513 --> 209.527863).\n",
            "\t Train_Loss: 81.7624 Val_Loss: 209.5279  BEST VAL Loss: 209.5279\n",
            "\n",
            "Epoch 1912: Validation loss decreased (209.527863 --> 209.462112).\n",
            "\t Train_Loss: 81.7291 Val_Loss: 209.4621  BEST VAL Loss: 209.4621\n",
            "\n",
            "Epoch 1913: Validation loss decreased (209.462112 --> 209.395752).\n",
            "\t Train_Loss: 81.6959 Val_Loss: 209.3958  BEST VAL Loss: 209.3958\n",
            "\n",
            "Epoch 1914: Validation loss decreased (209.395752 --> 209.329391).\n",
            "\t Train_Loss: 81.6627 Val_Loss: 209.3294  BEST VAL Loss: 209.3294\n",
            "\n",
            "Epoch 1915: Validation loss decreased (209.329391 --> 209.263626).\n",
            "\t Train_Loss: 81.6295 Val_Loss: 209.2636  BEST VAL Loss: 209.2636\n",
            "\n",
            "Epoch 1916: Validation loss decreased (209.263626 --> 209.197464).\n",
            "\t Train_Loss: 81.5964 Val_Loss: 209.1975  BEST VAL Loss: 209.1975\n",
            "\n",
            "Epoch 1917: Validation loss decreased (209.197464 --> 209.131302).\n",
            "\t Train_Loss: 81.5633 Val_Loss: 209.1313  BEST VAL Loss: 209.1313\n",
            "\n",
            "Epoch 1918: Validation loss decreased (209.131302 --> 209.065384).\n",
            "\t Train_Loss: 81.5302 Val_Loss: 209.0654  BEST VAL Loss: 209.0654\n",
            "\n",
            "Epoch 1919: Validation loss decreased (209.065384 --> 208.999466).\n",
            "\t Train_Loss: 81.4971 Val_Loss: 208.9995  BEST VAL Loss: 208.9995\n",
            "\n",
            "Epoch 1920: Validation loss decreased (208.999466 --> 208.933212).\n",
            "\t Train_Loss: 81.4640 Val_Loss: 208.9332  BEST VAL Loss: 208.9332\n",
            "\n",
            "Epoch 1921: Validation loss decreased (208.933212 --> 208.867050).\n",
            "\t Train_Loss: 81.4310 Val_Loss: 208.8671  BEST VAL Loss: 208.8671\n",
            "\n",
            "Epoch 1922: Validation loss decreased (208.867050 --> 208.801132).\n",
            "\t Train_Loss: 81.3980 Val_Loss: 208.8011  BEST VAL Loss: 208.8011\n",
            "\n",
            "Epoch 1923: Validation loss decreased (208.801132 --> 208.735306).\n",
            "\t Train_Loss: 81.3650 Val_Loss: 208.7353  BEST VAL Loss: 208.7353\n",
            "\n",
            "Epoch 1924: Validation loss decreased (208.735306 --> 208.669434).\n",
            "\t Train_Loss: 81.3321 Val_Loss: 208.6694  BEST VAL Loss: 208.6694\n",
            "\n",
            "Epoch 1925: Validation loss decreased (208.669434 --> 208.603317).\n",
            "\t Train_Loss: 81.2991 Val_Loss: 208.6033  BEST VAL Loss: 208.6033\n",
            "\n",
            "Epoch 1926: Validation loss decreased (208.603317 --> 208.537994).\n",
            "\t Train_Loss: 81.2662 Val_Loss: 208.5380  BEST VAL Loss: 208.5380\n",
            "\n",
            "Epoch 1927: Validation loss decreased (208.537994 --> 208.471848).\n",
            "\t Train_Loss: 81.2333 Val_Loss: 208.4718  BEST VAL Loss: 208.4718\n",
            "\n",
            "Epoch 1928: Validation loss decreased (208.471848 --> 208.406052).\n",
            "\t Train_Loss: 81.2004 Val_Loss: 208.4061  BEST VAL Loss: 208.4061\n",
            "\n",
            "Epoch 1929: Validation loss decreased (208.406052 --> 208.340622).\n",
            "\t Train_Loss: 81.1675 Val_Loss: 208.3406  BEST VAL Loss: 208.3406\n",
            "\n",
            "Epoch 1930: Validation loss decreased (208.340622 --> 208.274765).\n",
            "\t Train_Loss: 81.1347 Val_Loss: 208.2748  BEST VAL Loss: 208.2748\n",
            "\n",
            "Epoch 1931: Validation loss decreased (208.274765 --> 208.208908).\n",
            "\t Train_Loss: 81.1019 Val_Loss: 208.2089  BEST VAL Loss: 208.2089\n",
            "\n",
            "Epoch 1932: Validation loss decreased (208.208908 --> 208.143173).\n",
            "\t Train_Loss: 81.0691 Val_Loss: 208.1432  BEST VAL Loss: 208.1432\n",
            "\n",
            "Epoch 1933: Validation loss decreased (208.143173 --> 208.078079).\n",
            "\t Train_Loss: 81.0363 Val_Loss: 208.0781  BEST VAL Loss: 208.0781\n",
            "\n",
            "Epoch 1934: Validation loss decreased (208.078079 --> 208.012131).\n",
            "\t Train_Loss: 81.0036 Val_Loss: 208.0121  BEST VAL Loss: 208.0121\n",
            "\n",
            "Epoch 1935: Validation loss decreased (208.012131 --> 207.946533).\n",
            "\t Train_Loss: 80.9709 Val_Loss: 207.9465  BEST VAL Loss: 207.9465\n",
            "\n",
            "Epoch 1936: Validation loss decreased (207.946533 --> 207.881104).\n",
            "\t Train_Loss: 80.9382 Val_Loss: 207.8811  BEST VAL Loss: 207.8811\n",
            "\n",
            "Epoch 1937: Validation loss decreased (207.881104 --> 207.815720).\n",
            "\t Train_Loss: 80.9055 Val_Loss: 207.8157  BEST VAL Loss: 207.8157\n",
            "\n",
            "Epoch 1938: Validation loss decreased (207.815720 --> 207.749954).\n",
            "\t Train_Loss: 80.8728 Val_Loss: 207.7500  BEST VAL Loss: 207.7500\n",
            "\n",
            "Epoch 1939: Validation loss decreased (207.749954 --> 207.684525).\n",
            "\t Train_Loss: 80.8402 Val_Loss: 207.6845  BEST VAL Loss: 207.6845\n",
            "\n",
            "Epoch 1940: Validation loss decreased (207.684525 --> 207.619614).\n",
            "\t Train_Loss: 80.8076 Val_Loss: 207.6196  BEST VAL Loss: 207.6196\n",
            "\n",
            "Epoch 1941: Validation loss decreased (207.619614 --> 207.553986).\n",
            "\t Train_Loss: 80.7750 Val_Loss: 207.5540  BEST VAL Loss: 207.5540\n",
            "\n",
            "Epoch 1942: Validation loss decreased (207.553986 --> 207.488434).\n",
            "\t Train_Loss: 80.7425 Val_Loss: 207.4884  BEST VAL Loss: 207.4884\n",
            "\n",
            "Epoch 1943: Validation loss decreased (207.488434 --> 207.423325).\n",
            "\t Train_Loss: 80.7099 Val_Loss: 207.4233  BEST VAL Loss: 207.4233\n",
            "\n",
            "Epoch 1944: Validation loss decreased (207.423325 --> 207.358200).\n",
            "\t Train_Loss: 80.6774 Val_Loss: 207.3582  BEST VAL Loss: 207.3582\n",
            "\n",
            "Epoch 1945: Validation loss decreased (207.358200 --> 207.292587).\n",
            "\t Train_Loss: 80.6449 Val_Loss: 207.2926  BEST VAL Loss: 207.2926\n",
            "\n",
            "Epoch 1946: Validation loss decreased (207.292587 --> 207.227249).\n",
            "\t Train_Loss: 80.6124 Val_Loss: 207.2272  BEST VAL Loss: 207.2272\n",
            "\n",
            "Epoch 1947: Validation loss decreased (207.227249 --> 207.162521).\n",
            "\t Train_Loss: 80.5799 Val_Loss: 207.1625  BEST VAL Loss: 207.1625\n",
            "\n",
            "Epoch 1948: Validation loss decreased (207.162521 --> 207.097168).\n",
            "\t Train_Loss: 80.5475 Val_Loss: 207.0972  BEST VAL Loss: 207.0972\n",
            "\n",
            "Epoch 1949: Validation loss decreased (207.097168 --> 207.031891).\n",
            "\t Train_Loss: 80.5151 Val_Loss: 207.0319  BEST VAL Loss: 207.0319\n",
            "\n",
            "Epoch 1950: Validation loss decreased (207.031891 --> 206.967087).\n",
            "\t Train_Loss: 80.4827 Val_Loss: 206.9671  BEST VAL Loss: 206.9671\n",
            "\n",
            "Epoch 1951: Validation loss decreased (206.967087 --> 206.902176).\n",
            "\t Train_Loss: 80.4504 Val_Loss: 206.9022  BEST VAL Loss: 206.9022\n",
            "\n",
            "Epoch 1952: Validation loss decreased (206.902176 --> 206.836777).\n",
            "\t Train_Loss: 80.4180 Val_Loss: 206.8368  BEST VAL Loss: 206.8368\n",
            "\n",
            "Epoch 1953: Validation loss decreased (206.836777 --> 206.771606).\n",
            "\t Train_Loss: 80.3857 Val_Loss: 206.7716  BEST VAL Loss: 206.7716\n",
            "\n",
            "Epoch 1954: Validation loss decreased (206.771606 --> 206.707031).\n",
            "\t Train_Loss: 80.3534 Val_Loss: 206.7070  BEST VAL Loss: 206.7070\n",
            "\n",
            "Epoch 1955: Validation loss decreased (206.707031 --> 206.641968).\n",
            "\t Train_Loss: 80.3211 Val_Loss: 206.6420  BEST VAL Loss: 206.6420\n",
            "\n",
            "Epoch 1956: Validation loss decreased (206.641968 --> 206.576813).\n",
            "\t Train_Loss: 80.2889 Val_Loss: 206.5768  BEST VAL Loss: 206.5768\n",
            "\n",
            "Epoch 1957: Validation loss decreased (206.576813 --> 206.511719).\n",
            "\t Train_Loss: 80.2566 Val_Loss: 206.5117  BEST VAL Loss: 206.5117\n",
            "\n",
            "Epoch 1958: Validation loss decreased (206.511719 --> 206.447540).\n",
            "\t Train_Loss: 80.2244 Val_Loss: 206.4475  BEST VAL Loss: 206.4475\n",
            "\n",
            "Epoch 1959: Validation loss decreased (206.447540 --> 206.382431).\n",
            "\t Train_Loss: 80.1922 Val_Loss: 206.3824  BEST VAL Loss: 206.3824\n",
            "\n",
            "Epoch 1960: Validation loss decreased (206.382431 --> 206.317490).\n",
            "\t Train_Loss: 80.1601 Val_Loss: 206.3175  BEST VAL Loss: 206.3175\n",
            "\n",
            "Epoch 1961: Validation loss decreased (206.317490 --> 206.253052).\n",
            "\t Train_Loss: 80.1279 Val_Loss: 206.2531  BEST VAL Loss: 206.2531\n",
            "\n",
            "Epoch 1962: Validation loss decreased (206.253052 --> 206.188232).\n",
            "\t Train_Loss: 80.0958 Val_Loss: 206.1882  BEST VAL Loss: 206.1882\n",
            "\n",
            "Epoch 1963: Validation loss decreased (206.188232 --> 206.123489).\n",
            "\t Train_Loss: 80.0637 Val_Loss: 206.1235  BEST VAL Loss: 206.1235\n",
            "\n",
            "Epoch 1964: Validation loss decreased (206.123489 --> 206.058624).\n",
            "\t Train_Loss: 80.0316 Val_Loss: 206.0586  BEST VAL Loss: 206.0586\n",
            "\n",
            "Epoch 1965: Validation loss decreased (206.058624 --> 205.994537).\n",
            "\t Train_Loss: 79.9995 Val_Loss: 205.9945  BEST VAL Loss: 205.9945\n",
            "\n",
            "Epoch 1966: Validation loss decreased (205.994537 --> 205.929688).\n",
            "\t Train_Loss: 79.9675 Val_Loss: 205.9297  BEST VAL Loss: 205.9297\n",
            "\n",
            "Epoch 1967: Validation loss decreased (205.929688 --> 205.865097).\n",
            "\t Train_Loss: 79.9355 Val_Loss: 205.8651  BEST VAL Loss: 205.8651\n",
            "\n",
            "Epoch 1968: Validation loss decreased (205.865097 --> 205.801025).\n",
            "\t Train_Loss: 79.9035 Val_Loss: 205.8010  BEST VAL Loss: 205.8010\n",
            "\n",
            "Epoch 1969: Validation loss decreased (205.801025 --> 205.736237).\n",
            "\t Train_Loss: 79.8716 Val_Loss: 205.7362  BEST VAL Loss: 205.7362\n",
            "\n",
            "Epoch 1970: Validation loss decreased (205.736237 --> 205.671875).\n",
            "\t Train_Loss: 79.8396 Val_Loss: 205.6719  BEST VAL Loss: 205.6719\n",
            "\n",
            "Epoch 1971: Validation loss decreased (205.671875 --> 205.607590).\n",
            "\t Train_Loss: 79.8077 Val_Loss: 205.6076  BEST VAL Loss: 205.6076\n",
            "\n",
            "Epoch 1972: Validation loss decreased (205.607590 --> 205.543167).\n",
            "\t Train_Loss: 79.7758 Val_Loss: 205.5432  BEST VAL Loss: 205.5432\n",
            "\n",
            "Epoch 1973: Validation loss decreased (205.543167 --> 205.478516).\n",
            "\t Train_Loss: 79.7439 Val_Loss: 205.4785  BEST VAL Loss: 205.4785\n",
            "\n",
            "Epoch 1974: Validation loss decreased (205.478516 --> 205.414474).\n",
            "\t Train_Loss: 79.7121 Val_Loss: 205.4145  BEST VAL Loss: 205.4145\n",
            "\n",
            "Epoch 1975: Validation loss decreased (205.414474 --> 205.350250).\n",
            "\t Train_Loss: 79.6802 Val_Loss: 205.3503  BEST VAL Loss: 205.3503\n",
            "\n",
            "Epoch 1976: Validation loss decreased (205.350250 --> 205.285751).\n",
            "\t Train_Loss: 79.6484 Val_Loss: 205.2858  BEST VAL Loss: 205.2858\n",
            "\n",
            "Epoch 1977: Validation loss decreased (205.285751 --> 205.221832).\n",
            "\t Train_Loss: 79.6166 Val_Loss: 205.2218  BEST VAL Loss: 205.2218\n",
            "\n",
            "Epoch 1978: Validation loss decreased (205.221832 --> 205.157516).\n",
            "\t Train_Loss: 79.5849 Val_Loss: 205.1575  BEST VAL Loss: 205.1575\n",
            "\n",
            "Epoch 1979: Validation loss decreased (205.157516 --> 205.093246).\n",
            "\t Train_Loss: 79.5531 Val_Loss: 205.0932  BEST VAL Loss: 205.0932\n",
            "\n",
            "Epoch 1980: Validation loss decreased (205.093246 --> 205.029373).\n",
            "\t Train_Loss: 79.5214 Val_Loss: 205.0294  BEST VAL Loss: 205.0294\n",
            "\n",
            "Epoch 1981: Validation loss decreased (205.029373 --> 204.965012).\n",
            "\t Train_Loss: 79.4897 Val_Loss: 204.9650  BEST VAL Loss: 204.9650\n",
            "\n",
            "Epoch 1982: Validation loss decreased (204.965012 --> 204.900803).\n",
            "\t Train_Loss: 79.4580 Val_Loss: 204.9008  BEST VAL Loss: 204.9008\n",
            "\n",
            "Epoch 1983: Validation loss decreased (204.900803 --> 204.837082).\n",
            "\t Train_Loss: 79.4264 Val_Loss: 204.8371  BEST VAL Loss: 204.8371\n",
            "\n",
            "Epoch 1984: Validation loss decreased (204.837082 --> 204.772949).\n",
            "\t Train_Loss: 79.3947 Val_Loss: 204.7729  BEST VAL Loss: 204.7729\n",
            "\n",
            "Epoch 1985: Validation loss decreased (204.772949 --> 204.708939).\n",
            "\t Train_Loss: 79.3631 Val_Loss: 204.7089  BEST VAL Loss: 204.7089\n",
            "\n",
            "Epoch 1986: Validation loss decreased (204.708939 --> 204.644974).\n",
            "\t Train_Loss: 79.3315 Val_Loss: 204.6450  BEST VAL Loss: 204.6450\n",
            "\n",
            "Epoch 1987: Validation loss decreased (204.644974 --> 204.581070).\n",
            "\t Train_Loss: 79.2999 Val_Loss: 204.5811  BEST VAL Loss: 204.5811\n",
            "\n",
            "Epoch 1988: Validation loss decreased (204.581070 --> 204.517197).\n",
            "\t Train_Loss: 79.2684 Val_Loss: 204.5172  BEST VAL Loss: 204.5172\n",
            "\n",
            "Epoch 1989: Validation loss decreased (204.517197 --> 204.453293).\n",
            "\t Train_Loss: 79.2369 Val_Loss: 204.4533  BEST VAL Loss: 204.4533\n",
            "\n",
            "Epoch 1990: Validation loss decreased (204.453293 --> 204.389618).\n",
            "\t Train_Loss: 79.2054 Val_Loss: 204.3896  BEST VAL Loss: 204.3896\n",
            "\n",
            "Epoch 1991: Validation loss decreased (204.389618 --> 204.325546).\n",
            "\t Train_Loss: 79.1739 Val_Loss: 204.3255  BEST VAL Loss: 204.3255\n",
            "\n",
            "Epoch 1992: Validation loss decreased (204.325546 --> 204.261871).\n",
            "\t Train_Loss: 79.1425 Val_Loss: 204.2619  BEST VAL Loss: 204.2619\n",
            "\n",
            "Epoch 1993: Validation loss decreased (204.261871 --> 204.198318).\n",
            "\t Train_Loss: 79.1110 Val_Loss: 204.1983  BEST VAL Loss: 204.1983\n",
            "\n",
            "Epoch 1994: Validation loss decreased (204.198318 --> 204.134567).\n",
            "\t Train_Loss: 79.0796 Val_Loss: 204.1346  BEST VAL Loss: 204.1346\n",
            "\n",
            "Epoch 1995: Validation loss decreased (204.134567 --> 204.070862).\n",
            "\t Train_Loss: 79.0482 Val_Loss: 204.0709  BEST VAL Loss: 204.0709\n",
            "\n",
            "Epoch 1996: Validation loss decreased (204.070862 --> 204.007126).\n",
            "\t Train_Loss: 79.0168 Val_Loss: 204.0071  BEST VAL Loss: 204.0071\n",
            "\n",
            "Epoch 1997: Validation loss decreased (204.007126 --> 203.943634).\n",
            "\t Train_Loss: 78.9855 Val_Loss: 203.9436  BEST VAL Loss: 203.9436\n",
            "\n",
            "Epoch 1998: Validation loss decreased (203.943634 --> 203.879974).\n",
            "\t Train_Loss: 78.9542 Val_Loss: 203.8800  BEST VAL Loss: 203.8800\n",
            "\n",
            "Epoch 1999: Validation loss decreased (203.879974 --> 203.816269).\n",
            "\t Train_Loss: 78.9229 Val_Loss: 203.8163  BEST VAL Loss: 203.8163\n",
            "\n"
          ]
        }
      ],
      "source": [
        "SimpleRNN_best_model, train_losses, val_losses = trainer(SimpleRNN_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8x-goeR4Ef0o",
        "outputId": "8822ce58-d009-4dbd-f732-6d5929b0e9d3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-qi4WfFZQF-"
      },
      "source": [
        "---\n",
        "### 4.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCb4QQCPRrzm"
      },
      "outputs": [],
      "source": [
        "val_predict_RNN = SimpleRNN_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "kjQclrpURuOf",
        "outputId": "1dcc9b09-3f1d-4843-82c0-772953d7f319"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_RNN.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by Simple RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvDPP4kXEI-"
      },
      "source": [
        "---\n",
        "---\n",
        "## 5 GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIg9lbrXiKZ"
      },
      "source": [
        "---\n",
        "### 5.1 Define single GRU cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDjVZTlBXIFj"
      },
      "outputs": [],
      "source": [
        "class GRUCell(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_length=10, hidden_size=20, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.reset_gate_layer = nn.Linear(\n",
        "            input_length + hidden_size, hidden_size, bias=bias\n",
        "        )\n",
        "        self.update_gate_layer = nn.Linear(\n",
        "            input_length + hidden_size, hidden_size, bias=bias\n",
        "        )\n",
        "        self.output_gate_layer = nn.Linear(\n",
        "            input_length + hidden_size, hidden_size, bias=bias\n",
        "        )\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "    def reset_gate(self, x, h):\n",
        "        combined = torch.cat((x, h), 1)\n",
        "        r = torch.sigmoid(self.reset_gate_layer(combined))\n",
        "        return r\n",
        "\n",
        "    def update_gate(self, x, h):\n",
        "        combined = torch.cat((x, h), 1)\n",
        "        z = torch.sigmoid(self.update_gate_layer(combined))\n",
        "        return z\n",
        "\n",
        "    def output_gate(self, x, h, r):\n",
        "        combined = torch.cat((x, r * h), 1)\n",
        "        n_t = torch.tanh(self.output_gate_layer(combined))\n",
        "        return n_t\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        r = self.reset_gate(x, h)\n",
        "        z = self.update_gate(x, h)\n",
        "        n_t = self.output_gate(x, h, r)\n",
        "        h_new = (1 - z) * n_t + z * h\n",
        "        return h_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SlbDcQ-XuYx"
      },
      "source": [
        "---\n",
        "### 5.2 GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6frzQdtXyWW"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(\n",
        "            input_size, hidden_size, num_layers, bias=bias, batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIm3f8yX9XF"
      },
      "source": [
        "---\n",
        "### 5.3 Train GRU model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TTE5topYD_j",
        "outputId": "7f5e7900-60df-40ee-98ff-f95f4e8f55c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GRU(\n",
              "  (rnn_cell_list): ModuleList(\n",
              "    (0): GRUCell(\n",
              "      (reset_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (reset_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "      (update_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (update_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "      (output_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (output_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GRU_model = GRU(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "GRU_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdCfe-xRYKtx"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(GRU_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1xTqKgoYNNM",
        "outputId": "20a3e48b-3793-417d-fd60-5c6752aaa3bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\t Train_Loss: 193.3537 Val_Loss: 572.0673  BEST VAL Loss: 572.0673\n",
            "\n",
            "Epoch 334: Validation loss decreased (572.067322 --> 568.758972).\n",
            "\t Train_Loss: 192.2152 Val_Loss: 568.7590  BEST VAL Loss: 568.7590\n",
            "\n",
            "Epoch 335: Validation loss decreased (568.758972 --> 565.490662).\n",
            "\t Train_Loss: 191.0983 Val_Loss: 565.4907  BEST VAL Loss: 565.4907\n",
            "\n",
            "Epoch 336: Validation loss decreased (565.490662 --> 562.262146).\n",
            "\t Train_Loss: 190.0025 Val_Loss: 562.2621  BEST VAL Loss: 562.2621\n",
            "\n",
            "Epoch 337: Validation loss decreased (562.262146 --> 559.072571).\n",
            "\t Train_Loss: 188.9273 Val_Loss: 559.0726  BEST VAL Loss: 559.0726\n",
            "\n",
            "Epoch 338: Validation loss decreased (559.072571 --> 555.920837).\n",
            "\t Train_Loss: 187.8727 Val_Loss: 555.9208  BEST VAL Loss: 555.9208\n",
            "\n",
            "Epoch 339: Validation loss decreased (555.920837 --> 552.807495).\n",
            "\t Train_Loss: 186.8381 Val_Loss: 552.8075  BEST VAL Loss: 552.8075\n",
            "\n",
            "Epoch 340: Validation loss decreased (552.807495 --> 549.731628).\n",
            "\t Train_Loss: 185.8232 Val_Loss: 549.7316  BEST VAL Loss: 549.7316\n",
            "\n",
            "Epoch 341: Validation loss decreased (549.731628 --> 546.693054).\n",
            "\t Train_Loss: 184.8278 Val_Loss: 546.6931  BEST VAL Loss: 546.6931\n",
            "\n",
            "Epoch 342: Validation loss decreased (546.693054 --> 543.691284).\n",
            "\t Train_Loss: 183.8515 Val_Loss: 543.6913  BEST VAL Loss: 543.6913\n",
            "\n",
            "Epoch 343: Validation loss decreased (543.691284 --> 540.726013).\n",
            "\t Train_Loss: 182.8938 Val_Loss: 540.7260  BEST VAL Loss: 540.7260\n",
            "\n",
            "Epoch 344: Validation loss decreased (540.726013 --> 537.795837).\n",
            "\t Train_Loss: 181.9546 Val_Loss: 537.7958  BEST VAL Loss: 537.7958\n",
            "\n",
            "Epoch 345: Validation loss decreased (537.795837 --> 534.901550).\n",
            "\t Train_Loss: 181.0335 Val_Loss: 534.9016  BEST VAL Loss: 534.9016\n",
            "\n",
            "Epoch 346: Validation loss decreased (534.901550 --> 532.042542).\n",
            "\t Train_Loss: 180.1303 Val_Loss: 532.0425  BEST VAL Loss: 532.0425\n",
            "\n",
            "Epoch 347: Validation loss decreased (532.042542 --> 529.218384).\n",
            "\t Train_Loss: 179.2444 Val_Loss: 529.2184  BEST VAL Loss: 529.2184\n",
            "\n",
            "Epoch 348: Validation loss decreased (529.218384 --> 526.427856).\n",
            "\t Train_Loss: 178.3760 Val_Loss: 526.4279  BEST VAL Loss: 526.4279\n",
            "\n",
            "Epoch 349: Validation loss decreased (526.427856 --> 523.670837).\n",
            "\t Train_Loss: 177.5243 Val_Loss: 523.6708  BEST VAL Loss: 523.6708\n",
            "\n",
            "Epoch 350: Validation loss decreased (523.670837 --> 520.948425).\n",
            "\t Train_Loss: 176.6892 Val_Loss: 520.9484  BEST VAL Loss: 520.9484\n",
            "\n",
            "Epoch 351: Validation loss decreased (520.948425 --> 518.258118).\n",
            "\t Train_Loss: 175.8705 Val_Loss: 518.2581  BEST VAL Loss: 518.2581\n",
            "\n",
            "Epoch 352: Validation loss decreased (518.258118 --> 515.600098).\n",
            "\t Train_Loss: 175.0677 Val_Loss: 515.6001  BEST VAL Loss: 515.6001\n",
            "\n",
            "Epoch 353: Validation loss decreased (515.600098 --> 512.975159).\n",
            "\t Train_Loss: 174.2809 Val_Loss: 512.9752  BEST VAL Loss: 512.9752\n",
            "\n",
            "Epoch 354: Validation loss decreased (512.975159 --> 510.381592).\n",
            "\t Train_Loss: 173.5094 Val_Loss: 510.3816  BEST VAL Loss: 510.3816\n",
            "\n",
            "Epoch 355: Validation loss decreased (510.381592 --> 507.819733).\n",
            "\t Train_Loss: 172.7530 Val_Loss: 507.8197  BEST VAL Loss: 507.8197\n",
            "\n",
            "Epoch 356: Validation loss decreased (507.819733 --> 505.288879).\n",
            "\t Train_Loss: 172.0118 Val_Loss: 505.2889  BEST VAL Loss: 505.2889\n",
            "\n",
            "Epoch 357: Validation loss decreased (505.288879 --> 502.788879).\n",
            "\t Train_Loss: 171.2851 Val_Loss: 502.7889  BEST VAL Loss: 502.7889\n",
            "\n",
            "Epoch 358: Validation loss decreased (502.788879 --> 500.319244).\n",
            "\t Train_Loss: 170.5728 Val_Loss: 500.3192  BEST VAL Loss: 500.3192\n",
            "\n",
            "Epoch 359: Validation loss decreased (500.319244 --> 497.880035).\n",
            "\t Train_Loss: 169.8748 Val_Loss: 497.8800  BEST VAL Loss: 497.8800\n",
            "\n",
            "Epoch 360: Validation loss decreased (497.880035 --> 495.469482).\n",
            "\t Train_Loss: 169.1907 Val_Loss: 495.4695  BEST VAL Loss: 495.4695\n",
            "\n",
            "Epoch 361: Validation loss decreased (495.469482 --> 493.089844).\n",
            "\t Train_Loss: 168.5202 Val_Loss: 493.0898  BEST VAL Loss: 493.0898\n",
            "\n",
            "Epoch 362: Validation loss decreased (493.089844 --> 490.737946).\n",
            "\t Train_Loss: 167.8631 Val_Loss: 490.7379  BEST VAL Loss: 490.7379\n",
            "\n",
            "Epoch 363: Validation loss decreased (490.737946 --> 488.414856).\n",
            "\t Train_Loss: 167.2193 Val_Loss: 488.4149  BEST VAL Loss: 488.4149\n",
            "\n",
            "Epoch 364: Validation loss decreased (488.414856 --> 486.120850).\n",
            "\t Train_Loss: 166.5882 Val_Loss: 486.1208  BEST VAL Loss: 486.1208\n",
            "\n",
            "Epoch 365: Validation loss decreased (486.120850 --> 483.853516).\n",
            "\t Train_Loss: 165.9700 Val_Loss: 483.8535  BEST VAL Loss: 483.8535\n",
            "\n",
            "Epoch 366: Validation loss decreased (483.853516 --> 481.615234).\n",
            "\t Train_Loss: 165.3643 Val_Loss: 481.6152  BEST VAL Loss: 481.6152\n",
            "\n",
            "Epoch 367: Validation loss decreased (481.615234 --> 479.403534).\n",
            "\t Train_Loss: 164.7707 Val_Loss: 479.4035  BEST VAL Loss: 479.4035\n",
            "\n",
            "Epoch 368: Validation loss decreased (479.403534 --> 477.218964).\n",
            "\t Train_Loss: 164.1892 Val_Loss: 477.2190  BEST VAL Loss: 477.2190\n",
            "\n",
            "Epoch 369: Validation loss decreased (477.218964 --> 475.060760).\n",
            "\t Train_Loss: 163.6197 Val_Loss: 475.0608  BEST VAL Loss: 475.0608\n",
            "\n",
            "Epoch 370: Validation loss decreased (475.060760 --> 472.928711).\n",
            "\t Train_Loss: 163.0615 Val_Loss: 472.9287  BEST VAL Loss: 472.9287\n",
            "\n",
            "Epoch 371: Validation loss decreased (472.928711 --> 470.823303).\n",
            "\t Train_Loss: 162.5147 Val_Loss: 470.8233  BEST VAL Loss: 470.8233\n",
            "\n",
            "Epoch 372: Validation loss decreased (470.823303 --> 468.743469).\n",
            "\t Train_Loss: 161.9793 Val_Loss: 468.7435  BEST VAL Loss: 468.7435\n",
            "\n",
            "Epoch 373: Validation loss decreased (468.743469 --> 466.688934).\n",
            "\t Train_Loss: 161.4547 Val_Loss: 466.6889  BEST VAL Loss: 466.6889\n",
            "\n",
            "Epoch 374: Validation loss decreased (466.688934 --> 464.659668).\n",
            "\t Train_Loss: 160.9409 Val_Loss: 464.6597  BEST VAL Loss: 464.6597\n",
            "\n",
            "Epoch 375: Validation loss decreased (464.659668 --> 462.655182).\n",
            "\t Train_Loss: 160.4377 Val_Loss: 462.6552  BEST VAL Loss: 462.6552\n",
            "\n",
            "Epoch 376: Validation loss decreased (462.655182 --> 460.674957).\n",
            "\t Train_Loss: 159.9447 Val_Loss: 460.6750  BEST VAL Loss: 460.6750\n",
            "\n",
            "Epoch 377: Validation loss decreased (460.674957 --> 458.718842).\n",
            "\t Train_Loss: 159.4621 Val_Loss: 458.7188  BEST VAL Loss: 458.7188\n",
            "\n",
            "Epoch 378: Validation loss decreased (458.718842 --> 456.787079).\n",
            "\t Train_Loss: 158.9894 Val_Loss: 456.7871  BEST VAL Loss: 456.7871\n",
            "\n",
            "Epoch 379: Validation loss decreased (456.787079 --> 454.879242).\n",
            "\t Train_Loss: 158.5264 Val_Loss: 454.8792  BEST VAL Loss: 454.8792\n",
            "\n",
            "Epoch 380: Validation loss decreased (454.879242 --> 452.994232).\n",
            "\t Train_Loss: 158.0732 Val_Loss: 452.9942  BEST VAL Loss: 452.9942\n",
            "\n",
            "Epoch 381: Validation loss decreased (452.994232 --> 451.132629).\n",
            "\t Train_Loss: 157.6293 Val_Loss: 451.1326  BEST VAL Loss: 451.1326\n",
            "\n",
            "Epoch 382: Validation loss decreased (451.132629 --> 449.294250).\n",
            "\t Train_Loss: 157.1947 Val_Loss: 449.2943  BEST VAL Loss: 449.2943\n",
            "\n",
            "Epoch 383: Validation loss decreased (449.294250 --> 447.477692).\n",
            "\t Train_Loss: 156.7692 Val_Loss: 447.4777  BEST VAL Loss: 447.4777\n",
            "\n",
            "Epoch 384: Validation loss decreased (447.477692 --> 445.684296).\n",
            "\t Train_Loss: 156.3527 Val_Loss: 445.6843  BEST VAL Loss: 445.6843\n",
            "\n",
            "Epoch 385: Validation loss decreased (445.684296 --> 443.912018).\n",
            "\t Train_Loss: 155.9448 Val_Loss: 443.9120  BEST VAL Loss: 443.9120\n",
            "\n",
            "Epoch 386: Validation loss decreased (443.912018 --> 442.161713).\n",
            "\t Train_Loss: 155.5455 Val_Loss: 442.1617  BEST VAL Loss: 442.1617\n",
            "\n",
            "Epoch 387: Validation loss decreased (442.161713 --> 440.433075).\n",
            "\t Train_Loss: 155.1547 Val_Loss: 440.4331  BEST VAL Loss: 440.4331\n",
            "\n",
            "Epoch 388: Validation loss decreased (440.433075 --> 438.725250).\n",
            "\t Train_Loss: 154.7720 Val_Loss: 438.7253  BEST VAL Loss: 438.7253\n",
            "\n",
            "Epoch 389: Validation loss decreased (438.725250 --> 437.038910).\n",
            "\t Train_Loss: 154.3975 Val_Loss: 437.0389  BEST VAL Loss: 437.0389\n",
            "\n",
            "Epoch 390: Validation loss decreased (437.038910 --> 435.373199).\n",
            "\t Train_Loss: 154.0309 Val_Loss: 435.3732  BEST VAL Loss: 435.3732\n",
            "\n",
            "Epoch 391: Validation loss decreased (435.373199 --> 433.727753).\n",
            "\t Train_Loss: 153.6722 Val_Loss: 433.7278  BEST VAL Loss: 433.7278\n",
            "\n",
            "Epoch 392: Validation loss decreased (433.727753 --> 432.103088).\n",
            "\t Train_Loss: 153.3210 Val_Loss: 432.1031  BEST VAL Loss: 432.1031\n",
            "\n",
            "Epoch 393: Validation loss decreased (432.103088 --> 430.498383).\n",
            "\t Train_Loss: 152.9774 Val_Loss: 430.4984  BEST VAL Loss: 430.4984\n",
            "\n",
            "Epoch 394: Validation loss decreased (430.498383 --> 428.913300).\n",
            "\t Train_Loss: 152.6410 Val_Loss: 428.9133  BEST VAL Loss: 428.9133\n",
            "\n",
            "Epoch 395: Validation loss decreased (428.913300 --> 427.347870).\n",
            "\t Train_Loss: 152.3120 Val_Loss: 427.3479  BEST VAL Loss: 427.3479\n",
            "\n",
            "Epoch 396: Validation loss decreased (427.347870 --> 425.801178).\n",
            "\t Train_Loss: 151.9899 Val_Loss: 425.8012  BEST VAL Loss: 425.8012\n",
            "\n",
            "Epoch 397: Validation loss decreased (425.801178 --> 424.273987).\n",
            "\t Train_Loss: 151.6748 Val_Loss: 424.2740  BEST VAL Loss: 424.2740\n",
            "\n",
            "Epoch 398: Validation loss decreased (424.273987 --> 422.765289).\n",
            "\t Train_Loss: 151.3664 Val_Loss: 422.7653  BEST VAL Loss: 422.7653\n",
            "\n",
            "Epoch 399: Validation loss decreased (422.765289 --> 421.275726).\n",
            "\t Train_Loss: 151.0647 Val_Loss: 421.2757  BEST VAL Loss: 421.2757\n",
            "\n",
            "Epoch 400: Validation loss decreased (421.275726 --> 419.803772).\n",
            "\t Train_Loss: 150.7696 Val_Loss: 419.8038  BEST VAL Loss: 419.8038\n",
            "\n",
            "Epoch 401: Validation loss decreased (419.803772 --> 418.350830).\n",
            "\t Train_Loss: 150.4808 Val_Loss: 418.3508  BEST VAL Loss: 418.3508\n",
            "\n",
            "Epoch 402: Validation loss decreased (418.350830 --> 416.915619).\n",
            "\t Train_Loss: 150.1983 Val_Loss: 416.9156  BEST VAL Loss: 416.9156\n",
            "\n",
            "Epoch 403: Validation loss decreased (416.915619 --> 415.497406).\n",
            "\t Train_Loss: 149.9220 Val_Loss: 415.4974  BEST VAL Loss: 415.4974\n",
            "\n",
            "Epoch 404: Validation loss decreased (415.497406 --> 414.097961).\n",
            "\t Train_Loss: 149.6516 Val_Loss: 414.0980  BEST VAL Loss: 414.0980\n",
            "\n",
            "Epoch 405: Validation loss decreased (414.097961 --> 412.715424).\n",
            "\t Train_Loss: 149.3871 Val_Loss: 412.7154  BEST VAL Loss: 412.7154\n",
            "\n",
            "Epoch 406: Validation loss decreased (412.715424 --> 411.349609).\n",
            "\t Train_Loss: 149.1284 Val_Loss: 411.3496  BEST VAL Loss: 411.3496\n",
            "\n",
            "Epoch 407: Validation loss decreased (411.349609 --> 410.000732).\n",
            "\t Train_Loss: 148.8756 Val_Loss: 410.0007  BEST VAL Loss: 410.0007\n",
            "\n",
            "Epoch 408: Validation loss decreased (410.000732 --> 408.668457).\n",
            "\t Train_Loss: 148.6281 Val_Loss: 408.6685  BEST VAL Loss: 408.6685\n",
            "\n",
            "Epoch 409: Validation loss decreased (408.668457 --> 407.353302).\n",
            "\t Train_Loss: 148.3862 Val_Loss: 407.3533  BEST VAL Loss: 407.3533\n",
            "\n",
            "Epoch 410: Validation loss decreased (407.353302 --> 406.054047).\n",
            "\t Train_Loss: 148.1496 Val_Loss: 406.0540  BEST VAL Loss: 406.0540\n",
            "\n",
            "Epoch 411: Validation loss decreased (406.054047 --> 404.770844).\n",
            "\t Train_Loss: 147.9182 Val_Loss: 404.7708  BEST VAL Loss: 404.7708\n",
            "\n",
            "Epoch 412: Validation loss decreased (404.770844 --> 403.504150).\n",
            "\t Train_Loss: 147.6918 Val_Loss: 403.5042  BEST VAL Loss: 403.5042\n",
            "\n",
            "Epoch 413: Validation loss decreased (403.504150 --> 402.252686).\n",
            "\t Train_Loss: 147.4706 Val_Loss: 402.2527  BEST VAL Loss: 402.2527\n",
            "\n",
            "Epoch 414: Validation loss decreased (402.252686 --> 401.016266).\n",
            "\t Train_Loss: 147.2542 Val_Loss: 401.0163  BEST VAL Loss: 401.0163\n",
            "\n",
            "Epoch 415: Validation loss decreased (401.016266 --> 399.796112).\n",
            "\t Train_Loss: 147.0426 Val_Loss: 399.7961  BEST VAL Loss: 399.7961\n",
            "\n",
            "Epoch 416: Validation loss decreased (399.796112 --> 398.590668).\n",
            "\t Train_Loss: 146.8357 Val_Loss: 398.5907  BEST VAL Loss: 398.5907\n",
            "\n",
            "Epoch 417: Validation loss decreased (398.590668 --> 397.400116).\n",
            "\t Train_Loss: 146.6335 Val_Loss: 397.4001  BEST VAL Loss: 397.4001\n",
            "\n",
            "Epoch 418: Validation loss decreased (397.400116 --> 396.224182).\n",
            "\t Train_Loss: 146.4359 Val_Loss: 396.2242  BEST VAL Loss: 396.2242\n",
            "\n",
            "Epoch 419: Validation loss decreased (396.224182 --> 395.063202).\n",
            "\t Train_Loss: 146.2427 Val_Loss: 395.0632  BEST VAL Loss: 395.0632\n",
            "\n",
            "Epoch 420: Validation loss decreased (395.063202 --> 393.917328).\n",
            "\t Train_Loss: 146.0538 Val_Loss: 393.9173  BEST VAL Loss: 393.9173\n",
            "\n",
            "Epoch 421: Validation loss decreased (393.917328 --> 392.784973).\n",
            "\t Train_Loss: 145.8691 Val_Loss: 392.7850  BEST VAL Loss: 392.7850\n",
            "\n",
            "Epoch 422: Validation loss decreased (392.784973 --> 391.667053).\n",
            "\t Train_Loss: 145.6886 Val_Loss: 391.6671  BEST VAL Loss: 391.6671\n",
            "\n",
            "Epoch 423: Validation loss decreased (391.667053 --> 390.562927).\n",
            "\t Train_Loss: 145.5122 Val_Loss: 390.5629  BEST VAL Loss: 390.5629\n",
            "\n",
            "Epoch 424: Validation loss decreased (390.562927 --> 389.472229).\n",
            "\t Train_Loss: 145.3398 Val_Loss: 389.4722  BEST VAL Loss: 389.4722\n",
            "\n",
            "Epoch 425: Validation loss decreased (389.472229 --> 388.395966).\n",
            "\t Train_Loss: 145.1713 Val_Loss: 388.3960  BEST VAL Loss: 388.3960\n",
            "\n",
            "Epoch 426: Validation loss decreased (388.395966 --> 387.332001).\n",
            "\t Train_Loss: 145.0066 Val_Loss: 387.3320  BEST VAL Loss: 387.3320\n",
            "\n",
            "Epoch 427: Validation loss decreased (387.332001 --> 386.282410).\n",
            "\t Train_Loss: 144.8457 Val_Loss: 386.2824  BEST VAL Loss: 386.2824\n",
            "\n",
            "Epoch 428: Validation loss decreased (386.282410 --> 385.244720).\n",
            "\t Train_Loss: 144.6885 Val_Loss: 385.2447  BEST VAL Loss: 385.2447\n",
            "\n",
            "Epoch 429: Validation loss decreased (385.244720 --> 384.221497).\n",
            "\t Train_Loss: 144.5348 Val_Loss: 384.2215  BEST VAL Loss: 384.2215\n",
            "\n",
            "Epoch 430: Validation loss decreased (384.221497 --> 383.210266).\n",
            "\t Train_Loss: 144.3846 Val_Loss: 383.2103  BEST VAL Loss: 383.2103\n",
            "\n",
            "Epoch 431: Validation loss decreased (383.210266 --> 382.211700).\n",
            "\t Train_Loss: 144.2379 Val_Loss: 382.2117  BEST VAL Loss: 382.2117\n",
            "\n",
            "Epoch 432: Validation loss decreased (382.211700 --> 381.225922).\n",
            "\t Train_Loss: 144.0946 Val_Loss: 381.2259  BEST VAL Loss: 381.2259\n",
            "\n",
            "Epoch 433: Validation loss decreased (381.225922 --> 380.251923).\n",
            "\t Train_Loss: 143.9546 Val_Loss: 380.2519  BEST VAL Loss: 380.2519\n",
            "\n",
            "Epoch 434: Validation loss decreased (380.251923 --> 379.290253).\n",
            "\t Train_Loss: 143.8177 Val_Loss: 379.2903  BEST VAL Loss: 379.2903\n",
            "\n",
            "Epoch 435: Validation loss decreased (379.290253 --> 378.341156).\n",
            "\t Train_Loss: 143.6841 Val_Loss: 378.3412  BEST VAL Loss: 378.3412\n",
            "\n",
            "Epoch 436: Validation loss decreased (378.341156 --> 377.403778).\n",
            "\t Train_Loss: 143.5535 Val_Loss: 377.4038  BEST VAL Loss: 377.4038\n",
            "\n",
            "Epoch 437: Validation loss decreased (377.403778 --> 376.478516).\n",
            "\t Train_Loss: 143.4260 Val_Loss: 376.4785  BEST VAL Loss: 376.4785\n",
            "\n",
            "Epoch 438: Validation loss decreased (376.478516 --> 375.564270).\n",
            "\t Train_Loss: 143.3015 Val_Loss: 375.5643  BEST VAL Loss: 375.5643\n",
            "\n",
            "Epoch 439: Validation loss decreased (375.564270 --> 374.661469).\n",
            "\t Train_Loss: 143.1798 Val_Loss: 374.6615  BEST VAL Loss: 374.6615\n",
            "\n",
            "Epoch 440: Validation loss decreased (374.661469 --> 373.770294).\n",
            "\t Train_Loss: 143.0609 Val_Loss: 373.7703  BEST VAL Loss: 373.7703\n",
            "\n",
            "Epoch 441: Validation loss decreased (373.770294 --> 372.890320).\n",
            "\t Train_Loss: 142.9450 Val_Loss: 372.8903  BEST VAL Loss: 372.8903\n",
            "\n",
            "Epoch 442: Validation loss decreased (372.890320 --> 372.022064).\n",
            "\t Train_Loss: 142.8316 Val_Loss: 372.0221  BEST VAL Loss: 372.0221\n",
            "\n",
            "Epoch 443: Validation loss decreased (372.022064 --> 371.163910).\n",
            "\t Train_Loss: 142.7209 Val_Loss: 371.1639  BEST VAL Loss: 371.1639\n",
            "\n",
            "Epoch 444: Validation loss decreased (371.163910 --> 370.316406).\n",
            "\t Train_Loss: 142.6128 Val_Loss: 370.3164  BEST VAL Loss: 370.3164\n",
            "\n",
            "Epoch 445: Validation loss decreased (370.316406 --> 369.479950).\n",
            "\t Train_Loss: 142.5073 Val_Loss: 369.4799  BEST VAL Loss: 369.4799\n",
            "\n",
            "Epoch 446: Validation loss decreased (369.479950 --> 368.654480).\n",
            "\t Train_Loss: 142.4042 Val_Loss: 368.6545  BEST VAL Loss: 368.6545\n",
            "\n",
            "Epoch 447: Validation loss decreased (368.654480 --> 367.839172).\n",
            "\t Train_Loss: 142.3036 Val_Loss: 367.8392  BEST VAL Loss: 367.8392\n",
            "\n",
            "Epoch 448: Validation loss decreased (367.839172 --> 367.033844).\n",
            "\t Train_Loss: 142.2054 Val_Loss: 367.0338  BEST VAL Loss: 367.0338\n",
            "\n",
            "Epoch 449: Validation loss decreased (367.033844 --> 366.238617).\n",
            "\t Train_Loss: 142.1094 Val_Loss: 366.2386  BEST VAL Loss: 366.2386\n",
            "\n",
            "Epoch 450: Validation loss decreased (366.238617 --> 365.453827).\n",
            "\t Train_Loss: 142.0158 Val_Loss: 365.4538  BEST VAL Loss: 365.4538\n",
            "\n",
            "Epoch 451: Validation loss decreased (365.453827 --> 364.678680).\n",
            "\t Train_Loss: 141.9244 Val_Loss: 364.6787  BEST VAL Loss: 364.6787\n",
            "\n",
            "Epoch 452: Validation loss decreased (364.678680 --> 363.913544).\n",
            "\t Train_Loss: 141.8352 Val_Loss: 363.9135  BEST VAL Loss: 363.9135\n",
            "\n",
            "Epoch 453: Validation loss decreased (363.913544 --> 363.158447).\n",
            "\t Train_Loss: 141.7481 Val_Loss: 363.1584  BEST VAL Loss: 363.1584\n",
            "\n",
            "Epoch 454: Validation loss decreased (363.158447 --> 362.412445).\n",
            "\t Train_Loss: 141.6631 Val_Loss: 362.4124  BEST VAL Loss: 362.4124\n",
            "\n",
            "Epoch 455: Validation loss decreased (362.412445 --> 361.676514).\n",
            "\t Train_Loss: 141.5801 Val_Loss: 361.6765  BEST VAL Loss: 361.6765\n",
            "\n",
            "Epoch 456: Validation loss decreased (361.676514 --> 360.949432).\n",
            "\t Train_Loss: 141.4990 Val_Loss: 360.9494  BEST VAL Loss: 360.9494\n",
            "\n",
            "Epoch 457: Validation loss decreased (360.949432 --> 360.231445).\n",
            "\t Train_Loss: 141.4201 Val_Loss: 360.2314  BEST VAL Loss: 360.2314\n",
            "\n",
            "Epoch 458: Validation loss decreased (360.231445 --> 359.522522).\n",
            "\t Train_Loss: 141.3429 Val_Loss: 359.5225  BEST VAL Loss: 359.5225\n",
            "\n",
            "Epoch 459: Validation loss decreased (359.522522 --> 358.823090).\n",
            "\t Train_Loss: 141.2676 Val_Loss: 358.8231  BEST VAL Loss: 358.8231\n",
            "\n",
            "Epoch 460: Validation loss decreased (358.823090 --> 358.132477).\n",
            "\t Train_Loss: 141.1942 Val_Loss: 358.1325  BEST VAL Loss: 358.1325\n",
            "\n",
            "Epoch 461: Validation loss decreased (358.132477 --> 357.450714).\n",
            "\t Train_Loss: 141.1224 Val_Loss: 357.4507  BEST VAL Loss: 357.4507\n",
            "\n",
            "Epoch 462: Validation loss decreased (357.450714 --> 356.777283).\n",
            "\t Train_Loss: 141.0525 Val_Loss: 356.7773  BEST VAL Loss: 356.7773\n",
            "\n",
            "Epoch 463: Validation loss decreased (356.777283 --> 356.113373).\n",
            "\t Train_Loss: 140.9842 Val_Loss: 356.1134  BEST VAL Loss: 356.1134\n",
            "\n",
            "Epoch 464: Validation loss decreased (356.113373 --> 355.456635).\n",
            "\t Train_Loss: 140.9176 Val_Loss: 355.4566  BEST VAL Loss: 355.4566\n",
            "\n",
            "Epoch 465: Validation loss decreased (355.456635 --> 354.809021).\n",
            "\t Train_Loss: 140.8527 Val_Loss: 354.8090  BEST VAL Loss: 354.8090\n",
            "\n",
            "Epoch 466: Validation loss decreased (354.809021 --> 354.169220).\n",
            "\t Train_Loss: 140.7892 Val_Loss: 354.1692  BEST VAL Loss: 354.1692\n",
            "\n",
            "Epoch 467: Validation loss decreased (354.169220 --> 353.538239).\n",
            "\t Train_Loss: 140.7275 Val_Loss: 353.5382  BEST VAL Loss: 353.5382\n",
            "\n",
            "Epoch 468: Validation loss decreased (353.538239 --> 352.915009).\n",
            "\t Train_Loss: 140.6671 Val_Loss: 352.9150  BEST VAL Loss: 352.9150\n",
            "\n",
            "Epoch 469: Validation loss decreased (352.915009 --> 352.299530).\n",
            "\t Train_Loss: 140.6082 Val_Loss: 352.2995  BEST VAL Loss: 352.2995\n",
            "\n",
            "Epoch 470: Validation loss decreased (352.299530 --> 351.692627).\n",
            "\t Train_Loss: 140.5509 Val_Loss: 351.6926  BEST VAL Loss: 351.6926\n",
            "\n",
            "Epoch 471: Validation loss decreased (351.692627 --> 351.093231).\n",
            "\t Train_Loss: 140.4949 Val_Loss: 351.0932  BEST VAL Loss: 351.0932\n",
            "\n",
            "Epoch 472: Validation loss decreased (351.093231 --> 350.501617).\n",
            "\t Train_Loss: 140.4403 Val_Loss: 350.5016  BEST VAL Loss: 350.5016\n",
            "\n",
            "Epoch 473: Validation loss decreased (350.501617 --> 349.917236).\n",
            "\t Train_Loss: 140.3871 Val_Loss: 349.9172  BEST VAL Loss: 349.9172\n",
            "\n",
            "Epoch 474: Validation loss decreased (349.917236 --> 349.341156).\n",
            "\t Train_Loss: 140.3351 Val_Loss: 349.3412  BEST VAL Loss: 349.3412\n",
            "\n",
            "Epoch 475: Validation loss decreased (349.341156 --> 348.771240).\n",
            "\t Train_Loss: 140.2845 Val_Loss: 348.7712  BEST VAL Loss: 348.7712\n",
            "\n",
            "Epoch 476: Validation loss decreased (348.771240 --> 348.209534).\n",
            "\t Train_Loss: 140.2351 Val_Loss: 348.2095  BEST VAL Loss: 348.2095\n",
            "\n",
            "Epoch 477: Validation loss decreased (348.209534 --> 347.654999).\n",
            "\t Train_Loss: 140.1869 Val_Loss: 347.6550  BEST VAL Loss: 347.6550\n",
            "\n",
            "Epoch 478: Validation loss decreased (347.654999 --> 347.107727).\n",
            "\t Train_Loss: 140.1400 Val_Loss: 347.1077  BEST VAL Loss: 347.1077\n",
            "\n",
            "Epoch 479: Validation loss decreased (347.107727 --> 346.567291).\n",
            "\t Train_Loss: 140.0942 Val_Loss: 346.5673  BEST VAL Loss: 346.5673\n",
            "\n",
            "Epoch 480: Validation loss decreased (346.567291 --> 346.033630).\n",
            "\t Train_Loss: 140.0496 Val_Loss: 346.0336  BEST VAL Loss: 346.0336\n",
            "\n",
            "Epoch 481: Validation loss decreased (346.033630 --> 345.507416).\n",
            "\t Train_Loss: 140.0059 Val_Loss: 345.5074  BEST VAL Loss: 345.5074\n",
            "\n",
            "Epoch 482: Validation loss decreased (345.507416 --> 344.987518).\n",
            "\t Train_Loss: 139.9636 Val_Loss: 344.9875  BEST VAL Loss: 344.9875\n",
            "\n",
            "Epoch 483: Validation loss decreased (344.987518 --> 344.474365).\n",
            "\t Train_Loss: 139.9222 Val_Loss: 344.4744  BEST VAL Loss: 344.4744\n",
            "\n",
            "Epoch 484: Validation loss decreased (344.474365 --> 343.968506).\n",
            "\t Train_Loss: 139.8819 Val_Loss: 343.9685  BEST VAL Loss: 343.9685\n",
            "\n",
            "Epoch 485: Validation loss decreased (343.968506 --> 343.468506).\n",
            "\t Train_Loss: 139.8425 Val_Loss: 343.4685  BEST VAL Loss: 343.4685\n",
            "\n",
            "Epoch 486: Validation loss decreased (343.468506 --> 342.975494).\n",
            "\t Train_Loss: 139.8042 Val_Loss: 342.9755  BEST VAL Loss: 342.9755\n",
            "\n",
            "Epoch 487: Validation loss decreased (342.975494 --> 342.488495).\n",
            "\t Train_Loss: 139.7668 Val_Loss: 342.4885  BEST VAL Loss: 342.4885\n",
            "\n",
            "Epoch 488: Validation loss decreased (342.488495 --> 342.008362).\n",
            "\t Train_Loss: 139.7304 Val_Loss: 342.0084  BEST VAL Loss: 342.0084\n",
            "\n",
            "Epoch 489: Validation loss decreased (342.008362 --> 341.533905).\n",
            "\t Train_Loss: 139.6949 Val_Loss: 341.5339  BEST VAL Loss: 341.5339\n",
            "\n",
            "Epoch 490: Validation loss decreased (341.533905 --> 341.065979).\n",
            "\t Train_Loss: 139.6604 Val_Loss: 341.0660  BEST VAL Loss: 341.0660\n",
            "\n",
            "Epoch 491: Validation loss decreased (341.065979 --> 340.603729).\n",
            "\t Train_Loss: 139.6267 Val_Loss: 340.6037  BEST VAL Loss: 340.6037\n",
            "\n",
            "Epoch 492: Validation loss decreased (340.603729 --> 340.148285).\n",
            "\t Train_Loss: 139.5938 Val_Loss: 340.1483  BEST VAL Loss: 340.1483\n",
            "\n",
            "Epoch 493: Validation loss decreased (340.148285 --> 339.698151).\n",
            "\t Train_Loss: 139.5618 Val_Loss: 339.6982  BEST VAL Loss: 339.6982\n",
            "\n",
            "Epoch 494: Validation loss decreased (339.698151 --> 339.254333).\n",
            "\t Train_Loss: 139.5306 Val_Loss: 339.2543  BEST VAL Loss: 339.2543\n",
            "\n",
            "Epoch 495: Validation loss decreased (339.254333 --> 338.815857).\n",
            "\t Train_Loss: 139.5003 Val_Loss: 338.8159  BEST VAL Loss: 338.8159\n",
            "\n",
            "Epoch 496: Validation loss decreased (338.815857 --> 338.383209).\n",
            "\t Train_Loss: 139.4706 Val_Loss: 338.3832  BEST VAL Loss: 338.3832\n",
            "\n",
            "Epoch 497: Validation loss decreased (338.383209 --> 337.956696).\n",
            "\t Train_Loss: 139.4418 Val_Loss: 337.9567  BEST VAL Loss: 337.9567\n",
            "\n",
            "Epoch 498: Validation loss decreased (337.956696 --> 337.535614).\n",
            "\t Train_Loss: 139.4137 Val_Loss: 337.5356  BEST VAL Loss: 337.5356\n",
            "\n",
            "Epoch 499: Validation loss decreased (337.535614 --> 337.119904).\n",
            "\t Train_Loss: 139.3864 Val_Loss: 337.1199  BEST VAL Loss: 337.1199\n",
            "\n",
            "Epoch 500: Validation loss decreased (337.119904 --> 336.709686).\n",
            "\t Train_Loss: 139.3597 Val_Loss: 336.7097  BEST VAL Loss: 336.7097\n",
            "\n",
            "Epoch 501: Validation loss decreased (336.709686 --> 336.305023).\n",
            "\t Train_Loss: 139.3337 Val_Loss: 336.3050  BEST VAL Loss: 336.3050\n",
            "\n",
            "Epoch 502: Validation loss decreased (336.305023 --> 335.905853).\n",
            "\t Train_Loss: 139.3084 Val_Loss: 335.9059  BEST VAL Loss: 335.9059\n",
            "\n",
            "Epoch 503: Validation loss decreased (335.905853 --> 335.511322).\n",
            "\t Train_Loss: 139.2838 Val_Loss: 335.5113  BEST VAL Loss: 335.5113\n",
            "\n",
            "Epoch 504: Validation loss decreased (335.511322 --> 335.122650).\n",
            "\t Train_Loss: 139.2598 Val_Loss: 335.1227  BEST VAL Loss: 335.1227\n",
            "\n",
            "Epoch 505: Validation loss decreased (335.122650 --> 334.738770).\n",
            "\t Train_Loss: 139.2363 Val_Loss: 334.7388  BEST VAL Loss: 334.7388\n",
            "\n",
            "Epoch 506: Validation loss decreased (334.738770 --> 334.359924).\n",
            "\t Train_Loss: 139.2136 Val_Loss: 334.3599  BEST VAL Loss: 334.3599\n",
            "\n",
            "Epoch 507: Validation loss decreased (334.359924 --> 333.986084).\n",
            "\t Train_Loss: 139.1914 Val_Loss: 333.9861  BEST VAL Loss: 333.9861\n",
            "\n",
            "Epoch 508: Validation loss decreased (333.986084 --> 333.617279).\n",
            "\t Train_Loss: 139.1698 Val_Loss: 333.6173  BEST VAL Loss: 333.6173\n",
            "\n",
            "Epoch 509: Validation loss decreased (333.617279 --> 333.253479).\n",
            "\t Train_Loss: 139.1488 Val_Loss: 333.2535  BEST VAL Loss: 333.2535\n",
            "\n",
            "Epoch 510: Validation loss decreased (333.253479 --> 332.894379).\n",
            "\t Train_Loss: 139.1283 Val_Loss: 332.8944  BEST VAL Loss: 332.8944\n",
            "\n",
            "Epoch 511: Validation loss decreased (332.894379 --> 332.540131).\n",
            "\t Train_Loss: 139.1084 Val_Loss: 332.5401  BEST VAL Loss: 332.5401\n",
            "\n",
            "Epoch 512: Validation loss decreased (332.540131 --> 332.190765).\n",
            "\t Train_Loss: 139.0889 Val_Loss: 332.1908  BEST VAL Loss: 332.1908\n",
            "\n",
            "Epoch 513: Validation loss decreased (332.190765 --> 331.845825).\n",
            "\t Train_Loss: 139.0701 Val_Loss: 331.8458  BEST VAL Loss: 331.8458\n",
            "\n",
            "Epoch 514: Validation loss decreased (331.845825 --> 331.505554).\n",
            "\t Train_Loss: 139.0517 Val_Loss: 331.5056  BEST VAL Loss: 331.5056\n",
            "\n",
            "Epoch 515: Validation loss decreased (331.505554 --> 331.169891).\n",
            "\t Train_Loss: 139.0338 Val_Loss: 331.1699  BEST VAL Loss: 331.1699\n",
            "\n",
            "Epoch 516: Validation loss decreased (331.169891 --> 330.838348).\n",
            "\t Train_Loss: 139.0163 Val_Loss: 330.8383  BEST VAL Loss: 330.8383\n",
            "\n",
            "Epoch 517: Validation loss decreased (330.838348 --> 330.511932).\n",
            "\t Train_Loss: 138.9993 Val_Loss: 330.5119  BEST VAL Loss: 330.5119\n",
            "\n",
            "Epoch 518: Validation loss decreased (330.511932 --> 330.189270).\n",
            "\t Train_Loss: 138.9828 Val_Loss: 330.1893  BEST VAL Loss: 330.1893\n",
            "\n",
            "Epoch 519: Validation loss decreased (330.189270 --> 329.871063).\n",
            "\t Train_Loss: 138.9667 Val_Loss: 329.8711  BEST VAL Loss: 329.8711\n",
            "\n",
            "Epoch 520: Validation loss decreased (329.871063 --> 329.557281).\n",
            "\t Train_Loss: 138.9510 Val_Loss: 329.5573  BEST VAL Loss: 329.5573\n",
            "\n",
            "Epoch 521: Validation loss decreased (329.557281 --> 329.247559).\n",
            "\t Train_Loss: 138.9359 Val_Loss: 329.2476  BEST VAL Loss: 329.2476\n",
            "\n",
            "Epoch 522: Validation loss decreased (329.247559 --> 328.942444).\n",
            "\t Train_Loss: 138.9210 Val_Loss: 328.9424  BEST VAL Loss: 328.9424\n",
            "\n",
            "Epoch 523: Validation loss decreased (328.942444 --> 328.640808).\n",
            "\t Train_Loss: 138.9066 Val_Loss: 328.6408  BEST VAL Loss: 328.6408\n",
            "\n",
            "Epoch 524: Validation loss decreased (328.640808 --> 328.343323).\n",
            "\t Train_Loss: 138.8925 Val_Loss: 328.3433  BEST VAL Loss: 328.3433\n",
            "\n",
            "Epoch 525: Validation loss decreased (328.343323 --> 328.050079).\n",
            "\t Train_Loss: 138.8789 Val_Loss: 328.0501  BEST VAL Loss: 328.0501\n",
            "\n",
            "Epoch 526: Validation loss decreased (328.050079 --> 327.760834).\n",
            "\t Train_Loss: 138.8656 Val_Loss: 327.7608  BEST VAL Loss: 327.7608\n",
            "\n",
            "Epoch 527: Validation loss decreased (327.760834 --> 327.475433).\n",
            "\t Train_Loss: 138.8526 Val_Loss: 327.4754  BEST VAL Loss: 327.4754\n",
            "\n",
            "Epoch 528: Validation loss decreased (327.475433 --> 327.193970).\n",
            "\t Train_Loss: 138.8400 Val_Loss: 327.1940  BEST VAL Loss: 327.1940\n",
            "\n",
            "Epoch 529: Validation loss decreased (327.193970 --> 326.915802).\n",
            "\t Train_Loss: 138.8278 Val_Loss: 326.9158  BEST VAL Loss: 326.9158\n",
            "\n",
            "Epoch 530: Validation loss decreased (326.915802 --> 326.642548).\n",
            "\t Train_Loss: 138.8159 Val_Loss: 326.6425  BEST VAL Loss: 326.6425\n",
            "\n",
            "Epoch 531: Validation loss decreased (326.642548 --> 326.371979).\n",
            "\t Train_Loss: 138.8043 Val_Loss: 326.3720  BEST VAL Loss: 326.3720\n",
            "\n",
            "Epoch 532: Validation loss decreased (326.371979 --> 326.105194).\n",
            "\t Train_Loss: 138.7929 Val_Loss: 326.1052  BEST VAL Loss: 326.1052\n",
            "\n",
            "Epoch 533: Validation loss decreased (326.105194 --> 325.842499).\n",
            "\t Train_Loss: 138.7820 Val_Loss: 325.8425  BEST VAL Loss: 325.8425\n",
            "\n",
            "Epoch 534: Validation loss decreased (325.842499 --> 325.582886).\n",
            "\t Train_Loss: 138.7713 Val_Loss: 325.5829  BEST VAL Loss: 325.5829\n",
            "\n",
            "Epoch 535: Validation loss decreased (325.582886 --> 325.327118).\n",
            "\t Train_Loss: 138.7609 Val_Loss: 325.3271  BEST VAL Loss: 325.3271\n",
            "\n",
            "Epoch 536: Validation loss decreased (325.327118 --> 325.074860).\n",
            "\t Train_Loss: 138.7509 Val_Loss: 325.0749  BEST VAL Loss: 325.0749\n",
            "\n",
            "Epoch 537: Validation loss decreased (325.074860 --> 324.826141).\n",
            "\t Train_Loss: 138.7410 Val_Loss: 324.8261  BEST VAL Loss: 324.8261\n",
            "\n",
            "Epoch 538: Validation loss decreased (324.826141 --> 324.580536).\n",
            "\t Train_Loss: 138.7314 Val_Loss: 324.5805  BEST VAL Loss: 324.5805\n",
            "\n",
            "Epoch 539: Validation loss decreased (324.580536 --> 324.338593).\n",
            "\t Train_Loss: 138.7221 Val_Loss: 324.3386  BEST VAL Loss: 324.3386\n",
            "\n",
            "Epoch 540: Validation loss decreased (324.338593 --> 324.099701).\n",
            "\t Train_Loss: 138.7132 Val_Loss: 324.0997  BEST VAL Loss: 324.0997\n",
            "\n",
            "Epoch 541: Validation loss decreased (324.099701 --> 323.864105).\n",
            "\t Train_Loss: 138.7044 Val_Loss: 323.8641  BEST VAL Loss: 323.8641\n",
            "\n",
            "Epoch 542: Validation loss decreased (323.864105 --> 323.631866).\n",
            "\t Train_Loss: 138.6958 Val_Loss: 323.6319  BEST VAL Loss: 323.6319\n",
            "\n",
            "Epoch 543: Validation loss decreased (323.631866 --> 323.402679).\n",
            "\t Train_Loss: 138.6875 Val_Loss: 323.4027  BEST VAL Loss: 323.4027\n",
            "\n",
            "Epoch 544: Validation loss decreased (323.402679 --> 323.176849).\n",
            "\t Train_Loss: 138.6794 Val_Loss: 323.1768  BEST VAL Loss: 323.1768\n",
            "\n",
            "Epoch 545: Validation loss decreased (323.176849 --> 322.954102).\n",
            "\t Train_Loss: 138.6716 Val_Loss: 322.9541  BEST VAL Loss: 322.9541\n",
            "\n",
            "Epoch 546: Validation loss decreased (322.954102 --> 322.734375).\n",
            "\t Train_Loss: 138.6639 Val_Loss: 322.7344  BEST VAL Loss: 322.7344\n",
            "\n",
            "Epoch 547: Validation loss decreased (322.734375 --> 322.517181).\n",
            "\t Train_Loss: 138.6565 Val_Loss: 322.5172  BEST VAL Loss: 322.5172\n",
            "\n",
            "Epoch 548: Validation loss decreased (322.517181 --> 322.303925).\n",
            "\t Train_Loss: 138.6493 Val_Loss: 322.3039  BEST VAL Loss: 322.3039\n",
            "\n",
            "Epoch 549: Validation loss decreased (322.303925 --> 322.093292).\n",
            "\t Train_Loss: 138.6423 Val_Loss: 322.0933  BEST VAL Loss: 322.0933\n",
            "\n",
            "Epoch 550: Validation loss decreased (322.093292 --> 321.885742).\n",
            "\t Train_Loss: 138.6354 Val_Loss: 321.8857  BEST VAL Loss: 321.8857\n",
            "\n",
            "Epoch 551: Validation loss decreased (321.885742 --> 321.680756).\n",
            "\t Train_Loss: 138.6289 Val_Loss: 321.6808  BEST VAL Loss: 321.6808\n",
            "\n",
            "Epoch 552: Validation loss decreased (321.680756 --> 321.478149).\n",
            "\t Train_Loss: 138.6225 Val_Loss: 321.4781  BEST VAL Loss: 321.4781\n",
            "\n",
            "Epoch 553: Validation loss decreased (321.478149 --> 321.279205).\n",
            "\t Train_Loss: 138.6162 Val_Loss: 321.2792  BEST VAL Loss: 321.2792\n",
            "\n",
            "Epoch 554: Validation loss decreased (321.279205 --> 321.082367).\n",
            "\t Train_Loss: 138.6100 Val_Loss: 321.0824  BEST VAL Loss: 321.0824\n",
            "\n",
            "Epoch 555: Validation loss decreased (321.082367 --> 320.888824).\n",
            "\t Train_Loss: 138.6042 Val_Loss: 320.8888  BEST VAL Loss: 320.8888\n",
            "\n",
            "Epoch 556: Validation loss decreased (320.888824 --> 320.697815).\n",
            "\t Train_Loss: 138.5984 Val_Loss: 320.6978  BEST VAL Loss: 320.6978\n",
            "\n",
            "Epoch 557: Validation loss decreased (320.697815 --> 320.509125).\n",
            "\t Train_Loss: 138.5928 Val_Loss: 320.5091  BEST VAL Loss: 320.5091\n",
            "\n",
            "Epoch 558: Validation loss decreased (320.509125 --> 320.323486).\n",
            "\t Train_Loss: 138.5874 Val_Loss: 320.3235  BEST VAL Loss: 320.3235\n",
            "\n",
            "Epoch 559: Validation loss decreased (320.323486 --> 320.140533).\n",
            "\t Train_Loss: 138.5821 Val_Loss: 320.1405  BEST VAL Loss: 320.1405\n",
            "\n",
            "Epoch 560: Validation loss decreased (320.140533 --> 319.959564).\n",
            "\t Train_Loss: 138.5770 Val_Loss: 319.9596  BEST VAL Loss: 319.9596\n",
            "\n",
            "Epoch 561: Validation loss decreased (319.959564 --> 319.781708).\n",
            "\t Train_Loss: 138.5720 Val_Loss: 319.7817  BEST VAL Loss: 319.7817\n",
            "\n",
            "Epoch 562: Validation loss decreased (319.781708 --> 319.606201).\n",
            "\t Train_Loss: 138.5672 Val_Loss: 319.6062  BEST VAL Loss: 319.6062\n",
            "\n",
            "Epoch 563: Validation loss decreased (319.606201 --> 319.433044).\n",
            "\t Train_Loss: 138.5625 Val_Loss: 319.4330  BEST VAL Loss: 319.4330\n",
            "\n",
            "Epoch 564: Validation loss decreased (319.433044 --> 319.262299).\n",
            "\t Train_Loss: 138.5579 Val_Loss: 319.2623  BEST VAL Loss: 319.2623\n",
            "\n",
            "Epoch 565: Validation loss decreased (319.262299 --> 319.094208).\n",
            "\t Train_Loss: 138.5535 Val_Loss: 319.0942  BEST VAL Loss: 319.0942\n",
            "\n",
            "Epoch 566: Validation loss decreased (319.094208 --> 318.928680).\n",
            "\t Train_Loss: 138.5492 Val_Loss: 318.9287  BEST VAL Loss: 318.9287\n",
            "\n",
            "Epoch 567: Validation loss decreased (318.928680 --> 318.765198).\n",
            "\t Train_Loss: 138.5450 Val_Loss: 318.7652  BEST VAL Loss: 318.7652\n",
            "\n",
            "Epoch 568: Validation loss decreased (318.765198 --> 318.603760).\n",
            "\t Train_Loss: 138.5409 Val_Loss: 318.6038  BEST VAL Loss: 318.6038\n",
            "\n",
            "Epoch 569: Validation loss decreased (318.603760 --> 318.445068).\n",
            "\t Train_Loss: 138.5370 Val_Loss: 318.4451  BEST VAL Loss: 318.4451\n",
            "\n",
            "Epoch 570: Validation loss decreased (318.445068 --> 318.288330).\n",
            "\t Train_Loss: 138.5332 Val_Loss: 318.2883  BEST VAL Loss: 318.2883\n",
            "\n",
            "Epoch 571: Validation loss decreased (318.288330 --> 318.134125).\n",
            "\t Train_Loss: 138.5294 Val_Loss: 318.1341  BEST VAL Loss: 318.1341\n",
            "\n",
            "Epoch 572: Validation loss decreased (318.134125 --> 317.981995).\n",
            "\t Train_Loss: 138.5258 Val_Loss: 317.9820  BEST VAL Loss: 317.9820\n",
            "\n",
            "Epoch 573: Validation loss decreased (317.981995 --> 317.831970).\n",
            "\t Train_Loss: 138.5223 Val_Loss: 317.8320  BEST VAL Loss: 317.8320\n",
            "\n",
            "Epoch 574: Validation loss decreased (317.831970 --> 317.684052).\n",
            "\t Train_Loss: 138.5189 Val_Loss: 317.6841  BEST VAL Loss: 317.6841\n",
            "\n",
            "Epoch 575: Validation loss decreased (317.684052 --> 317.538422).\n",
            "\t Train_Loss: 138.5155 Val_Loss: 317.5384  BEST VAL Loss: 317.5384\n",
            "\n",
            "Epoch 576: Validation loss decreased (317.538422 --> 317.394012).\n",
            "\t Train_Loss: 138.5124 Val_Loss: 317.3940  BEST VAL Loss: 317.3940\n",
            "\n",
            "Epoch 577: Validation loss decreased (317.394012 --> 317.252899).\n",
            "\t Train_Loss: 138.5093 Val_Loss: 317.2529  BEST VAL Loss: 317.2529\n",
            "\n",
            "Epoch 578: Validation loss decreased (317.252899 --> 317.113129).\n",
            "\t Train_Loss: 138.5062 Val_Loss: 317.1131  BEST VAL Loss: 317.1131\n",
            "\n",
            "Epoch 579: Validation loss decreased (317.113129 --> 316.975494).\n",
            "\t Train_Loss: 138.5033 Val_Loss: 316.9755  BEST VAL Loss: 316.9755\n",
            "\n",
            "Epoch 580: Validation loss decreased (316.975494 --> 316.840088).\n",
            "\t Train_Loss: 138.5004 Val_Loss: 316.8401  BEST VAL Loss: 316.8401\n",
            "\n",
            "Epoch 581: Validation loss decreased (316.840088 --> 316.706238).\n",
            "\t Train_Loss: 138.4976 Val_Loss: 316.7062  BEST VAL Loss: 316.7062\n",
            "\n",
            "Epoch 582: Validation loss decreased (316.706238 --> 316.574677).\n",
            "\t Train_Loss: 138.4949 Val_Loss: 316.5747  BEST VAL Loss: 316.5747\n",
            "\n",
            "Epoch 583: Validation loss decreased (316.574677 --> 316.445038).\n",
            "\t Train_Loss: 138.4924 Val_Loss: 316.4450  BEST VAL Loss: 316.4450\n",
            "\n",
            "Epoch 584: Validation loss decreased (316.445038 --> 316.316620).\n",
            "\t Train_Loss: 138.4898 Val_Loss: 316.3166  BEST VAL Loss: 316.3166\n",
            "\n",
            "Epoch 585: Validation loss decreased (316.316620 --> 316.190552).\n",
            "\t Train_Loss: 138.4873 Val_Loss: 316.1906  BEST VAL Loss: 316.1906\n",
            "\n",
            "Epoch 586: Validation loss decreased (316.190552 --> 316.066162).\n",
            "\t Train_Loss: 138.4849 Val_Loss: 316.0662  BEST VAL Loss: 316.0662\n",
            "\n",
            "Epoch 587: Validation loss decreased (316.066162 --> 315.943970).\n",
            "\t Train_Loss: 138.4827 Val_Loss: 315.9440  BEST VAL Loss: 315.9440\n",
            "\n",
            "Epoch 588: Validation loss decreased (315.943970 --> 315.823395).\n",
            "\t Train_Loss: 138.4804 Val_Loss: 315.8234  BEST VAL Loss: 315.8234\n",
            "\n",
            "Epoch 589: Validation loss decreased (315.823395 --> 315.704193).\n",
            "\t Train_Loss: 138.4782 Val_Loss: 315.7042  BEST VAL Loss: 315.7042\n",
            "\n",
            "Epoch 590: Validation loss decreased (315.704193 --> 315.586945).\n",
            "\t Train_Loss: 138.4761 Val_Loss: 315.5869  BEST VAL Loss: 315.5869\n",
            "\n",
            "Epoch 591: Validation loss decreased (315.586945 --> 315.471588).\n",
            "\t Train_Loss: 138.4741 Val_Loss: 315.4716  BEST VAL Loss: 315.4716\n",
            "\n",
            "Epoch 592: Validation loss decreased (315.471588 --> 315.357697).\n",
            "\t Train_Loss: 138.4720 Val_Loss: 315.3577  BEST VAL Loss: 315.3577\n",
            "\n",
            "Epoch 593: Validation loss decreased (315.357697 --> 315.245544).\n",
            "\t Train_Loss: 138.4701 Val_Loss: 315.2455  BEST VAL Loss: 315.2455\n",
            "\n",
            "Epoch 594: Validation loss decreased (315.245544 --> 315.135223).\n",
            "\t Train_Loss: 138.4682 Val_Loss: 315.1352  BEST VAL Loss: 315.1352\n",
            "\n",
            "Epoch 595: Validation loss decreased (315.135223 --> 315.026184).\n",
            "\t Train_Loss: 138.4664 Val_Loss: 315.0262  BEST VAL Loss: 315.0262\n",
            "\n",
            "Epoch 596: Validation loss decreased (315.026184 --> 314.918945).\n",
            "\t Train_Loss: 138.4647 Val_Loss: 314.9189  BEST VAL Loss: 314.9189\n",
            "\n",
            "Epoch 597: Validation loss decreased (314.918945 --> 314.813385).\n",
            "\t Train_Loss: 138.4629 Val_Loss: 314.8134  BEST VAL Loss: 314.8134\n",
            "\n",
            "Epoch 598: Validation loss decreased (314.813385 --> 314.709076).\n",
            "\t Train_Loss: 138.4613 Val_Loss: 314.7091  BEST VAL Loss: 314.7091\n",
            "\n",
            "Epoch 599: Validation loss decreased (314.709076 --> 314.606201).\n",
            "\t Train_Loss: 138.4597 Val_Loss: 314.6062  BEST VAL Loss: 314.6062\n",
            "\n",
            "Epoch 600: Validation loss decreased (314.606201 --> 314.505280).\n",
            "\t Train_Loss: 138.4581 Val_Loss: 314.5053  BEST VAL Loss: 314.5053\n",
            "\n",
            "Epoch 601: Validation loss decreased (314.505280 --> 314.405762).\n",
            "\t Train_Loss: 138.4566 Val_Loss: 314.4058  BEST VAL Loss: 314.4058\n",
            "\n",
            "Epoch 602: Validation loss decreased (314.405762 --> 314.307404).\n",
            "\t Train_Loss: 138.4551 Val_Loss: 314.3074  BEST VAL Loss: 314.3074\n",
            "\n",
            "Epoch 603: Validation loss decreased (314.307404 --> 314.210602).\n",
            "\t Train_Loss: 138.4537 Val_Loss: 314.2106  BEST VAL Loss: 314.2106\n",
            "\n",
            "Epoch 604: Validation loss decreased (314.210602 --> 314.115051).\n",
            "\t Train_Loss: 138.4523 Val_Loss: 314.1151  BEST VAL Loss: 314.1151\n",
            "\n",
            "Epoch 605: Validation loss decreased (314.115051 --> 314.021454).\n",
            "\t Train_Loss: 138.4509 Val_Loss: 314.0215  BEST VAL Loss: 314.0215\n",
            "\n",
            "Epoch 606: Validation loss decreased (314.021454 --> 313.929047).\n",
            "\t Train_Loss: 138.4497 Val_Loss: 313.9290  BEST VAL Loss: 313.9290\n",
            "\n",
            "Epoch 607: Validation loss decreased (313.929047 --> 313.838074).\n",
            "\t Train_Loss: 138.4484 Val_Loss: 313.8381  BEST VAL Loss: 313.8381\n",
            "\n",
            "Epoch 608: Validation loss decreased (313.838074 --> 313.748596).\n",
            "\t Train_Loss: 138.4472 Val_Loss: 313.7486  BEST VAL Loss: 313.7486\n",
            "\n",
            "Epoch 609: Validation loss decreased (313.748596 --> 313.660095).\n",
            "\t Train_Loss: 138.4460 Val_Loss: 313.6601  BEST VAL Loss: 313.6601\n",
            "\n",
            "Epoch 610: Validation loss decreased (313.660095 --> 313.572998).\n",
            "\t Train_Loss: 138.4448 Val_Loss: 313.5730  BEST VAL Loss: 313.5730\n",
            "\n",
            "Epoch 611: Validation loss decreased (313.572998 --> 313.487122).\n",
            "\t Train_Loss: 138.4437 Val_Loss: 313.4871  BEST VAL Loss: 313.4871\n",
            "\n",
            "Epoch 612: Validation loss decreased (313.487122 --> 313.402863).\n",
            "\t Train_Loss: 138.4426 Val_Loss: 313.4029  BEST VAL Loss: 313.4029\n",
            "\n",
            "Epoch 613: Validation loss decreased (313.402863 --> 313.319397).\n",
            "\t Train_Loss: 138.4416 Val_Loss: 313.3194  BEST VAL Loss: 313.3194\n",
            "\n",
            "Epoch 614: Validation loss decreased (313.319397 --> 313.237335).\n",
            "\t Train_Loss: 138.4406 Val_Loss: 313.2373  BEST VAL Loss: 313.2373\n",
            "\n",
            "Epoch 615: Validation loss decreased (313.237335 --> 313.156799).\n",
            "\t Train_Loss: 138.4396 Val_Loss: 313.1568  BEST VAL Loss: 313.1568\n",
            "\n",
            "Epoch 616: Validation loss decreased (313.156799 --> 313.077240).\n",
            "\t Train_Loss: 138.4386 Val_Loss: 313.0772  BEST VAL Loss: 313.0772\n",
            "\n",
            "Epoch 617: Validation loss decreased (313.077240 --> 312.999054).\n",
            "\t Train_Loss: 138.4377 Val_Loss: 312.9991  BEST VAL Loss: 312.9991\n",
            "\n",
            "Epoch 618: Validation loss decreased (312.999054 --> 312.921722).\n",
            "\t Train_Loss: 138.4368 Val_Loss: 312.9217  BEST VAL Loss: 312.9217\n",
            "\n",
            "Epoch 619: Validation loss decreased (312.921722 --> 312.845673).\n",
            "\t Train_Loss: 138.4359 Val_Loss: 312.8457  BEST VAL Loss: 312.8457\n",
            "\n",
            "Epoch 620: Validation loss decreased (312.845673 --> 312.771332).\n",
            "\t Train_Loss: 138.4351 Val_Loss: 312.7713  BEST VAL Loss: 312.7713\n",
            "\n",
            "Epoch 621: Validation loss decreased (312.771332 --> 312.697388).\n",
            "\t Train_Loss: 138.4343 Val_Loss: 312.6974  BEST VAL Loss: 312.6974\n",
            "\n",
            "Epoch 622: Validation loss decreased (312.697388 --> 312.624756).\n",
            "\t Train_Loss: 138.4335 Val_Loss: 312.6248  BEST VAL Loss: 312.6248\n",
            "\n",
            "Epoch 623: Validation loss decreased (312.624756 --> 312.553436).\n",
            "\t Train_Loss: 138.4327 Val_Loss: 312.5534  BEST VAL Loss: 312.5534\n",
            "\n",
            "Epoch 624: Validation loss decreased (312.553436 --> 312.482880).\n",
            "\t Train_Loss: 138.4320 Val_Loss: 312.4829  BEST VAL Loss: 312.4829\n",
            "\n",
            "Epoch 625: Validation loss decreased (312.482880 --> 312.413574).\n",
            "\t Train_Loss: 138.4313 Val_Loss: 312.4136  BEST VAL Loss: 312.4136\n",
            "\n",
            "Epoch 626: Validation loss decreased (312.413574 --> 312.345703).\n",
            "\t Train_Loss: 138.4305 Val_Loss: 312.3457  BEST VAL Loss: 312.3457\n",
            "\n",
            "Epoch 627: Validation loss decreased (312.345703 --> 312.277954).\n",
            "\t Train_Loss: 138.4298 Val_Loss: 312.2780  BEST VAL Loss: 312.2780\n",
            "\n",
            "Epoch 628: Validation loss decreased (312.277954 --> 312.212219).\n",
            "\t Train_Loss: 138.4292 Val_Loss: 312.2122  BEST VAL Loss: 312.2122\n",
            "\n",
            "Epoch 629: Validation loss decreased (312.212219 --> 312.147003).\n",
            "\t Train_Loss: 138.4286 Val_Loss: 312.1470  BEST VAL Loss: 312.1470\n",
            "\n",
            "Epoch 630: Validation loss decreased (312.147003 --> 312.082703).\n",
            "\t Train_Loss: 138.4280 Val_Loss: 312.0827  BEST VAL Loss: 312.0827\n",
            "\n",
            "Epoch 631: Validation loss decreased (312.082703 --> 312.019592).\n",
            "\t Train_Loss: 138.4274 Val_Loss: 312.0196  BEST VAL Loss: 312.0196\n",
            "\n",
            "Epoch 632: Validation loss decreased (312.019592 --> 311.957275).\n",
            "\t Train_Loss: 138.4268 Val_Loss: 311.9573  BEST VAL Loss: 311.9573\n",
            "\n",
            "Epoch 633: Validation loss decreased (311.957275 --> 311.895996).\n",
            "\t Train_Loss: 138.4262 Val_Loss: 311.8960  BEST VAL Loss: 311.8960\n",
            "\n",
            "Epoch 634: Validation loss decreased (311.895996 --> 311.835632).\n",
            "\t Train_Loss: 138.4257 Val_Loss: 311.8356  BEST VAL Loss: 311.8356\n",
            "\n",
            "Epoch 635: Validation loss decreased (311.835632 --> 311.776489).\n",
            "\t Train_Loss: 138.4252 Val_Loss: 311.7765  BEST VAL Loss: 311.7765\n",
            "\n",
            "Epoch 636: Validation loss decreased (311.776489 --> 311.717804).\n",
            "\t Train_Loss: 138.4247 Val_Loss: 311.7178  BEST VAL Loss: 311.7178\n",
            "\n",
            "Epoch 637: Validation loss decreased (311.717804 --> 311.660431).\n",
            "\t Train_Loss: 138.4242 Val_Loss: 311.6604  BEST VAL Loss: 311.6604\n",
            "\n",
            "Epoch 638: Validation loss decreased (311.660431 --> 311.603912).\n",
            "\t Train_Loss: 138.4237 Val_Loss: 311.6039  BEST VAL Loss: 311.6039\n",
            "\n",
            "Epoch 639: Validation loss decreased (311.603912 --> 311.548065).\n",
            "\t Train_Loss: 138.4233 Val_Loss: 311.5481  BEST VAL Loss: 311.5481\n",
            "\n",
            "Epoch 640: Validation loss decreased (311.548065 --> 311.493134).\n",
            "\t Train_Loss: 138.4228 Val_Loss: 311.4931  BEST VAL Loss: 311.4931\n",
            "\n",
            "Epoch 641: Validation loss decreased (311.493134 --> 311.439178).\n",
            "\t Train_Loss: 138.4223 Val_Loss: 311.4392  BEST VAL Loss: 311.4392\n",
            "\n",
            "Epoch 642: Validation loss decreased (311.439178 --> 311.386200).\n",
            "\t Train_Loss: 138.4219 Val_Loss: 311.3862  BEST VAL Loss: 311.3862\n",
            "\n",
            "Epoch 643: Validation loss decreased (311.386200 --> 311.333344).\n",
            "\t Train_Loss: 138.4215 Val_Loss: 311.3333  BEST VAL Loss: 311.3333\n",
            "\n",
            "Epoch 644: Validation loss decreased (311.333344 --> 311.281952).\n",
            "\t Train_Loss: 138.4212 Val_Loss: 311.2820  BEST VAL Loss: 311.2820\n",
            "\n",
            "Epoch 645: Validation loss decreased (311.281952 --> 311.231079).\n",
            "\t Train_Loss: 138.4208 Val_Loss: 311.2311  BEST VAL Loss: 311.2311\n",
            "\n",
            "Epoch 646: Validation loss decreased (311.231079 --> 311.181152).\n",
            "\t Train_Loss: 138.4204 Val_Loss: 311.1812  BEST VAL Loss: 311.1812\n",
            "\n",
            "Epoch 647: Validation loss decreased (311.181152 --> 311.132050).\n",
            "\t Train_Loss: 138.4201 Val_Loss: 311.1320  BEST VAL Loss: 311.1320\n",
            "\n",
            "Epoch 648: Validation loss decreased (311.132050 --> 311.083466).\n",
            "\t Train_Loss: 138.4197 Val_Loss: 311.0835  BEST VAL Loss: 311.0835\n",
            "\n",
            "Epoch 649: Validation loss decreased (311.083466 --> 311.035980).\n",
            "\t Train_Loss: 138.4193 Val_Loss: 311.0360  BEST VAL Loss: 311.0360\n",
            "\n",
            "Epoch 650: Validation loss decreased (311.035980 --> 310.988770).\n",
            "\t Train_Loss: 138.4190 Val_Loss: 310.9888  BEST VAL Loss: 310.9888\n",
            "\n",
            "Epoch 651: Validation loss decreased (310.988770 --> 310.943176).\n",
            "\t Train_Loss: 138.4187 Val_Loss: 310.9432  BEST VAL Loss: 310.9432\n",
            "\n",
            "Epoch 652: Validation loss decreased (310.943176 --> 310.897369).\n",
            "\t Train_Loss: 138.4185 Val_Loss: 310.8974  BEST VAL Loss: 310.8974\n",
            "\n",
            "Epoch 653: Validation loss decreased (310.897369 --> 310.852631).\n",
            "\t Train_Loss: 138.4182 Val_Loss: 310.8526  BEST VAL Loss: 310.8526\n",
            "\n",
            "Epoch 654: Validation loss decreased (310.852631 --> 310.808655).\n",
            "\t Train_Loss: 138.4179 Val_Loss: 310.8087  BEST VAL Loss: 310.8087\n",
            "\n",
            "Epoch 655: Validation loss decreased (310.808655 --> 310.765594).\n",
            "\t Train_Loss: 138.4176 Val_Loss: 310.7656  BEST VAL Loss: 310.7656\n",
            "\n",
            "Epoch 656: Validation loss decreased (310.765594 --> 310.722809).\n",
            "\t Train_Loss: 138.4173 Val_Loss: 310.7228  BEST VAL Loss: 310.7228\n",
            "\n",
            "Epoch 657: Validation loss decreased (310.722809 --> 310.680786).\n",
            "\t Train_Loss: 138.4170 Val_Loss: 310.6808  BEST VAL Loss: 310.6808\n",
            "\n",
            "Epoch 658: Validation loss decreased (310.680786 --> 310.639709).\n",
            "\t Train_Loss: 138.4169 Val_Loss: 310.6397  BEST VAL Loss: 310.6397\n",
            "\n",
            "Epoch 659: Validation loss decreased (310.639709 --> 310.599335).\n",
            "\t Train_Loss: 138.4166 Val_Loss: 310.5993  BEST VAL Loss: 310.5993\n",
            "\n",
            "Epoch 660: Validation loss decreased (310.599335 --> 310.558960).\n",
            "\t Train_Loss: 138.4164 Val_Loss: 310.5590  BEST VAL Loss: 310.5590\n",
            "\n",
            "Epoch 661: Validation loss decreased (310.558960 --> 310.519379).\n",
            "\t Train_Loss: 138.4161 Val_Loss: 310.5194  BEST VAL Loss: 310.5194\n",
            "\n",
            "Epoch 662: Validation loss decreased (310.519379 --> 310.481323).\n",
            "\t Train_Loss: 138.4158 Val_Loss: 310.4813  BEST VAL Loss: 310.4813\n",
            "\n",
            "Epoch 663: Validation loss decreased (310.481323 --> 310.442596).\n",
            "\t Train_Loss: 138.4157 Val_Loss: 310.4426  BEST VAL Loss: 310.4426\n",
            "\n",
            "Epoch 664: Validation loss decreased (310.442596 --> 310.405182).\n",
            "\t Train_Loss: 138.4155 Val_Loss: 310.4052  BEST VAL Loss: 310.4052\n",
            "\n",
            "Epoch 665: Validation loss decreased (310.405182 --> 310.368530).\n",
            "\t Train_Loss: 138.4153 Val_Loss: 310.3685  BEST VAL Loss: 310.3685\n",
            "\n",
            "Epoch 666: Validation loss decreased (310.368530 --> 310.332153).\n",
            "\t Train_Loss: 138.4151 Val_Loss: 310.3322  BEST VAL Loss: 310.3322\n",
            "\n",
            "Epoch 667: Validation loss decreased (310.332153 --> 310.296509).\n",
            "\t Train_Loss: 138.4149 Val_Loss: 310.2965  BEST VAL Loss: 310.2965\n",
            "\n",
            "Epoch 668: Validation loss decreased (310.296509 --> 310.261475).\n",
            "\t Train_Loss: 138.4147 Val_Loss: 310.2615  BEST VAL Loss: 310.2615\n",
            "\n",
            "Epoch 669: Validation loss decreased (310.261475 --> 310.226929).\n",
            "\t Train_Loss: 138.4146 Val_Loss: 310.2269  BEST VAL Loss: 310.2269\n",
            "\n",
            "Epoch 670: Validation loss decreased (310.226929 --> 310.192902).\n",
            "\t Train_Loss: 138.4144 Val_Loss: 310.1929  BEST VAL Loss: 310.1929\n",
            "\n",
            "Epoch 671: Validation loss decreased (310.192902 --> 310.159393).\n",
            "\t Train_Loss: 138.4142 Val_Loss: 310.1594  BEST VAL Loss: 310.1594\n",
            "\n",
            "Epoch 672: Validation loss decreased (310.159393 --> 310.126312).\n",
            "\t Train_Loss: 138.4140 Val_Loss: 310.1263  BEST VAL Loss: 310.1263\n",
            "\n",
            "Epoch 673: Validation loss decreased (310.126312 --> 310.093658).\n",
            "\t Train_Loss: 138.4139 Val_Loss: 310.0937  BEST VAL Loss: 310.0937\n",
            "\n",
            "Epoch 674: Validation loss decreased (310.093658 --> 310.062164).\n",
            "\t Train_Loss: 138.4137 Val_Loss: 310.0622  BEST VAL Loss: 310.0622\n",
            "\n",
            "Epoch 675: Validation loss decreased (310.062164 --> 310.031219).\n",
            "\t Train_Loss: 138.4136 Val_Loss: 310.0312  BEST VAL Loss: 310.0312\n",
            "\n",
            "Epoch 676: Validation loss decreased (310.031219 --> 310.000549).\n",
            "\t Train_Loss: 138.4135 Val_Loss: 310.0005  BEST VAL Loss: 310.0005\n",
            "\n",
            "Epoch 677: Validation loss decreased (310.000549 --> 309.970062).\n",
            "\t Train_Loss: 138.4134 Val_Loss: 309.9701  BEST VAL Loss: 309.9701\n",
            "\n",
            "Epoch 678: Validation loss decreased (309.970062 --> 309.940399).\n",
            "\t Train_Loss: 138.4132 Val_Loss: 309.9404  BEST VAL Loss: 309.9404\n",
            "\n",
            "Epoch 679: Validation loss decreased (309.940399 --> 309.910797).\n",
            "\t Train_Loss: 138.4131 Val_Loss: 309.9108  BEST VAL Loss: 309.9108\n",
            "\n",
            "Epoch 680: Validation loss decreased (309.910797 --> 309.882111).\n",
            "\t Train_Loss: 138.4130 Val_Loss: 309.8821  BEST VAL Loss: 309.8821\n",
            "\n",
            "Epoch 681: Validation loss decreased (309.882111 --> 309.853607).\n",
            "\t Train_Loss: 138.4128 Val_Loss: 309.8536  BEST VAL Loss: 309.8536\n",
            "\n",
            "Epoch 682: Validation loss decreased (309.853607 --> 309.825653).\n",
            "\t Train_Loss: 138.4127 Val_Loss: 309.8257  BEST VAL Loss: 309.8257\n",
            "\n",
            "Epoch 683: Validation loss decreased (309.825653 --> 309.798187).\n",
            "\t Train_Loss: 138.4126 Val_Loss: 309.7982  BEST VAL Loss: 309.7982\n",
            "\n",
            "Epoch 684: Validation loss decreased (309.798187 --> 309.771454).\n",
            "\t Train_Loss: 138.4125 Val_Loss: 309.7715  BEST VAL Loss: 309.7715\n",
            "\n",
            "Epoch 685: Validation loss decreased (309.771454 --> 309.744690).\n",
            "\t Train_Loss: 138.4124 Val_Loss: 309.7447  BEST VAL Loss: 309.7447\n",
            "\n",
            "Epoch 686: Validation loss decreased (309.744690 --> 309.718353).\n",
            "\t Train_Loss: 138.4124 Val_Loss: 309.7184  BEST VAL Loss: 309.7184\n",
            "\n",
            "Epoch 687: Validation loss decreased (309.718353 --> 309.692719).\n",
            "\t Train_Loss: 138.4122 Val_Loss: 309.6927  BEST VAL Loss: 309.6927\n",
            "\n",
            "Epoch 688: Validation loss decreased (309.692719 --> 309.667938).\n",
            "\t Train_Loss: 138.4121 Val_Loss: 309.6679  BEST VAL Loss: 309.6679\n",
            "\n",
            "Epoch 689: Validation loss decreased (309.667938 --> 309.642975).\n",
            "\t Train_Loss: 138.4119 Val_Loss: 309.6430  BEST VAL Loss: 309.6430\n",
            "\n",
            "Epoch 690: Validation loss decreased (309.642975 --> 309.618622).\n",
            "\t Train_Loss: 138.4119 Val_Loss: 309.6186  BEST VAL Loss: 309.6186\n",
            "\n",
            "Epoch 691: Validation loss decreased (309.618622 --> 309.594330).\n",
            "\t Train_Loss: 138.4117 Val_Loss: 309.5943  BEST VAL Loss: 309.5943\n",
            "\n",
            "Epoch 692: Validation loss decreased (309.594330 --> 309.571350).\n",
            "\t Train_Loss: 138.4113 Val_Loss: 309.5714  BEST VAL Loss: 309.5714\n",
            "\n",
            "Epoch 693: Validation loss decreased (309.571350 --> 309.559906).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 309.5599  BEST VAL Loss: 309.5599\n",
            "\n",
            "Epoch 694: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3732 Val_Loss: 310.1260  BEST VAL Loss: 309.5599\n",
            "\n",
            "Epoch 695: Validation loss did not decrease\n",
            "\t Train_Loss: 137.8848 Val_Loss: 335.3004  BEST VAL Loss: 309.5599\n",
            "\n",
            "Epoch 696: Validation loss did not decrease\n",
            "\t Train_Loss: 137.6184 Val_Loss: 310.8494  BEST VAL Loss: 309.5599\n",
            "\n",
            "Epoch 697: Validation loss did not decrease\n",
            "\t Train_Loss: 137.5358 Val_Loss: 309.5751  BEST VAL Loss: 309.5599\n",
            "\n",
            "Epoch 698: Validation loss decreased (309.559906 --> 309.352539).\n",
            "\t Train_Loss: 138.1353 Val_Loss: 309.3525  BEST VAL Loss: 309.3525\n",
            "\n",
            "Epoch 699: Validation loss decreased (309.352539 --> 309.258484).\n",
            "\t Train_Loss: 138.3031 Val_Loss: 309.2585  BEST VAL Loss: 309.2585\n",
            "\n",
            "Epoch 700: Validation loss decreased (309.258484 --> 309.193512).\n",
            "\t Train_Loss: 138.3616 Val_Loss: 309.1935  BEST VAL Loss: 309.1935\n",
            "\n",
            "Epoch 701: Validation loss decreased (309.193512 --> 309.139221).\n",
            "\t Train_Loss: 138.3857 Val_Loss: 309.1392  BEST VAL Loss: 309.1392\n",
            "\n",
            "Epoch 702: Validation loss decreased (309.139221 --> 309.090790).\n",
            "\t Train_Loss: 138.3967 Val_Loss: 309.0908  BEST VAL Loss: 309.0908\n",
            "\n",
            "Epoch 703: Validation loss decreased (309.090790 --> 309.046356).\n",
            "\t Train_Loss: 138.4023 Val_Loss: 309.0464  BEST VAL Loss: 309.0464\n",
            "\n",
            "Epoch 704: Validation loss decreased (309.046356 --> 309.005615).\n",
            "\t Train_Loss: 138.4053 Val_Loss: 309.0056  BEST VAL Loss: 309.0056\n",
            "\n",
            "Epoch 705: Validation loss decreased (309.005615 --> 308.967896).\n",
            "\t Train_Loss: 138.4069 Val_Loss: 308.9679  BEST VAL Loss: 308.9679\n",
            "\n",
            "Epoch 706: Validation loss decreased (308.967896 --> 308.932953).\n",
            "\t Train_Loss: 138.4079 Val_Loss: 308.9330  BEST VAL Loss: 308.9330\n",
            "\n",
            "Epoch 707: Validation loss decreased (308.932953 --> 308.900452).\n",
            "\t Train_Loss: 138.4085 Val_Loss: 308.9005  BEST VAL Loss: 308.9005\n",
            "\n",
            "Epoch 708: Validation loss decreased (308.900452 --> 308.870148).\n",
            "\t Train_Loss: 138.4089 Val_Loss: 308.8701  BEST VAL Loss: 308.8701\n",
            "\n",
            "Epoch 709: Validation loss decreased (308.870148 --> 308.841583).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.8416  BEST VAL Loss: 308.8416\n",
            "\n",
            "Epoch 710: Validation loss decreased (308.841583 --> 308.815857).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.8159  BEST VAL Loss: 308.8159\n",
            "\n",
            "Epoch 711: Validation loss decreased (308.815857 --> 308.791412).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.7914  BEST VAL Loss: 308.7914\n",
            "\n",
            "Epoch 712: Validation loss decreased (308.791412 --> 308.768402).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.7684  BEST VAL Loss: 308.7684\n",
            "\n",
            "Epoch 713: Validation loss decreased (308.768402 --> 308.746918).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.7469  BEST VAL Loss: 308.7469\n",
            "\n",
            "Epoch 714: Validation loss decreased (308.746918 --> 308.727142).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.7271  BEST VAL Loss: 308.7271\n",
            "\n",
            "Epoch 715: Validation loss decreased (308.727142 --> 308.708557).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.7086  BEST VAL Loss: 308.7086\n",
            "\n",
            "Epoch 716: Validation loss decreased (308.708557 --> 308.691010).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.6910  BEST VAL Loss: 308.6910\n",
            "\n",
            "Epoch 717: Validation loss decreased (308.691010 --> 308.674683).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.6747  BEST VAL Loss: 308.6747\n",
            "\n",
            "Epoch 718: Validation loss decreased (308.674683 --> 308.659363).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.6594  BEST VAL Loss: 308.6594\n",
            "\n",
            "Epoch 719: Validation loss decreased (308.659363 --> 308.644745).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.6447  BEST VAL Loss: 308.6447\n",
            "\n",
            "Epoch 720: Validation loss decreased (308.644745 --> 308.631256).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.6313  BEST VAL Loss: 308.6313\n",
            "\n",
            "Epoch 721: Validation loss decreased (308.631256 --> 308.618317).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.6183  BEST VAL Loss: 308.6183\n",
            "\n",
            "Epoch 722: Validation loss decreased (308.618317 --> 308.606232).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.6062  BEST VAL Loss: 308.6062\n",
            "\n",
            "Epoch 723: Validation loss decreased (308.606232 --> 308.595093).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.5951  BEST VAL Loss: 308.5951\n",
            "\n",
            "Epoch 724: Validation loss decreased (308.595093 --> 308.584442).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.5844  BEST VAL Loss: 308.5844\n",
            "\n",
            "Epoch 725: Validation loss decreased (308.584442 --> 308.574066).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.5741  BEST VAL Loss: 308.5741\n",
            "\n",
            "Epoch 726: Validation loss decreased (308.574066 --> 308.564453).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.5645  BEST VAL Loss: 308.5645\n",
            "\n",
            "Epoch 727: Validation loss decreased (308.564453 --> 308.555389).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.5554  BEST VAL Loss: 308.5554\n",
            "\n",
            "Epoch 728: Validation loss decreased (308.555389 --> 308.546570).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.5466  BEST VAL Loss: 308.5466\n",
            "\n",
            "Epoch 729: Validation loss decreased (308.546570 --> 308.538422).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 308.5384  BEST VAL Loss: 308.5384\n",
            "\n",
            "Epoch 730: Validation loss decreased (308.538422 --> 308.530823).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.5308  BEST VAL Loss: 308.5308\n",
            "\n",
            "Epoch 731: Validation loss decreased (308.530823 --> 308.523163).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.5232  BEST VAL Loss: 308.5232\n",
            "\n",
            "Epoch 732: Validation loss decreased (308.523163 --> 308.516022).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.5160  BEST VAL Loss: 308.5160\n",
            "\n",
            "Epoch 733: Validation loss decreased (308.516022 --> 308.509430).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.5094  BEST VAL Loss: 308.5094\n",
            "\n",
            "Epoch 734: Validation loss decreased (308.509430 --> 308.503021).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.5030  BEST VAL Loss: 308.5030\n",
            "\n",
            "Epoch 735: Validation loss decreased (308.503021 --> 308.496735).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.4967  BEST VAL Loss: 308.4967\n",
            "\n",
            "Epoch 736: Validation loss decreased (308.496735 --> 308.490601).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.4906  BEST VAL Loss: 308.4906\n",
            "\n",
            "Epoch 737: Validation loss decreased (308.490601 --> 308.484802).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.4848  BEST VAL Loss: 308.4848\n",
            "\n",
            "Epoch 738: Validation loss decreased (308.484802 --> 308.479279).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.4793  BEST VAL Loss: 308.4793\n",
            "\n",
            "Epoch 739: Validation loss decreased (308.479279 --> 308.474091).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4741  BEST VAL Loss: 308.4741\n",
            "\n",
            "Epoch 740: Validation loss decreased (308.474091 --> 308.469086).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4691  BEST VAL Loss: 308.4691\n",
            "\n",
            "Epoch 741: Validation loss decreased (308.469086 --> 308.464355).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.4644  BEST VAL Loss: 308.4644\n",
            "\n",
            "Epoch 742: Validation loss decreased (308.464355 --> 308.459808).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4598  BEST VAL Loss: 308.4598\n",
            "\n",
            "Epoch 743: Validation loss decreased (308.459808 --> 308.455170).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4552  BEST VAL Loss: 308.4552\n",
            "\n",
            "Epoch 744: Validation loss decreased (308.455170 --> 308.450439).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4504  BEST VAL Loss: 308.4504\n",
            "\n",
            "Epoch 745: Validation loss decreased (308.450439 --> 308.446503).\n",
            "\t Train_Loss: 138.4095 Val_Loss: 308.4465  BEST VAL Loss: 308.4465\n",
            "\n",
            "Epoch 746: Validation loss decreased (308.446503 --> 308.442291).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4423  BEST VAL Loss: 308.4423\n",
            "\n",
            "Epoch 747: Validation loss decreased (308.442291 --> 308.438599).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4386  BEST VAL Loss: 308.4386\n",
            "\n",
            "Epoch 748: Validation loss decreased (308.438599 --> 308.435089).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4351  BEST VAL Loss: 308.4351\n",
            "\n",
            "Epoch 749: Validation loss decreased (308.435089 --> 308.430756).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4308  BEST VAL Loss: 308.4308\n",
            "\n",
            "Epoch 750: Validation loss decreased (308.430756 --> 308.427582).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4276  BEST VAL Loss: 308.4276\n",
            "\n",
            "Epoch 751: Validation loss decreased (308.427582 --> 308.424225).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4242  BEST VAL Loss: 308.4242\n",
            "\n",
            "Epoch 752: Validation loss decreased (308.424225 --> 308.420868).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4209  BEST VAL Loss: 308.4209\n",
            "\n",
            "Epoch 753: Validation loss decreased (308.420868 --> 308.417389).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4174  BEST VAL Loss: 308.4174\n",
            "\n",
            "Epoch 754: Validation loss decreased (308.417389 --> 308.414307).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4143  BEST VAL Loss: 308.4143\n",
            "\n",
            "Epoch 755: Validation loss decreased (308.414307 --> 308.411469).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.4115  BEST VAL Loss: 308.4115\n",
            "\n",
            "Epoch 756: Validation loss decreased (308.411469 --> 308.408264).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.4083  BEST VAL Loss: 308.4083\n",
            "\n",
            "Epoch 757: Validation loss decreased (308.408264 --> 308.404999).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.4050  BEST VAL Loss: 308.4050\n",
            "\n",
            "Epoch 758: Validation loss decreased (308.404999 --> 308.402740).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.4027  BEST VAL Loss: 308.4027\n",
            "\n",
            "Epoch 759: Validation loss decreased (308.402740 --> 308.399689).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3997  BEST VAL Loss: 308.3997\n",
            "\n",
            "Epoch 760: Validation loss decreased (308.399689 --> 308.396912).\n",
            "\t Train_Loss: 138.4094 Val_Loss: 308.3969  BEST VAL Loss: 308.3969\n",
            "\n",
            "Epoch 761: Validation loss decreased (308.396912 --> 308.394043).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3940  BEST VAL Loss: 308.3940\n",
            "\n",
            "Epoch 762: Validation loss decreased (308.394043 --> 308.391815).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3918  BEST VAL Loss: 308.3918\n",
            "\n",
            "Epoch 763: Validation loss decreased (308.391815 --> 308.389221).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3892  BEST VAL Loss: 308.3892\n",
            "\n",
            "Epoch 764: Validation loss decreased (308.389221 --> 308.386810).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3868  BEST VAL Loss: 308.3868\n",
            "\n",
            "Epoch 765: Validation loss decreased (308.386810 --> 308.384460).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3845  BEST VAL Loss: 308.3845\n",
            "\n",
            "Epoch 766: Validation loss decreased (308.384460 --> 308.382080).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3821  BEST VAL Loss: 308.3821\n",
            "\n",
            "Epoch 767: Validation loss decreased (308.382080 --> 308.379852).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3799  BEST VAL Loss: 308.3799\n",
            "\n",
            "Epoch 768: Validation loss decreased (308.379852 --> 308.377594).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3776  BEST VAL Loss: 308.3776\n",
            "\n",
            "Epoch 769: Validation loss decreased (308.377594 --> 308.375458).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3755  BEST VAL Loss: 308.3755\n",
            "\n",
            "Epoch 770: Validation loss decreased (308.375458 --> 308.373108).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 308.3731  BEST VAL Loss: 308.3731\n",
            "\n",
            "Epoch 771: Validation loss decreased (308.373108 --> 308.371307).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3713  BEST VAL Loss: 308.3713\n",
            "\n",
            "Epoch 772: Validation loss decreased (308.371307 --> 308.369141).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3691  BEST VAL Loss: 308.3691\n",
            "\n",
            "Epoch 773: Validation loss decreased (308.369141 --> 308.367157).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3672  BEST VAL Loss: 308.3672\n",
            "\n",
            "Epoch 774: Validation loss decreased (308.367157 --> 308.364868).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3649  BEST VAL Loss: 308.3649\n",
            "\n",
            "Epoch 775: Validation loss decreased (308.364868 --> 308.363495).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3635  BEST VAL Loss: 308.3635\n",
            "\n",
            "Epoch 776: Validation loss decreased (308.363495 --> 308.361420).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3614  BEST VAL Loss: 308.3614\n",
            "\n",
            "Epoch 777: Validation loss decreased (308.361420 --> 308.359833).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3598  BEST VAL Loss: 308.3598\n",
            "\n",
            "Epoch 778: Validation loss decreased (308.359833 --> 308.357880).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3579  BEST VAL Loss: 308.3579\n",
            "\n",
            "Epoch 779: Validation loss decreased (308.357880 --> 308.356293).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3563  BEST VAL Loss: 308.3563\n",
            "\n",
            "Epoch 780: Validation loss decreased (308.356293 --> 308.354095).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3541  BEST VAL Loss: 308.3541\n",
            "\n",
            "Epoch 781: Validation loss decreased (308.354095 --> 308.352783).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3528  BEST VAL Loss: 308.3528\n",
            "\n",
            "Epoch 782: Validation loss decreased (308.352783 --> 308.350739).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 308.3507  BEST VAL Loss: 308.3507\n",
            "\n",
            "Epoch 783: Validation loss decreased (308.350739 --> 308.349335).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3493  BEST VAL Loss: 308.3493\n",
            "\n",
            "Epoch 784: Validation loss decreased (308.349335 --> 308.347809).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3478  BEST VAL Loss: 308.3478\n",
            "\n",
            "Epoch 785: Validation loss decreased (308.347809 --> 308.346252).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3463  BEST VAL Loss: 308.3463\n",
            "\n",
            "Epoch 786: Validation loss decreased (308.346252 --> 308.344879).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3449  BEST VAL Loss: 308.3449\n",
            "\n",
            "Epoch 787: Validation loss decreased (308.344879 --> 308.343353).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3434  BEST VAL Loss: 308.3434\n",
            "\n",
            "Epoch 788: Validation loss decreased (308.343353 --> 308.341888).\n",
            "\t Train_Loss: 138.4091 Val_Loss: 308.3419  BEST VAL Loss: 308.3419\n",
            "\n",
            "Epoch 789: Validation loss decreased (308.341888 --> 308.340057).\n",
            "\t Train_Loss: 138.4090 Val_Loss: 308.3401  BEST VAL Loss: 308.3401\n",
            "\n",
            "Epoch 790: Validation loss decreased (308.340057 --> 308.339020).\n",
            "\t Train_Loss: 138.4090 Val_Loss: 308.3390  BEST VAL Loss: 308.3390\n",
            "\n",
            "Epoch 791: Validation loss decreased (308.339020 --> 308.337708).\n",
            "\t Train_Loss: 138.4090 Val_Loss: 308.3377  BEST VAL Loss: 308.3377\n",
            "\n",
            "Epoch 792: Validation loss decreased (308.337708 --> 308.336426).\n",
            "\t Train_Loss: 138.4090 Val_Loss: 308.3364  BEST VAL Loss: 308.3364\n",
            "\n",
            "Epoch 793: Validation loss decreased (308.336426 --> 308.334961).\n",
            "\t Train_Loss: 138.4090 Val_Loss: 308.3350  BEST VAL Loss: 308.3350\n",
            "\n",
            "Epoch 794: Validation loss decreased (308.334961 --> 308.333557).\n",
            "\t Train_Loss: 138.4090 Val_Loss: 308.3336  BEST VAL Loss: 308.3336\n",
            "\n",
            "Epoch 795: Validation loss decreased (308.333557 --> 308.332428).\n",
            "\t Train_Loss: 138.4090 Val_Loss: 308.3324  BEST VAL Loss: 308.3324\n",
            "\n",
            "Epoch 796: Validation loss decreased (308.332428 --> 308.331146).\n",
            "\t Train_Loss: 138.4089 Val_Loss: 308.3311  BEST VAL Loss: 308.3311\n",
            "\n",
            "Epoch 797: Validation loss decreased (308.331146 --> 308.329834).\n",
            "\t Train_Loss: 138.4088 Val_Loss: 308.3298  BEST VAL Loss: 308.3298\n",
            "\n",
            "Epoch 798: Validation loss decreased (308.329834 --> 308.328949).\n",
            "\t Train_Loss: 138.4089 Val_Loss: 308.3289  BEST VAL Loss: 308.3289\n",
            "\n",
            "Epoch 799: Validation loss decreased (308.328949 --> 308.327545).\n",
            "\t Train_Loss: 138.4088 Val_Loss: 308.3275  BEST VAL Loss: 308.3275\n",
            "\n",
            "Epoch 800: Validation loss decreased (308.327545 --> 308.326416).\n",
            "\t Train_Loss: 138.4089 Val_Loss: 308.3264  BEST VAL Loss: 308.3264\n",
            "\n",
            "Epoch 801: Validation loss decreased (308.326416 --> 308.325378).\n",
            "\t Train_Loss: 138.4088 Val_Loss: 308.3254  BEST VAL Loss: 308.3254\n",
            "\n",
            "Epoch 802: Validation loss decreased (308.325378 --> 308.324219).\n",
            "\t Train_Loss: 138.4088 Val_Loss: 308.3242  BEST VAL Loss: 308.3242\n",
            "\n",
            "Epoch 803: Validation loss decreased (308.324219 --> 308.323059).\n",
            "\t Train_Loss: 138.4088 Val_Loss: 308.3231  BEST VAL Loss: 308.3231\n",
            "\n",
            "Epoch 804: Validation loss decreased (308.323059 --> 308.321930).\n",
            "\t Train_Loss: 138.4087 Val_Loss: 308.3219  BEST VAL Loss: 308.3219\n",
            "\n",
            "Epoch 805: Validation loss decreased (308.321930 --> 308.320984).\n",
            "\t Train_Loss: 138.4086 Val_Loss: 308.3210  BEST VAL Loss: 308.3210\n",
            "\n",
            "Epoch 806: Validation loss decreased (308.320984 --> 308.320038).\n",
            "\t Train_Loss: 138.4086 Val_Loss: 308.3200  BEST VAL Loss: 308.3200\n",
            "\n",
            "Epoch 807: Validation loss decreased (308.320038 --> 308.319061).\n",
            "\t Train_Loss: 138.4085 Val_Loss: 308.3191  BEST VAL Loss: 308.3191\n",
            "\n",
            "Epoch 808: Validation loss decreased (308.319061 --> 308.318024).\n",
            "\t Train_Loss: 138.4084 Val_Loss: 308.3180  BEST VAL Loss: 308.3180\n",
            "\n",
            "Epoch 809: Validation loss decreased (308.318024 --> 308.317200).\n",
            "\t Train_Loss: 138.4083 Val_Loss: 308.3172  BEST VAL Loss: 308.3172\n",
            "\n",
            "Epoch 810: Validation loss decreased (308.317200 --> 308.316071).\n",
            "\t Train_Loss: 138.4082 Val_Loss: 308.3161  BEST VAL Loss: 308.3161\n",
            "\n",
            "Epoch 811: Validation loss decreased (308.316071 --> 308.315186).\n",
            "\t Train_Loss: 138.4081 Val_Loss: 308.3152  BEST VAL Loss: 308.3152\n",
            "\n",
            "Epoch 812: Validation loss decreased (308.315186 --> 308.314209).\n",
            "\t Train_Loss: 138.4079 Val_Loss: 308.3142  BEST VAL Loss: 308.3142\n",
            "\n",
            "Epoch 813: Validation loss decreased (308.314209 --> 308.313507).\n",
            "\t Train_Loss: 138.4077 Val_Loss: 308.3135  BEST VAL Loss: 308.3135\n",
            "\n",
            "Epoch 814: Validation loss decreased (308.313507 --> 308.312653).\n",
            "\t Train_Loss: 138.4073 Val_Loss: 308.3127  BEST VAL Loss: 308.3127\n",
            "\n",
            "Epoch 815: Validation loss decreased (308.312653 --> 308.311523).\n",
            "\t Train_Loss: 138.4069 Val_Loss: 308.3115  BEST VAL Loss: 308.3115\n",
            "\n",
            "Epoch 816: Validation loss decreased (308.311523 --> 308.310944).\n",
            "\t Train_Loss: 138.4062 Val_Loss: 308.3109  BEST VAL Loss: 308.3109\n",
            "\n",
            "Epoch 817: Validation loss decreased (308.310944 --> 308.310425).\n",
            "\t Train_Loss: 138.4053 Val_Loss: 308.3104  BEST VAL Loss: 308.3104\n",
            "\n",
            "Epoch 818: Validation loss decreased (308.310425 --> 308.309723).\n",
            "\t Train_Loss: 138.4038 Val_Loss: 308.3097  BEST VAL Loss: 308.3097\n",
            "\n",
            "Epoch 819: Validation loss decreased (308.309723 --> 308.309113).\n",
            "\t Train_Loss: 138.4014 Val_Loss: 308.3091  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 820: Validation loss decreased (308.309113 --> 308.309082).\n",
            "\t Train_Loss: 138.3971 Val_Loss: 308.3091  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 821: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3890 Val_Loss: 308.3101  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 822: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3711 Val_Loss: 308.3147  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 823: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3259 Val_Loss: 308.3397  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 824: Validation loss did not decrease\n",
            "\t Train_Loss: 138.1975 Val_Loss: 308.5452  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 825: Validation loss did not decrease\n",
            "\t Train_Loss: 137.9999 Val_Loss: 309.2419  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 826: Validation loss did not decrease\n",
            "\t Train_Loss: 138.0729 Val_Loss: 308.8789  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 827: Validation loss did not decrease\n",
            "\t Train_Loss: 137.7680 Val_Loss: 308.4164  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 828: Validation loss did not decrease\n",
            "\t Train_Loss: 137.6682 Val_Loss: 308.4054  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 829: Validation loss did not decrease\n",
            "\t Train_Loss: 137.5470 Val_Loss: 308.7659  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 830: Validation loss did not decrease\n",
            "\t Train_Loss: 137.1314 Val_Loss: 309.6464  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 831: Validation loss did not decrease\n",
            "\t Train_Loss: 137.1885 Val_Loss: 308.7729  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 832: Validation loss did not decrease\n",
            "\t Train_Loss: 136.6755 Val_Loss: 308.4369  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 833: Validation loss did not decrease\n",
            "\t Train_Loss: 136.6512 Val_Loss: 309.4269  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 834: Validation loss did not decrease\n",
            "\t Train_Loss: 135.5764 Val_Loss: 348.9323  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 835: Validation loss did not decrease\n",
            "\t Train_Loss: 137.3734 Val_Loss: 310.0396  BEST VAL Loss: 308.3091\n",
            "\n",
            "Epoch 836: Validation loss decreased (308.309082 --> 308.282440).\n",
            "\t Train_Loss: 134.7862 Val_Loss: 308.2824  BEST VAL Loss: 308.2824\n",
            "\n",
            "Epoch 837: Validation loss did not decrease\n",
            "\t Train_Loss: 135.5282 Val_Loss: 308.3845  BEST VAL Loss: 308.2824\n",
            "\n",
            "Epoch 838: Validation loss did not decrease\n",
            "\t Train_Loss: 135.1385 Val_Loss: 309.0710  BEST VAL Loss: 308.2824\n",
            "\n",
            "Epoch 839: Validation loss did not decrease\n",
            "\t Train_Loss: 135.1517 Val_Loss: 308.3907  BEST VAL Loss: 308.2824\n",
            "\n",
            "Epoch 840: Validation loss decreased (308.282440 --> 307.779205).\n",
            "\t Train_Loss: 134.6506 Val_Loss: 307.7792  BEST VAL Loss: 307.7792\n",
            "\n",
            "Epoch 841: Validation loss decreased (307.779205 --> 307.746643).\n",
            "\t Train_Loss: 134.5402 Val_Loss: 307.7466  BEST VAL Loss: 307.7466\n",
            "\n",
            "Epoch 842: Validation loss did not decrease\n",
            "\t Train_Loss: 134.1593 Val_Loss: 308.1990  BEST VAL Loss: 307.7466\n",
            "\n",
            "Epoch 843: Validation loss did not decrease\n",
            "\t Train_Loss: 133.9073 Val_Loss: 308.0363  BEST VAL Loss: 307.7466\n",
            "\n",
            "Epoch 844: Validation loss decreased (307.746643 --> 307.384552).\n",
            "\t Train_Loss: 133.6228 Val_Loss: 307.3846  BEST VAL Loss: 307.3846\n",
            "\n",
            "Epoch 845: Validation loss decreased (307.384552 --> 307.179047).\n",
            "\t Train_Loss: 133.2982 Val_Loss: 307.1790  BEST VAL Loss: 307.1790\n",
            "\n",
            "Epoch 846: Validation loss did not decrease\n",
            "\t Train_Loss: 133.0794 Val_Loss: 307.3544  BEST VAL Loss: 307.1790\n",
            "\n",
            "Epoch 847: Validation loss did not decrease\n",
            "\t Train_Loss: 132.7019 Val_Loss: 307.4120  BEST VAL Loss: 307.1790\n",
            "\n",
            "Epoch 848: Validation loss decreased (307.179047 --> 306.903198).\n",
            "\t Train_Loss: 132.5221 Val_Loss: 306.9032  BEST VAL Loss: 306.9032\n",
            "\n",
            "Epoch 849: Validation loss decreased (306.903198 --> 306.579559).\n",
            "\t Train_Loss: 132.1361 Val_Loss: 306.5796  BEST VAL Loss: 306.5796\n",
            "\n",
            "Epoch 850: Validation loss decreased (306.579559 --> 306.570526).\n",
            "\t Train_Loss: 131.9551 Val_Loss: 306.5705  BEST VAL Loss: 306.5705\n",
            "\n",
            "Epoch 851: Validation loss did not decrease\n",
            "\t Train_Loss: 131.5867 Val_Loss: 306.6334  BEST VAL Loss: 306.5705\n",
            "\n",
            "Epoch 852: Validation loss decreased (306.570526 --> 306.288147).\n",
            "\t Train_Loss: 131.3912 Val_Loss: 306.2881  BEST VAL Loss: 306.2881\n",
            "\n",
            "Epoch 853: Validation loss decreased (306.288147 --> 305.918793).\n",
            "\t Train_Loss: 131.0510 Val_Loss: 305.9188  BEST VAL Loss: 305.9188\n",
            "\n",
            "Epoch 854: Validation loss decreased (305.918793 --> 305.787415).\n",
            "\t Train_Loss: 130.8317 Val_Loss: 305.7874  BEST VAL Loss: 305.7874\n",
            "\n",
            "Epoch 855: Validation loss decreased (305.787415 --> 305.780487).\n",
            "\t Train_Loss: 130.5247 Val_Loss: 305.7805  BEST VAL Loss: 305.7805\n",
            "\n",
            "Epoch 856: Validation loss decreased (305.780487 --> 305.733307).\n",
            "\t Train_Loss: 130.2740 Val_Loss: 305.7333  BEST VAL Loss: 305.7333\n",
            "\n",
            "Epoch 857: Validation loss did not decrease\n",
            "\t Train_Loss: 129.7997 Val_Loss: 316.3551  BEST VAL Loss: 305.7333\n",
            "\n",
            "Epoch 858: Validation loss decreased (305.733307 --> 305.599579).\n",
            "\t Train_Loss: 128.6578 Val_Loss: 305.5996  BEST VAL Loss: 305.5996\n",
            "\n",
            "Epoch 859: Validation loss decreased (305.599579 --> 305.300842).\n",
            "\t Train_Loss: 129.0795 Val_Loss: 305.3008  BEST VAL Loss: 305.3008\n",
            "\n",
            "Epoch 860: Validation loss did not decrease\n",
            "\t Train_Loss: 128.8183 Val_Loss: 306.3041  BEST VAL Loss: 305.3008\n",
            "\n",
            "Epoch 861: Validation loss did not decrease\n",
            "\t Train_Loss: 128.2733 Val_Loss: 329.1718  BEST VAL Loss: 305.3008\n",
            "\n",
            "Epoch 862: Validation loss decreased (305.300842 --> 305.221741).\n",
            "\t Train_Loss: 128.7523 Val_Loss: 305.2217  BEST VAL Loss: 305.2217\n",
            "\n",
            "Epoch 863: Validation loss decreased (305.221741 --> 303.781311).\n",
            "\t Train_Loss: 127.9880 Val_Loss: 303.7813  BEST VAL Loss: 303.7813\n",
            "\n",
            "Epoch 864: Validation loss decreased (303.781311 --> 303.628571).\n",
            "\t Train_Loss: 128.0657 Val_Loss: 303.6286  BEST VAL Loss: 303.6286\n",
            "\n",
            "Epoch 865: Validation loss decreased (303.628571 --> 303.274048).\n",
            "\t Train_Loss: 128.0894 Val_Loss: 303.2740  BEST VAL Loss: 303.2740\n",
            "\n",
            "Epoch 866: Validation loss decreased (303.274048 --> 302.741455).\n",
            "\t Train_Loss: 127.8654 Val_Loss: 302.7415  BEST VAL Loss: 302.7415\n",
            "\n",
            "Epoch 867: Validation loss decreased (302.741455 --> 302.397644).\n",
            "\t Train_Loss: 127.5660 Val_Loss: 302.3976  BEST VAL Loss: 302.3976\n",
            "\n",
            "Epoch 868: Validation loss decreased (302.397644 --> 302.224426).\n",
            "\t Train_Loss: 127.4657 Val_Loss: 302.2244  BEST VAL Loss: 302.2244\n",
            "\n",
            "Epoch 869: Validation loss decreased (302.224426 --> 302.099579).\n",
            "\t Train_Loss: 127.0827 Val_Loss: 302.0996  BEST VAL Loss: 302.0996\n",
            "\n",
            "Epoch 870: Validation loss decreased (302.099579 --> 301.742584).\n",
            "\t Train_Loss: 126.9792 Val_Loss: 301.7426  BEST VAL Loss: 301.7426\n",
            "\n",
            "Epoch 871: Validation loss decreased (301.742584 --> 301.322357).\n",
            "\t Train_Loss: 126.6761 Val_Loss: 301.3224  BEST VAL Loss: 301.3224\n",
            "\n",
            "Epoch 872: Validation loss decreased (301.322357 --> 301.021942).\n",
            "\t Train_Loss: 126.4569 Val_Loss: 301.0219  BEST VAL Loss: 301.0219\n",
            "\n",
            "Epoch 873: Validation loss decreased (301.021942 --> 300.829681).\n",
            "\t Train_Loss: 126.2802 Val_Loss: 300.8297  BEST VAL Loss: 300.8297\n",
            "\n",
            "Epoch 874: Validation loss decreased (300.829681 --> 300.643829).\n",
            "\t Train_Loss: 125.9788 Val_Loss: 300.6438  BEST VAL Loss: 300.6438\n",
            "\n",
            "Epoch 875: Validation loss decreased (300.643829 --> 300.311401).\n",
            "\t Train_Loss: 125.8359 Val_Loss: 300.3114  BEST VAL Loss: 300.3114\n",
            "\n",
            "Epoch 876: Validation loss decreased (300.311401 --> 299.950134).\n",
            "\t Train_Loss: 125.5607 Val_Loss: 299.9501  BEST VAL Loss: 299.9501\n",
            "\n",
            "Epoch 877: Validation loss decreased (299.950134 --> 299.680664).\n",
            "\t Train_Loss: 125.3489 Val_Loss: 299.6807  BEST VAL Loss: 299.6807\n",
            "\n",
            "Epoch 878: Validation loss decreased (299.680664 --> 299.537445).\n",
            "\t Train_Loss: 125.1331 Val_Loss: 299.5374  BEST VAL Loss: 299.5374\n",
            "\n",
            "Epoch 879: Validation loss did not decrease\n",
            "\t Train_Loss: 124.8096 Val_Loss: 299.8058  BEST VAL Loss: 299.5374\n",
            "\n",
            "Epoch 880: Validation loss did not decrease\n",
            "\t Train_Loss: 124.3985 Val_Loss: 310.0116  BEST VAL Loss: 299.5374\n",
            "\n",
            "Epoch 881: Validation loss did not decrease\n",
            "\t Train_Loss: 123.7547 Val_Loss: 301.6721  BEST VAL Loss: 299.5374\n",
            "\n",
            "Epoch 882: Validation loss did not decrease\n",
            "\t Train_Loss: 123.4875 Val_Loss: 301.8484  BEST VAL Loss: 299.5374\n",
            "\n",
            "Epoch 883: Validation loss did not decrease\n",
            "\t Train_Loss: 123.2341 Val_Loss: 310.3480  BEST VAL Loss: 299.5374\n",
            "\n",
            "Epoch 884: Validation loss did not decrease\n",
            "\t Train_Loss: 123.1010 Val_Loss: 300.1102  BEST VAL Loss: 299.5374\n",
            "\n",
            "Epoch 885: Validation loss decreased (299.537445 --> 298.977631).\n",
            "\t Train_Loss: 122.8952 Val_Loss: 298.9776  BEST VAL Loss: 298.9776\n",
            "\n",
            "Epoch 886: Validation loss did not decrease\n",
            "\t Train_Loss: 122.7901 Val_Loss: 300.6205  BEST VAL Loss: 298.9776\n",
            "\n",
            "Epoch 887: Validation loss did not decrease\n",
            "\t Train_Loss: 122.2893 Val_Loss: 314.2930  BEST VAL Loss: 298.9776\n",
            "\n",
            "Epoch 888: Validation loss decreased (298.977631 --> 298.918549).\n",
            "\t Train_Loss: 122.5324 Val_Loss: 298.9185  BEST VAL Loss: 298.9185\n",
            "\n",
            "Epoch 889: Validation loss decreased (298.918549 --> 296.855743).\n",
            "\t Train_Loss: 121.9582 Val_Loss: 296.8557  BEST VAL Loss: 296.8557\n",
            "\n",
            "Epoch 890: Validation loss decreased (296.855743 --> 296.350220).\n",
            "\t Train_Loss: 122.1329 Val_Loss: 296.3502  BEST VAL Loss: 296.3502\n",
            "\n",
            "Epoch 891: Validation loss did not decrease\n",
            "\t Train_Loss: 121.9867 Val_Loss: 296.7009  BEST VAL Loss: 296.3502\n",
            "\n",
            "Epoch 892: Validation loss did not decrease\n",
            "\t Train_Loss: 121.5398 Val_Loss: 300.8613  BEST VAL Loss: 296.3502\n",
            "\n",
            "Epoch 893: Validation loss did not decrease\n",
            "\t Train_Loss: 120.7607 Val_Loss: 321.0949  BEST VAL Loss: 296.3502\n",
            "\n",
            "Epoch 894: Validation loss did not decrease\n",
            "\t Train_Loss: 122.0593 Val_Loss: 297.2127  BEST VAL Loss: 296.3502\n",
            "\n",
            "Epoch 895: Validation loss decreased (296.350220 --> 294.184265).\n",
            "\t Train_Loss: 120.5851 Val_Loss: 294.1843  BEST VAL Loss: 294.1843\n",
            "\n",
            "Epoch 896: Validation loss decreased (294.184265 --> 293.308807).\n",
            "\t Train_Loss: 121.0821 Val_Loss: 293.3088  BEST VAL Loss: 293.3088\n",
            "\n",
            "Epoch 897: Validation loss decreased (293.308807 --> 292.759857).\n",
            "\t Train_Loss: 121.1891 Val_Loss: 292.7599  BEST VAL Loss: 292.7599\n",
            "\n",
            "Epoch 898: Validation loss decreased (292.759857 --> 292.305664).\n",
            "\t Train_Loss: 121.1180 Val_Loss: 292.3057  BEST VAL Loss: 292.3057\n",
            "\n",
            "Epoch 899: Validation loss decreased (292.305664 --> 291.920380).\n",
            "\t Train_Loss: 120.9904 Val_Loss: 291.9204  BEST VAL Loss: 291.9204\n",
            "\n",
            "Epoch 900: Validation loss decreased (291.920380 --> 291.592010).\n",
            "\t Train_Loss: 120.8260 Val_Loss: 291.5920  BEST VAL Loss: 291.5920\n",
            "\n",
            "Epoch 901: Validation loss decreased (291.592010 --> 291.310394).\n",
            "\t Train_Loss: 120.6249 Val_Loss: 291.3104  BEST VAL Loss: 291.3104\n",
            "\n",
            "Epoch 902: Validation loss decreased (291.310394 --> 291.119690).\n",
            "\t Train_Loss: 120.3936 Val_Loss: 291.1197  BEST VAL Loss: 291.1197\n",
            "\n",
            "Epoch 903: Validation loss did not decrease\n",
            "\t Train_Loss: 120.0755 Val_Loss: 291.2720  BEST VAL Loss: 291.1197\n",
            "\n",
            "Epoch 904: Validation loss did not decrease\n",
            "\t Train_Loss: 119.5932 Val_Loss: 293.0811  BEST VAL Loss: 291.1197\n",
            "\n",
            "Epoch 905: Validation loss did not decrease\n",
            "\t Train_Loss: 118.6949 Val_Loss: 305.9350  BEST VAL Loss: 291.1197\n",
            "\n",
            "Epoch 906: Validation loss did not decrease\n",
            "\t Train_Loss: 118.2055 Val_Loss: 313.2066  BEST VAL Loss: 291.1197\n",
            "\n",
            "Epoch 907: Validation loss did not decrease\n",
            "\t Train_Loss: 118.7627 Val_Loss: 293.9701  BEST VAL Loss: 291.1197\n",
            "\n",
            "Epoch 908: Validation loss decreased (291.119690 --> 290.395355).\n",
            "\t Train_Loss: 117.7992 Val_Loss: 290.3954  BEST VAL Loss: 290.3954\n",
            "\n",
            "Epoch 909: Validation loss decreased (290.395355 --> 289.495911).\n",
            "\t Train_Loss: 118.2771 Val_Loss: 289.4959  BEST VAL Loss: 289.4959\n",
            "\n",
            "Epoch 910: Validation loss decreased (289.495911 --> 289.482513).\n",
            "\t Train_Loss: 118.3103 Val_Loss: 289.4825  BEST VAL Loss: 289.4825\n",
            "\n",
            "Epoch 911: Validation loss did not decrease\n",
            "\t Train_Loss: 117.9740 Val_Loss: 291.1736  BEST VAL Loss: 289.4825\n",
            "\n",
            "Epoch 912: Validation loss did not decrease\n",
            "\t Train_Loss: 117.1929 Val_Loss: 300.4403  BEST VAL Loss: 289.4825\n",
            "\n",
            "Epoch 913: Validation loss did not decrease\n",
            "\t Train_Loss: 116.5464 Val_Loss: 312.5048  BEST VAL Loss: 289.4825\n",
            "\n",
            "Epoch 914: Validation loss did not decrease\n",
            "\t Train_Loss: 117.5023 Val_Loss: 293.9935  BEST VAL Loss: 289.4825\n",
            "\n",
            "Epoch 915: Validation loss decreased (289.482513 --> 288.895813).\n",
            "\t Train_Loss: 116.1198 Val_Loss: 288.8958  BEST VAL Loss: 288.8958\n",
            "\n",
            "Epoch 916: Validation loss decreased (288.895813 --> 287.817017).\n",
            "\t Train_Loss: 116.5630 Val_Loss: 287.8170  BEST VAL Loss: 287.8170\n",
            "\n",
            "Epoch 917: Validation loss did not decrease\n",
            "\t Train_Loss: 116.5623 Val_Loss: 288.6743  BEST VAL Loss: 287.8170\n",
            "\n",
            "Epoch 918: Validation loss did not decrease\n",
            "\t Train_Loss: 116.0249 Val_Loss: 294.1779  BEST VAL Loss: 287.8170\n",
            "\n",
            "Epoch 919: Validation loss did not decrease\n",
            "\t Train_Loss: 115.2150 Val_Loss: 307.5633  BEST VAL Loss: 287.8170\n",
            "\n",
            "Epoch 920: Validation loss did not decrease\n",
            "\t Train_Loss: 115.8927 Val_Loss: 296.8559  BEST VAL Loss: 287.8170\n",
            "\n",
            "Epoch 921: Validation loss did not decrease\n",
            "\t Train_Loss: 114.8213 Val_Loss: 289.0135  BEST VAL Loss: 287.8170\n",
            "\n",
            "Epoch 922: Validation loss decreased (287.817017 --> 287.203033).\n",
            "\t Train_Loss: 114.8661 Val_Loss: 287.2030  BEST VAL Loss: 287.2030\n",
            "\n",
            "Epoch 923: Validation loss did not decrease\n",
            "\t Train_Loss: 114.8909 Val_Loss: 288.7052  BEST VAL Loss: 287.2030\n",
            "\n",
            "Epoch 924: Validation loss did not decrease\n",
            "\t Train_Loss: 114.3881 Val_Loss: 295.5191  BEST VAL Loss: 287.2030\n",
            "\n",
            "Epoch 925: Validation loss did not decrease\n",
            "\t Train_Loss: 113.9712 Val_Loss: 301.2698  BEST VAL Loss: 287.2030\n",
            "\n",
            "Epoch 926: Validation loss did not decrease\n",
            "\t Train_Loss: 114.1936 Val_Loss: 292.1529  BEST VAL Loss: 287.2030\n",
            "\n",
            "Epoch 927: Validation loss decreased (287.203033 --> 286.977570).\n",
            "\t Train_Loss: 113.5043 Val_Loss: 286.9776  BEST VAL Loss: 286.9776\n",
            "\n",
            "Epoch 928: Validation loss decreased (286.977570 --> 286.467621).\n",
            "\t Train_Loss: 113.5422 Val_Loss: 286.4676  BEST VAL Loss: 286.4676\n",
            "\n",
            "Epoch 929: Validation loss did not decrease\n",
            "\t Train_Loss: 113.3519 Val_Loss: 290.0670  BEST VAL Loss: 286.4676\n",
            "\n",
            "Epoch 930: Validation loss did not decrease\n",
            "\t Train_Loss: 112.8590 Val_Loss: 296.5461  BEST VAL Loss: 286.4676\n",
            "\n",
            "Epoch 931: Validation loss did not decrease\n",
            "\t Train_Loss: 112.8946 Val_Loss: 293.0357  BEST VAL Loss: 286.4676\n",
            "\n",
            "Epoch 932: Validation loss did not decrease\n",
            "\t Train_Loss: 112.4946 Val_Loss: 286.8699  BEST VAL Loss: 286.4676\n",
            "\n",
            "Epoch 933: Validation loss decreased (286.467621 --> 285.114502).\n",
            "\t Train_Loss: 112.2796 Val_Loss: 285.1145  BEST VAL Loss: 285.1145\n",
            "\n",
            "Epoch 934: Validation loss did not decrease\n",
            "\t Train_Loss: 112.1716 Val_Loss: 287.2956  BEST VAL Loss: 285.1145\n",
            "\n",
            "Epoch 935: Validation loss did not decrease\n",
            "\t Train_Loss: 111.7293 Val_Loss: 292.1924  BEST VAL Loss: 285.1145\n",
            "\n",
            "Epoch 936: Validation loss did not decrease\n",
            "\t Train_Loss: 110.8757 Val_Loss: 311.1784  BEST VAL Loss: 285.1145\n",
            "\n",
            "Epoch 937: Validation loss decreased (285.114502 --> 277.912567).\n",
            "\t Train_Loss: 105.0399 Val_Loss: 277.9126  BEST VAL Loss: 277.9126\n",
            "\n",
            "Epoch 938: Validation loss decreased (277.912567 --> 275.421204).\n",
            "\t Train_Loss: 110.7481 Val_Loss: 275.4212  BEST VAL Loss: 275.4212\n",
            "\n",
            "Epoch 939: Validation loss decreased (275.421204 --> 274.704865).\n",
            "\t Train_Loss: 113.1062 Val_Loss: 274.7049  BEST VAL Loss: 274.7049\n",
            "\n",
            "Epoch 940: Validation loss did not decrease\n",
            "\t Train_Loss: 113.5743 Val_Loss: 274.7517  BEST VAL Loss: 274.7049\n",
            "\n",
            "Epoch 941: Validation loss did not decrease\n",
            "\t Train_Loss: 113.0024 Val_Loss: 277.0029  BEST VAL Loss: 274.7049\n",
            "\n",
            "Epoch 942: Validation loss did not decrease\n",
            "\t Train_Loss: 111.3641 Val_Loss: 290.8051  BEST VAL Loss: 274.7049\n",
            "\n",
            "Epoch 943: Validation loss did not decrease\n",
            "\t Train_Loss: 110.5484 Val_Loss: 306.3997  BEST VAL Loss: 274.7049\n",
            "\n",
            "Epoch 944: Validation loss did not decrease\n",
            "\t Train_Loss: 112.4403 Val_Loss: 284.0010  BEST VAL Loss: 274.7049\n",
            "\n",
            "Epoch 945: Validation loss did not decrease\n",
            "\t Train_Loss: 109.7746 Val_Loss: 275.1483  BEST VAL Loss: 274.7049\n",
            "\n",
            "Epoch 946: Validation loss decreased (274.704865 --> 273.295624).\n",
            "\t Train_Loss: 110.6632 Val_Loss: 273.2956  BEST VAL Loss: 273.2956\n",
            "\n",
            "Epoch 947: Validation loss did not decrease\n",
            "\t Train_Loss: 111.0023 Val_Loss: 274.1306  BEST VAL Loss: 273.2956\n",
            "\n",
            "Epoch 948: Validation loss did not decrease\n",
            "\t Train_Loss: 110.2891 Val_Loss: 280.3445  BEST VAL Loss: 273.2956\n",
            "\n",
            "Epoch 949: Validation loss did not decrease\n",
            "\t Train_Loss: 109.0847 Val_Loss: 295.6519  BEST VAL Loss: 273.2956\n",
            "\n",
            "Epoch 950: Validation loss did not decrease\n",
            "\t Train_Loss: 110.0417 Val_Loss: 289.0429  BEST VAL Loss: 273.2956\n",
            "\n",
            "Epoch 951: Validation loss did not decrease\n",
            "\t Train_Loss: 109.0969 Val_Loss: 276.4266  BEST VAL Loss: 273.2956\n",
            "\n",
            "Epoch 952: Validation loss decreased (273.295624 --> 272.623932).\n",
            "\t Train_Loss: 108.7166 Val_Loss: 272.6239  BEST VAL Loss: 272.6239\n",
            "\n",
            "Epoch 953: Validation loss did not decrease\n",
            "\t Train_Loss: 109.1588 Val_Loss: 273.0526  BEST VAL Loss: 272.6239\n",
            "\n",
            "Epoch 954: Validation loss did not decrease\n",
            "\t Train_Loss: 108.7725 Val_Loss: 278.3061  BEST VAL Loss: 272.6239\n",
            "\n",
            "Epoch 955: Validation loss did not decrease\n",
            "\t Train_Loss: 107.9622 Val_Loss: 288.0789  BEST VAL Loss: 272.6239\n",
            "\n",
            "Epoch 956: Validation loss did not decrease\n",
            "\t Train_Loss: 108.3382 Val_Loss: 284.8975  BEST VAL Loss: 272.6239\n",
            "\n",
            "Epoch 957: Validation loss did not decrease\n",
            "\t Train_Loss: 107.9247 Val_Loss: 275.6201  BEST VAL Loss: 272.6239\n",
            "\n",
            "Epoch 958: Validation loss decreased (272.623932 --> 271.925018).\n",
            "\t Train_Loss: 107.4844 Val_Loss: 271.9250  BEST VAL Loss: 271.9250\n",
            "\n",
            "Epoch 959: Validation loss did not decrease\n",
            "\t Train_Loss: 107.6594 Val_Loss: 272.4872  BEST VAL Loss: 271.9250\n",
            "\n",
            "Epoch 960: Validation loss did not decrease\n",
            "\t Train_Loss: 107.3782 Val_Loss: 277.1117  BEST VAL Loss: 271.9250\n",
            "\n",
            "Epoch 961: Validation loss did not decrease\n",
            "\t Train_Loss: 106.8687 Val_Loss: 282.5318  BEST VAL Loss: 271.9250\n",
            "\n",
            "Epoch 962: Validation loss did not decrease\n",
            "\t Train_Loss: 107.0224 Val_Loss: 279.5239  BEST VAL Loss: 271.9250\n",
            "\n",
            "Epoch 963: Validation loss did not decrease\n",
            "\t Train_Loss: 106.6840 Val_Loss: 273.4034  BEST VAL Loss: 271.9250\n",
            "\n",
            "Epoch 964: Validation loss decreased (271.925018 --> 270.730804).\n",
            "\t Train_Loss: 106.3658 Val_Loss: 270.7308  BEST VAL Loss: 270.7308\n",
            "\n",
            "Epoch 965: Validation loss did not decrease\n",
            "\t Train_Loss: 106.4149 Val_Loss: 271.5718  BEST VAL Loss: 270.7308\n",
            "\n",
            "Epoch 966: Validation loss did not decrease\n",
            "\t Train_Loss: 106.1091 Val_Loss: 275.2635  BEST VAL Loss: 270.7308\n",
            "\n",
            "Epoch 967: Validation loss did not decrease\n",
            "\t Train_Loss: 105.8121 Val_Loss: 277.8419  BEST VAL Loss: 270.7308\n",
            "\n",
            "Epoch 968: Validation loss did not decrease\n",
            "\t Train_Loss: 105.8389 Val_Loss: 274.9212  BEST VAL Loss: 270.7308\n",
            "\n",
            "Epoch 969: Validation loss decreased (270.730804 --> 270.714264).\n",
            "\t Train_Loss: 105.4970 Val_Loss: 270.7143  BEST VAL Loss: 270.7143\n",
            "\n",
            "Epoch 970: Validation loss decreased (270.714264 --> 269.038849).\n",
            "\t Train_Loss: 105.3273 Val_Loss: 269.0388  BEST VAL Loss: 269.0388\n",
            "\n",
            "Epoch 971: Validation loss did not decrease\n",
            "\t Train_Loss: 105.2587 Val_Loss: 270.1764  BEST VAL Loss: 269.0388\n",
            "\n",
            "Epoch 972: Validation loss did not decrease\n",
            "\t Train_Loss: 104.9639 Val_Loss: 272.9749  BEST VAL Loss: 269.0388\n",
            "\n",
            "Epoch 973: Validation loss did not decrease\n",
            "\t Train_Loss: 104.8003 Val_Loss: 273.7472  BEST VAL Loss: 269.0388\n",
            "\n",
            "Epoch 974: Validation loss did not decrease\n",
            "\t Train_Loss: 104.7013 Val_Loss: 270.8835  BEST VAL Loss: 269.0388\n",
            "\n",
            "Epoch 975: Validation loss decreased (269.038849 --> 267.922821).\n",
            "\t Train_Loss: 104.4217 Val_Loss: 267.9228  BEST VAL Loss: 267.9228\n",
            "\n",
            "Epoch 976: Validation loss decreased (267.922821 --> 267.170898).\n",
            "\t Train_Loss: 104.3048 Val_Loss: 267.1709  BEST VAL Loss: 267.1709\n",
            "\n",
            "Epoch 977: Validation loss did not decrease\n",
            "\t Train_Loss: 104.1540 Val_Loss: 268.5920  BEST VAL Loss: 267.1709\n",
            "\n",
            "Epoch 978: Validation loss did not decrease\n",
            "\t Train_Loss: 103.9148 Val_Loss: 270.4064  BEST VAL Loss: 267.1709\n",
            "\n",
            "Epoch 979: Validation loss did not decrease\n",
            "\t Train_Loss: 103.7945 Val_Loss: 269.7817  BEST VAL Loss: 267.1709\n",
            "\n",
            "Epoch 980: Validation loss did not decrease\n",
            "\t Train_Loss: 103.6236 Val_Loss: 267.1990  BEST VAL Loss: 267.1709\n",
            "\n",
            "Epoch 981: Validation loss decreased (267.170898 --> 265.393097).\n",
            "\t Train_Loss: 103.4162 Val_Loss: 265.3931  BEST VAL Loss: 265.3931\n",
            "\n",
            "Epoch 982: Validation loss did not decrease\n",
            "\t Train_Loss: 103.2938 Val_Loss: 265.4462  BEST VAL Loss: 265.3931\n",
            "\n",
            "Epoch 983: Validation loss did not decrease\n",
            "\t Train_Loss: 103.1126 Val_Loss: 266.7203  BEST VAL Loss: 265.3931\n",
            "\n",
            "Epoch 984: Validation loss did not decrease\n",
            "\t Train_Loss: 102.9268 Val_Loss: 267.2732  BEST VAL Loss: 265.3931\n",
            "\n",
            "Epoch 985: Validation loss did not decrease\n",
            "\t Train_Loss: 102.7957 Val_Loss: 265.9714  BEST VAL Loss: 265.3931\n",
            "\n",
            "Epoch 986: Validation loss decreased (265.393097 --> 264.147034).\n",
            "\t Train_Loss: 102.6125 Val_Loss: 264.1470  BEST VAL Loss: 264.1470\n",
            "\n",
            "Epoch 987: Validation loss decreased (264.147034 --> 263.295288).\n",
            "\t Train_Loss: 102.4462 Val_Loss: 263.2953  BEST VAL Loss: 263.2953\n",
            "\n",
            "Epoch 988: Validation loss did not decrease\n",
            "\t Train_Loss: 102.3049 Val_Loss: 263.6050  BEST VAL Loss: 263.2953\n",
            "\n",
            "Epoch 989: Validation loss did not decrease\n",
            "\t Train_Loss: 102.1293 Val_Loss: 264.1772  BEST VAL Loss: 263.2953\n",
            "\n",
            "Epoch 990: Validation loss did not decrease\n",
            "\t Train_Loss: 101.9678 Val_Loss: 263.8949  BEST VAL Loss: 263.2953\n",
            "\n",
            "Epoch 991: Validation loss decreased (263.295288 --> 262.790192).\n",
            "\t Train_Loss: 101.8240 Val_Loss: 262.7902  BEST VAL Loss: 262.7902\n",
            "\n",
            "Epoch 992: Validation loss decreased (262.790192 --> 261.714996).\n",
            "\t Train_Loss: 101.6518 Val_Loss: 261.7150  BEST VAL Loss: 261.7150\n",
            "\n",
            "Epoch 993: Validation loss decreased (261.714996 --> 261.229614).\n",
            "\t Train_Loss: 101.4984 Val_Loss: 261.2296  BEST VAL Loss: 261.2296\n",
            "\n",
            "Epoch 994: Validation loss did not decrease\n",
            "\t Train_Loss: 101.3507 Val_Loss: 261.2479  BEST VAL Loss: 261.2296\n",
            "\n",
            "Epoch 995: Validation loss did not decrease\n",
            "\t Train_Loss: 101.1855 Val_Loss: 261.2761  BEST VAL Loss: 261.2296\n",
            "\n",
            "Epoch 996: Validation loss decreased (261.229614 --> 260.923462).\n",
            "\t Train_Loss: 101.0326 Val_Loss: 260.9235  BEST VAL Loss: 260.9235\n",
            "\n",
            "Epoch 997: Validation loss decreased (260.923462 --> 260.289032).\n",
            "\t Train_Loss: 100.8813 Val_Loss: 260.2890  BEST VAL Loss: 260.2890\n",
            "\n",
            "Epoch 998: Validation loss did not decrease\n",
            "\t Train_Loss: 100.6181 Val_Loss: 265.3008  BEST VAL Loss: 260.2890\n",
            "\n",
            "Epoch 999: Validation loss decreased (260.289032 --> 257.715149).\n",
            "\t Train_Loss: 99.8148 Val_Loss: 257.7151  BEST VAL Loss: 257.7151\n",
            "\n",
            "Epoch 1000: Validation loss did not decrease\n",
            "\t Train_Loss: 100.0554 Val_Loss: 263.7891  BEST VAL Loss: 257.7151\n",
            "\n",
            "Epoch 1001: Validation loss did not decrease\n",
            "\t Train_Loss: 99.4405 Val_Loss: 261.0664  BEST VAL Loss: 257.7151\n",
            "\n",
            "Epoch 1002: Validation loss did not decrease\n",
            "\t Train_Loss: 99.2520 Val_Loss: 270.4937  BEST VAL Loss: 257.7151\n",
            "\n",
            "Epoch 1003: Validation loss decreased (257.715149 --> 255.171143).\n",
            "\t Train_Loss: 99.5127 Val_Loss: 255.1711  BEST VAL Loss: 255.1711\n",
            "\n",
            "Epoch 1004: Validation loss did not decrease\n",
            "\t Train_Loss: 99.5741 Val_Loss: 255.4460  BEST VAL Loss: 255.1711\n",
            "\n",
            "Epoch 1005: Validation loss did not decrease\n",
            "\t Train_Loss: 99.6289 Val_Loss: 258.0410  BEST VAL Loss: 255.1711\n",
            "\n",
            "Epoch 1006: Validation loss did not decrease\n",
            "\t Train_Loss: 99.5375 Val_Loss: 258.2948  BEST VAL Loss: 255.1711\n",
            "\n",
            "Epoch 1007: Validation loss did not decrease\n",
            "\t Train_Loss: 99.4088 Val_Loss: 255.4775  BEST VAL Loss: 255.1711\n",
            "\n",
            "Epoch 1008: Validation loss decreased (255.171143 --> 253.717148).\n",
            "\t Train_Loss: 99.2225 Val_Loss: 253.7171  BEST VAL Loss: 253.7171\n",
            "\n",
            "Epoch 1009: Validation loss did not decrease\n",
            "\t Train_Loss: 99.0858 Val_Loss: 254.2803  BEST VAL Loss: 253.7171\n",
            "\n",
            "Epoch 1010: Validation loss did not decrease\n",
            "\t Train_Loss: 98.8690 Val_Loss: 255.4949  BEST VAL Loss: 253.7171\n",
            "\n",
            "Epoch 1011: Validation loss did not decrease\n",
            "\t Train_Loss: 98.6962 Val_Loss: 255.0251  BEST VAL Loss: 253.7171\n",
            "\n",
            "Epoch 1012: Validation loss did not decrease\n",
            "\t Train_Loss: 98.3622 Val_Loss: 254.6840  BEST VAL Loss: 253.7171\n",
            "\n",
            "Epoch 1013: Validation loss did not decrease\n",
            "\t Train_Loss: 97.8413 Val_Loss: 266.9154  BEST VAL Loss: 253.7171\n",
            "\n",
            "Epoch 1014: Validation loss decreased (253.717148 --> 252.710800).\n",
            "\t Train_Loss: 98.0445 Val_Loss: 252.7108  BEST VAL Loss: 252.7108\n",
            "\n",
            "Epoch 1015: Validation loss decreased (252.710800 --> 251.498123).\n",
            "\t Train_Loss: 97.4559 Val_Loss: 251.4981  BEST VAL Loss: 251.4981\n",
            "\n",
            "Epoch 1016: Validation loss did not decrease\n",
            "\t Train_Loss: 97.4816 Val_Loss: 254.5335  BEST VAL Loss: 251.4981\n",
            "\n",
            "Epoch 1017: Validation loss did not decrease\n",
            "\t Train_Loss: 97.3379 Val_Loss: 256.4828  BEST VAL Loss: 251.4981\n",
            "\n",
            "Epoch 1018: Validation loss did not decrease\n",
            "\t Train_Loss: 96.9961 Val_Loss: 258.4758  BEST VAL Loss: 251.4981\n",
            "\n",
            "Epoch 1019: Validation loss did not decrease\n",
            "\t Train_Loss: 96.7464 Val_Loss: 258.0879  BEST VAL Loss: 251.4981\n",
            "\n",
            "Epoch 1020: Validation loss decreased (251.498123 --> 250.986130).\n",
            "\t Train_Loss: 96.6805 Val_Loss: 250.9861  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1021: Validation loss did not decrease\n",
            "\t Train_Loss: 96.4500 Val_Loss: 251.9811  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1022: Validation loss did not decrease\n",
            "\t Train_Loss: 96.3293 Val_Loss: 255.6612  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1023: Validation loss did not decrease\n",
            "\t Train_Loss: 96.0513 Val_Loss: 257.0930  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1024: Validation loss did not decrease\n",
            "\t Train_Loss: 95.8663 Val_Loss: 254.0226  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1025: Validation loss did not decrease\n",
            "\t Train_Loss: 95.6760 Val_Loss: 251.6228  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1026: Validation loss did not decrease\n",
            "\t Train_Loss: 95.4198 Val_Loss: 252.3418  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1027: Validation loss did not decrease\n",
            "\t Train_Loss: 95.3720 Val_Loss: 253.1955  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1028: Validation loss did not decrease\n",
            "\t Train_Loss: 95.0312 Val_Loss: 252.5431  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1029: Validation loss did not decrease\n",
            "\t Train_Loss: 94.8322 Val_Loss: 252.7426  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1030: Validation loss did not decrease\n",
            "\t Train_Loss: 94.7199 Val_Loss: 252.9648  BEST VAL Loss: 250.9861\n",
            "\n",
            "Epoch 1031: Validation loss decreased (250.986130 --> 250.657623).\n",
            "\t Train_Loss: 94.4132 Val_Loss: 250.6576  BEST VAL Loss: 250.6576\n",
            "\n",
            "Epoch 1032: Validation loss decreased (250.657623 --> 249.574020).\n",
            "\t Train_Loss: 94.2771 Val_Loss: 249.5740  BEST VAL Loss: 249.5740\n",
            "\n",
            "Epoch 1033: Validation loss did not decrease\n",
            "\t Train_Loss: 94.0677 Val_Loss: 251.2294  BEST VAL Loss: 249.5740\n",
            "\n",
            "Epoch 1034: Validation loss did not decrease\n",
            "\t Train_Loss: 93.8317 Val_Loss: 250.7416  BEST VAL Loss: 249.5740\n",
            "\n",
            "Epoch 1035: Validation loss did not decrease\n",
            "\t Train_Loss: 93.6482 Val_Loss: 250.3786  BEST VAL Loss: 249.5740\n",
            "\n",
            "Epoch 1036: Validation loss did not decrease\n",
            "\t Train_Loss: 93.4416 Val_Loss: 250.5985  BEST VAL Loss: 249.5740\n",
            "\n",
            "Epoch 1037: Validation loss decreased (249.574020 --> 249.189804).\n",
            "\t Train_Loss: 93.2390 Val_Loss: 249.1898  BEST VAL Loss: 249.1898\n",
            "\n",
            "Epoch 1038: Validation loss decreased (249.189804 --> 247.876785).\n",
            "\t Train_Loss: 93.0148 Val_Loss: 247.8768  BEST VAL Loss: 247.8768\n",
            "\n",
            "Epoch 1039: Validation loss did not decrease\n",
            "\t Train_Loss: 92.8421 Val_Loss: 248.3137  BEST VAL Loss: 247.8768\n",
            "\n",
            "Epoch 1040: Validation loss did not decrease\n",
            "\t Train_Loss: 92.6222 Val_Loss: 248.3666  BEST VAL Loss: 247.8768\n",
            "\n",
            "Epoch 1041: Validation loss did not decrease\n",
            "\t Train_Loss: 92.3955 Val_Loss: 248.5355  BEST VAL Loss: 247.8768\n",
            "\n",
            "Epoch 1042: Validation loss did not decrease\n",
            "\t Train_Loss: 92.2242 Val_Loss: 248.1430  BEST VAL Loss: 247.8768\n",
            "\n",
            "Epoch 1043: Validation loss decreased (247.876785 --> 246.899948).\n",
            "\t Train_Loss: 92.0028 Val_Loss: 246.8999  BEST VAL Loss: 246.8999\n",
            "\n",
            "Epoch 1044: Validation loss decreased (246.899948 --> 246.009888).\n",
            "\t Train_Loss: 91.7882 Val_Loss: 246.0099  BEST VAL Loss: 246.0099\n",
            "\n",
            "Epoch 1045: Validation loss decreased (246.009888 --> 245.875153).\n",
            "\t Train_Loss: 91.6000 Val_Loss: 245.8752  BEST VAL Loss: 245.8752\n",
            "\n",
            "Epoch 1046: Validation loss decreased (245.875153 --> 245.767654).\n",
            "\t Train_Loss: 91.3890 Val_Loss: 245.7677  BEST VAL Loss: 245.7677\n",
            "\n",
            "Epoch 1047: Validation loss decreased (245.767654 --> 245.719589).\n",
            "\t Train_Loss: 91.1848 Val_Loss: 245.7196  BEST VAL Loss: 245.7196\n",
            "\n",
            "Epoch 1048: Validation loss decreased (245.719589 --> 245.252243).\n",
            "\t Train_Loss: 90.9772 Val_Loss: 245.2522  BEST VAL Loss: 245.2522\n",
            "\n",
            "Epoch 1049: Validation loss decreased (245.252243 --> 244.540634).\n",
            "\t Train_Loss: 90.7783 Val_Loss: 244.5406  BEST VAL Loss: 244.5406\n",
            "\n",
            "Epoch 1050: Validation loss decreased (244.540634 --> 244.048706).\n",
            "\t Train_Loss: 90.5749 Val_Loss: 244.0487  BEST VAL Loss: 244.0487\n",
            "\n",
            "Epoch 1051: Validation loss decreased (244.048706 --> 243.461670).\n",
            "\t Train_Loss: 90.3650 Val_Loss: 243.4617  BEST VAL Loss: 243.4617\n",
            "\n",
            "Epoch 1052: Validation loss decreased (243.461670 --> 243.148514).\n",
            "\t Train_Loss: 90.1727 Val_Loss: 243.1485  BEST VAL Loss: 243.1485\n",
            "\n",
            "Epoch 1053: Validation loss decreased (243.148514 --> 242.540283).\n",
            "\t Train_Loss: 89.9707 Val_Loss: 242.5403  BEST VAL Loss: 242.5403\n",
            "\n",
            "Epoch 1054: Validation loss decreased (242.540283 --> 242.493774).\n",
            "\t Train_Loss: 89.7636 Val_Loss: 242.4938  BEST VAL Loss: 242.4938\n",
            "\n",
            "Epoch 1055: Validation loss decreased (242.493774 --> 241.898788).\n",
            "\t Train_Loss: 89.5676 Val_Loss: 241.8988  BEST VAL Loss: 241.8988\n",
            "\n",
            "Epoch 1056: Validation loss did not decrease\n",
            "\t Train_Loss: 89.3694 Val_Loss: 242.1072  BEST VAL Loss: 241.8988\n",
            "\n",
            "Epoch 1057: Validation loss decreased (241.898788 --> 240.450119).\n",
            "\t Train_Loss: 89.1714 Val_Loss: 240.4501  BEST VAL Loss: 240.4501\n",
            "\n",
            "Epoch 1058: Validation loss did not decrease\n",
            "\t Train_Loss: 88.9757 Val_Loss: 241.4639  BEST VAL Loss: 240.4501\n",
            "\n",
            "Epoch 1059: Validation loss decreased (240.450119 --> 237.706055).\n",
            "\t Train_Loss: 88.7883 Val_Loss: 237.7061  BEST VAL Loss: 237.7061\n",
            "\n",
            "Epoch 1060: Validation loss did not decrease\n",
            "\t Train_Loss: 88.6309 Val_Loss: 243.5810  BEST VAL Loss: 237.7061\n",
            "\n",
            "Epoch 1061: Validation loss decreased (237.706055 --> 233.236130).\n",
            "\t Train_Loss: 88.5295 Val_Loss: 233.2361  BEST VAL Loss: 233.2361\n",
            "\n",
            "Epoch 1062: Validation loss did not decrease\n",
            "\t Train_Loss: 88.7156 Val_Loss: 242.2860  BEST VAL Loss: 233.2361\n",
            "\n",
            "Epoch 1063: Validation loss did not decrease\n",
            "\t Train_Loss: 88.1124 Val_Loss: 239.9519  BEST VAL Loss: 233.2361\n",
            "\n",
            "Epoch 1064: Validation loss decreased (233.236130 --> 233.012741).\n",
            "\t Train_Loss: 87.8368 Val_Loss: 233.0127  BEST VAL Loss: 233.0127\n",
            "\n",
            "Epoch 1065: Validation loss did not decrease\n",
            "\t Train_Loss: 87.8946 Val_Loss: 242.4578  BEST VAL Loss: 233.0127\n",
            "\n",
            "Epoch 1066: Validation loss decreased (233.012741 --> 232.375565).\n",
            "\t Train_Loss: 87.7024 Val_Loss: 232.3756  BEST VAL Loss: 232.3756\n",
            "\n",
            "Epoch 1067: Validation loss did not decrease\n",
            "\t Train_Loss: 87.5233 Val_Loss: 236.5843  BEST VAL Loss: 232.3756\n",
            "\n",
            "Epoch 1068: Validation loss did not decrease\n",
            "\t Train_Loss: 87.1102 Val_Loss: 241.8193  BEST VAL Loss: 232.3756\n",
            "\n",
            "Epoch 1069: Validation loss decreased (232.375565 --> 228.495590).\n",
            "\t Train_Loss: 87.1908 Val_Loss: 228.4956  BEST VAL Loss: 228.4956\n",
            "\n",
            "Epoch 1070: Validation loss did not decrease\n",
            "\t Train_Loss: 87.3816 Val_Loss: 233.6979  BEST VAL Loss: 228.4956\n",
            "\n",
            "Epoch 1071: Validation loss did not decrease\n",
            "\t Train_Loss: 86.5576 Val_Loss: 246.3505  BEST VAL Loss: 228.4956\n",
            "\n",
            "Epoch 1072: Validation loss decreased (228.495590 --> 225.935669).\n",
            "\t Train_Loss: 87.4049 Val_Loss: 225.9357  BEST VAL Loss: 225.9357\n",
            "\n",
            "Epoch 1073: Validation loss did not decrease\n",
            "\t Train_Loss: 87.2397 Val_Loss: 226.1495  BEST VAL Loss: 225.9357\n",
            "\n",
            "Epoch 1074: Validation loss did not decrease\n",
            "\t Train_Loss: 87.0641 Val_Loss: 239.0922  BEST VAL Loss: 225.9357\n",
            "\n",
            "Epoch 1075: Validation loss did not decrease\n",
            "\t Train_Loss: 86.2918 Val_Loss: 240.9398  BEST VAL Loss: 225.9357\n",
            "\n",
            "Epoch 1076: Validation loss decreased (225.935669 --> 224.275635).\n",
            "\t Train_Loss: 86.2469 Val_Loss: 224.2756  BEST VAL Loss: 224.2756\n",
            "\n",
            "Epoch 1077: Validation loss did not decrease\n",
            "\t Train_Loss: 86.5914 Val_Loss: 226.0268  BEST VAL Loss: 224.2756\n",
            "\n",
            "Epoch 1078: Validation loss did not decrease\n",
            "\t Train_Loss: 85.7670 Val_Loss: 240.7935  BEST VAL Loss: 224.2756\n",
            "\n",
            "Epoch 1079: Validation loss did not decrease\n",
            "\t Train_Loss: 86.2480 Val_Loss: 231.4976  BEST VAL Loss: 224.2756\n",
            "\n",
            "Epoch 1080: Validation loss did not decrease\n",
            "\t Train_Loss: 85.0970 Val_Loss: 224.3306  BEST VAL Loss: 224.2756\n",
            "\n",
            "Epoch 1081: Validation loss did not decrease\n",
            "\t Train_Loss: 85.5403 Val_Loss: 229.1055  BEST VAL Loss: 224.2756\n",
            "\n",
            "Epoch 1082: Validation loss did not decrease\n",
            "\t Train_Loss: 84.7812 Val_Loss: 238.7431  BEST VAL Loss: 224.2756\n",
            "\n",
            "Epoch 1083: Validation loss did not decrease\n",
            "\t Train_Loss: 85.2683 Val_Loss: 226.9460  BEST VAL Loss: 224.2756\n",
            "\n",
            "Epoch 1084: Validation loss decreased (224.275635 --> 222.686325).\n",
            "\t Train_Loss: 84.4974 Val_Loss: 222.6863  BEST VAL Loss: 222.6863\n",
            "\n",
            "Epoch 1085: Validation loss did not decrease\n",
            "\t Train_Loss: 84.7611 Val_Loss: 227.9657  BEST VAL Loss: 222.6863\n",
            "\n",
            "Epoch 1086: Validation loss did not decrease\n",
            "\t Train_Loss: 84.1381 Val_Loss: 235.1655  BEST VAL Loss: 222.6863\n",
            "\n",
            "Epoch 1087: Validation loss did not decrease\n",
            "\t Train_Loss: 84.4370 Val_Loss: 225.8981  BEST VAL Loss: 222.6863\n",
            "\n",
            "Epoch 1088: Validation loss decreased (222.686325 --> 222.668930).\n",
            "\t Train_Loss: 83.8824 Val_Loss: 222.6689  BEST VAL Loss: 222.6689\n",
            "\n",
            "Epoch 1089: Validation loss did not decrease\n",
            "\t Train_Loss: 83.9870 Val_Loss: 227.9609  BEST VAL Loss: 222.6689\n",
            "\n",
            "Epoch 1090: Validation loss did not decrease\n",
            "\t Train_Loss: 83.5591 Val_Loss: 231.1461  BEST VAL Loss: 222.6689\n",
            "\n",
            "Epoch 1091: Validation loss did not decrease\n",
            "\t Train_Loss: 83.6615 Val_Loss: 223.5332  BEST VAL Loss: 222.6689\n",
            "\n",
            "Epoch 1092: Validation loss decreased (222.668930 --> 221.310303).\n",
            "\t Train_Loss: 83.2877 Val_Loss: 221.3103  BEST VAL Loss: 221.3103\n",
            "\n",
            "Epoch 1093: Validation loss did not decrease\n",
            "\t Train_Loss: 83.3038 Val_Loss: 226.5210  BEST VAL Loss: 221.3103\n",
            "\n",
            "Epoch 1094: Validation loss did not decrease\n",
            "\t Train_Loss: 82.9790 Val_Loss: 228.5285  BEST VAL Loss: 221.3103\n",
            "\n",
            "Epoch 1095: Validation loss did not decrease\n",
            "\t Train_Loss: 82.9569 Val_Loss: 222.4560  BEST VAL Loss: 221.3103\n",
            "\n",
            "Epoch 1096: Validation loss decreased (221.310303 --> 220.776077).\n",
            "\t Train_Loss: 82.7185 Val_Loss: 220.7761  BEST VAL Loss: 220.7761\n",
            "\n",
            "Epoch 1097: Validation loss did not decrease\n",
            "\t Train_Loss: 82.6489 Val_Loss: 224.8183  BEST VAL Loss: 220.7761\n",
            "\n",
            "Epoch 1098: Validation loss did not decrease\n",
            "\t Train_Loss: 82.3991 Val_Loss: 225.9243  BEST VAL Loss: 220.7761\n",
            "\n",
            "Epoch 1099: Validation loss decreased (220.776077 --> 220.550400).\n",
            "\t Train_Loss: 82.3442 Val_Loss: 220.5504  BEST VAL Loss: 220.5504\n",
            "\n",
            "Epoch 1100: Validation loss decreased (220.550400 --> 219.468460).\n",
            "\t Train_Loss: 82.1369 Val_Loss: 219.4685  BEST VAL Loss: 219.4685\n",
            "\n",
            "Epoch 1101: Validation loss did not decrease\n",
            "\t Train_Loss: 82.0339 Val_Loss: 223.0418  BEST VAL Loss: 219.4685\n",
            "\n",
            "Epoch 1102: Validation loss did not decrease\n",
            "\t Train_Loss: 81.8548 Val_Loss: 223.4400  BEST VAL Loss: 219.4685\n",
            "\n",
            "Epoch 1103: Validation loss decreased (219.468460 --> 219.366455).\n",
            "\t Train_Loss: 81.7360 Val_Loss: 219.3665  BEST VAL Loss: 219.3665\n",
            "\n",
            "Epoch 1104: Validation loss decreased (219.366455 --> 218.801514).\n",
            "\t Train_Loss: 81.5796 Val_Loss: 218.8015  BEST VAL Loss: 218.8015\n",
            "\n",
            "Epoch 1105: Validation loss did not decrease\n",
            "\t Train_Loss: 81.4530 Val_Loss: 221.7272  BEST VAL Loss: 218.8015\n",
            "\n",
            "Epoch 1106: Validation loss did not decrease\n",
            "\t Train_Loss: 81.1988 Val_Loss: 223.5535  BEST VAL Loss: 218.8015\n",
            "\n",
            "Epoch 1107: Validation loss did not decrease\n",
            "\t Train_Loss: 75.5656 Val_Loss: 304.8539  BEST VAL Loss: 218.8015\n",
            "\n",
            "Epoch 1108: Validation loss did not decrease\n",
            "\t Train_Loss: 96.2918 Val_Loss: 269.0457  BEST VAL Loss: 218.8015\n",
            "\n",
            "Epoch 1109: Validation loss decreased (218.801514 --> 199.309845).\n",
            "\t Train_Loss: 91.2858 Val_Loss: 199.3098  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1110: Validation loss did not decrease\n",
            "\t Train_Loss: 90.4317 Val_Loss: 199.3176  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1111: Validation loss did not decrease\n",
            "\t Train_Loss: 87.7731 Val_Loss: 209.2236  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1112: Validation loss did not decrease\n",
            "\t Train_Loss: 80.7556 Val_Loss: 254.7165  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1113: Validation loss did not decrease\n",
            "\t Train_Loss: 91.8037 Val_Loss: 211.9184  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1114: Validation loss did not decrease\n",
            "\t Train_Loss: 72.0108 Val_Loss: 206.3153  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1115: Validation loss did not decrease\n",
            "\t Train_Loss: 79.4591 Val_Loss: 238.3907  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1116: Validation loss did not decrease\n",
            "\t Train_Loss: 83.1644 Val_Loss: 238.8729  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1117: Validation loss did not decrease\n",
            "\t Train_Loss: 78.1138 Val_Loss: 211.1473  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1118: Validation loss did not decrease\n",
            "\t Train_Loss: 70.1685 Val_Loss: 210.6976  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1119: Validation loss did not decrease\n",
            "\t Train_Loss: 72.3741 Val_Loss: 212.7467  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1120: Validation loss did not decrease\n",
            "\t Train_Loss: 74.8177 Val_Loss: 204.7343  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1121: Validation loss did not decrease\n",
            "\t Train_Loss: 73.4214 Val_Loss: 199.3608  BEST VAL Loss: 199.3098\n",
            "\n",
            "Epoch 1122: Validation loss decreased (199.309845 --> 198.927353).\n",
            "\t Train_Loss: 73.7176 Val_Loss: 198.9274  BEST VAL Loss: 198.9274\n",
            "\n",
            "Epoch 1123: Validation loss did not decrease\n",
            "\t Train_Loss: 73.1432 Val_Loss: 203.3546  BEST VAL Loss: 198.9274\n",
            "\n",
            "Epoch 1124: Validation loss did not decrease\n",
            "\t Train_Loss: 72.3037 Val_Loss: 210.7397  BEST VAL Loss: 198.9274\n",
            "\n",
            "Epoch 1125: Validation loss did not decrease\n",
            "\t Train_Loss: 72.9881 Val_Loss: 210.2628  BEST VAL Loss: 198.9274\n",
            "\n",
            "Epoch 1126: Validation loss did not decrease\n",
            "\t Train_Loss: 72.6135 Val_Loss: 203.1689  BEST VAL Loss: 198.9274\n",
            "\n",
            "Epoch 1127: Validation loss decreased (198.927353 --> 198.489059).\n",
            "\t Train_Loss: 71.4579 Val_Loss: 198.4891  BEST VAL Loss: 198.4891\n",
            "\n",
            "Epoch 1128: Validation loss decreased (198.489059 --> 197.268051).\n",
            "\t Train_Loss: 71.5726 Val_Loss: 197.2681  BEST VAL Loss: 197.2681\n",
            "\n",
            "Epoch 1129: Validation loss did not decrease\n",
            "\t Train_Loss: 71.5658 Val_Loss: 198.4763  BEST VAL Loss: 197.2681\n",
            "\n",
            "Epoch 1130: Validation loss did not decrease\n",
            "\t Train_Loss: 71.0433 Val_Loss: 200.5930  BEST VAL Loss: 197.2681\n",
            "\n",
            "Epoch 1131: Validation loss did not decrease\n",
            "\t Train_Loss: 70.8492 Val_Loss: 200.8784  BEST VAL Loss: 197.2681\n",
            "\n",
            "Epoch 1132: Validation loss did not decrease\n",
            "\t Train_Loss: 70.5710 Val_Loss: 198.8200  BEST VAL Loss: 197.2681\n",
            "\n",
            "Epoch 1133: Validation loss decreased (197.268051 --> 197.001007).\n",
            "\t Train_Loss: 70.2504 Val_Loss: 197.0010  BEST VAL Loss: 197.0010\n",
            "\n",
            "Epoch 1134: Validation loss did not decrease\n",
            "\t Train_Loss: 70.2631 Val_Loss: 197.0444  BEST VAL Loss: 197.0010\n",
            "\n",
            "Epoch 1135: Validation loss did not decrease\n",
            "\t Train_Loss: 69.8864 Val_Loss: 198.5334  BEST VAL Loss: 197.0010\n",
            "\n",
            "Epoch 1136: Validation loss did not decrease\n",
            "\t Train_Loss: 69.5985 Val_Loss: 199.0384  BEST VAL Loss: 197.0010\n",
            "\n",
            "Epoch 1137: Validation loss did not decrease\n",
            "\t Train_Loss: 68.4642 Val_Loss: 217.2461  BEST VAL Loss: 197.0010\n",
            "\n",
            "Epoch 1138: Validation loss decreased (197.001007 --> 189.507278).\n",
            "\t Train_Loss: 69.0446 Val_Loss: 189.5073  BEST VAL Loss: 189.5073\n",
            "\n",
            "Epoch 1139: Validation loss decreased (189.507278 --> 187.895950).\n",
            "\t Train_Loss: 68.1755 Val_Loss: 187.8960  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1140: Validation loss did not decrease\n",
            "\t Train_Loss: 71.3325 Val_Loss: 189.2997  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1141: Validation loss did not decrease\n",
            "\t Train_Loss: 69.9302 Val_Loss: 194.3250  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1142: Validation loss did not decrease\n",
            "\t Train_Loss: 69.0845 Val_Loss: 201.2794  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1143: Validation loss did not decrease\n",
            "\t Train_Loss: 70.2348 Val_Loss: 201.1414  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1144: Validation loss did not decrease\n",
            "\t Train_Loss: 69.4762 Val_Loss: 194.7049  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1145: Validation loss did not decrease\n",
            "\t Train_Loss: 68.2353 Val_Loss: 190.1904  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1146: Validation loss did not decrease\n",
            "\t Train_Loss: 69.1828 Val_Loss: 189.2419  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1147: Validation loss did not decrease\n",
            "\t Train_Loss: 69.0788 Val_Loss: 191.0745  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1148: Validation loss did not decrease\n",
            "\t Train_Loss: 67.9601 Val_Loss: 194.0177  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1149: Validation loss did not decrease\n",
            "\t Train_Loss: 68.0525 Val_Loss: 194.6129  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1150: Validation loss did not decrease\n",
            "\t Train_Loss: 68.2473 Val_Loss: 192.0210  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1151: Validation loss did not decrease\n",
            "\t Train_Loss: 67.6783 Val_Loss: 189.0775  BEST VAL Loss: 187.8960\n",
            "\n",
            "Epoch 1152: Validation loss decreased (187.895950 --> 187.585007).\n",
            "\t Train_Loss: 67.3942 Val_Loss: 187.5850  BEST VAL Loss: 187.5850\n",
            "\n",
            "Epoch 1153: Validation loss did not decrease\n",
            "\t Train_Loss: 67.6115 Val_Loss: 188.1418  BEST VAL Loss: 187.5850\n",
            "\n",
            "Epoch 1154: Validation loss did not decrease\n",
            "\t Train_Loss: 67.2029 Val_Loss: 190.8683  BEST VAL Loss: 187.5850\n",
            "\n",
            "Epoch 1155: Validation loss did not decrease\n",
            "\t Train_Loss: 66.6789 Val_Loss: 193.6150  BEST VAL Loss: 187.5850\n",
            "\n",
            "Epoch 1156: Validation loss did not decrease\n",
            "\t Train_Loss: 66.9433 Val_Loss: 192.6927  BEST VAL Loss: 187.5850\n",
            "\n",
            "Epoch 1157: Validation loss did not decrease\n",
            "\t Train_Loss: 66.7066 Val_Loss: 189.0767  BEST VAL Loss: 187.5850\n",
            "\n",
            "Epoch 1158: Validation loss decreased (187.585007 --> 186.317001).\n",
            "\t Train_Loss: 66.0956 Val_Loss: 186.3170  BEST VAL Loss: 186.3170\n",
            "\n",
            "Epoch 1159: Validation loss decreased (186.317001 --> 185.594177).\n",
            "\t Train_Loss: 65.9283 Val_Loss: 185.5942  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1160: Validation loss did not decrease\n",
            "\t Train_Loss: 65.6255 Val_Loss: 186.6830  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1161: Validation loss did not decrease\n",
            "\t Train_Loss: 64.6688 Val_Loss: 189.3144  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1162: Validation loss did not decrease\n",
            "\t Train_Loss: 63.4668 Val_Loss: 194.0878  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1163: Validation loss did not decrease\n",
            "\t Train_Loss: 62.5808 Val_Loss: 200.6328  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1164: Validation loss did not decrease\n",
            "\t Train_Loss: 63.0725 Val_Loss: 194.0150  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1165: Validation loss did not decrease\n",
            "\t Train_Loss: 61.9624 Val_Loss: 186.7747  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1166: Validation loss did not decrease\n",
            "\t Train_Loss: 63.1903 Val_Loss: 186.5796  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1167: Validation loss did not decrease\n",
            "\t Train_Loss: 62.7214 Val_Loss: 191.9162  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1168: Validation loss did not decrease\n",
            "\t Train_Loss: 61.4751 Val_Loss: 194.8016  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1169: Validation loss did not decrease\n",
            "\t Train_Loss: 62.0581 Val_Loss: 189.2730  BEST VAL Loss: 185.5942\n",
            "\n",
            "Epoch 1170: Validation loss decreased (185.594177 --> 184.533340).\n",
            "\t Train_Loss: 61.3993 Val_Loss: 184.5333  BEST VAL Loss: 184.5333\n",
            "\n",
            "Epoch 1171: Validation loss decreased (184.533340 --> 182.764557).\n",
            "\t Train_Loss: 61.2731 Val_Loss: 182.7646  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1172: Validation loss did not decrease\n",
            "\t Train_Loss: 61.3746 Val_Loss: 183.5559  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1173: Validation loss did not decrease\n",
            "\t Train_Loss: 61.0840 Val_Loss: 187.1625  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1174: Validation loss did not decrease\n",
            "\t Train_Loss: 60.5744 Val_Loss: 192.1496  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1175: Validation loss did not decrease\n",
            "\t Train_Loss: 60.7751 Val_Loss: 190.9661  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1176: Validation loss did not decrease\n",
            "\t Train_Loss: 60.3078 Val_Loss: 185.6322  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1177: Validation loss did not decrease\n",
            "\t Train_Loss: 60.0213 Val_Loss: 183.2282  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1178: Validation loss did not decrease\n",
            "\t Train_Loss: 60.1934 Val_Loss: 184.2961  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1179: Validation loss did not decrease\n",
            "\t Train_Loss: 59.7568 Val_Loss: 186.8947  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1180: Validation loss did not decrease\n",
            "\t Train_Loss: 59.7197 Val_Loss: 186.2462  BEST VAL Loss: 182.7646\n",
            "\n",
            "Epoch 1181: Validation loss decreased (182.764557 --> 182.438995).\n",
            "\t Train_Loss: 59.6380 Val_Loss: 182.4390  BEST VAL Loss: 182.4390\n",
            "\n",
            "Epoch 1182: Validation loss decreased (182.438995 --> 180.005280).\n",
            "\t Train_Loss: 59.2251 Val_Loss: 180.0053  BEST VAL Loss: 180.0053\n",
            "\n",
            "Epoch 1183: Validation loss did not decrease\n",
            "\t Train_Loss: 59.2314 Val_Loss: 180.2726  BEST VAL Loss: 180.0053\n",
            "\n",
            "Epoch 1184: Validation loss did not decrease\n",
            "\t Train_Loss: 59.0819 Val_Loss: 182.7450  BEST VAL Loss: 180.0053\n",
            "\n",
            "Epoch 1185: Validation loss did not decrease\n",
            "\t Train_Loss: 58.8227 Val_Loss: 185.0861  BEST VAL Loss: 180.0053\n",
            "\n",
            "Epoch 1186: Validation loss did not decrease\n",
            "\t Train_Loss: 58.7983 Val_Loss: 184.2032  BEST VAL Loss: 180.0053\n",
            "\n",
            "Epoch 1187: Validation loss did not decrease\n",
            "\t Train_Loss: 58.5639 Val_Loss: 181.5377  BEST VAL Loss: 180.0053\n",
            "\n",
            "Epoch 1188: Validation loss decreased (180.005280 --> 179.949463).\n",
            "\t Train_Loss: 58.4148 Val_Loss: 179.9495  BEST VAL Loss: 179.9495\n",
            "\n",
            "Epoch 1189: Validation loss decreased (179.949463 --> 178.472137).\n",
            "\t Train_Loss: 58.3407 Val_Loss: 178.4721  BEST VAL Loss: 178.4721\n",
            "\n",
            "Epoch 1190: Validation loss decreased (178.472137 --> 176.379898).\n",
            "\t Train_Loss: 57.5908 Val_Loss: 176.3799  BEST VAL Loss: 176.3799\n",
            "\n",
            "Epoch 1191: Validation loss did not decrease\n",
            "\t Train_Loss: 59.4029 Val_Loss: 186.8906  BEST VAL Loss: 176.3799\n",
            "\n",
            "Epoch 1192: Validation loss did not decrease\n",
            "\t Train_Loss: 58.4018 Val_Loss: 183.6265  BEST VAL Loss: 176.3799\n",
            "\n",
            "Epoch 1193: Validation loss decreased (176.379898 --> 173.639938).\n",
            "\t Train_Loss: 58.1679 Val_Loss: 173.6399  BEST VAL Loss: 173.6399\n",
            "\n",
            "Epoch 1194: Validation loss decreased (173.639938 --> 168.864975).\n",
            "\t Train_Loss: 56.9484 Val_Loss: 168.8650  BEST VAL Loss: 168.8650\n",
            "\n",
            "Epoch 1195: Validation loss did not decrease\n",
            "\t Train_Loss: 57.9437 Val_Loss: 169.8031  BEST VAL Loss: 168.8650\n",
            "\n",
            "Epoch 1196: Validation loss did not decrease\n",
            "\t Train_Loss: 56.4371 Val_Loss: 176.4983  BEST VAL Loss: 168.8650\n",
            "\n",
            "Epoch 1197: Validation loss did not decrease\n",
            "\t Train_Loss: 55.3009 Val_Loss: 180.2286  BEST VAL Loss: 168.8650\n",
            "\n",
            "Epoch 1198: Validation loss did not decrease\n",
            "\t Train_Loss: 55.4918 Val_Loss: 174.2509  BEST VAL Loss: 168.8650\n",
            "\n",
            "Epoch 1199: Validation loss did not decrease\n",
            "\t Train_Loss: 54.6753 Val_Loss: 170.9532  BEST VAL Loss: 168.8650\n",
            "\n",
            "Epoch 1200: Validation loss did not decrease\n",
            "\t Train_Loss: 54.0249 Val_Loss: 172.9030  BEST VAL Loss: 168.8650\n",
            "\n",
            "Epoch 1201: Validation loss did not decrease\n",
            "\t Train_Loss: 53.2489 Val_Loss: 172.6151  BEST VAL Loss: 168.8650\n",
            "\n",
            "Epoch 1202: Validation loss decreased (168.864975 --> 168.685837).\n",
            "\t Train_Loss: 53.3392 Val_Loss: 168.6858  BEST VAL Loss: 168.6858\n",
            "\n",
            "Epoch 1203: Validation loss did not decrease\n",
            "\t Train_Loss: 52.7727 Val_Loss: 170.2622  BEST VAL Loss: 168.6858\n",
            "\n",
            "Epoch 1204: Validation loss did not decrease\n",
            "\t Train_Loss: 52.1986 Val_Loss: 175.4961  BEST VAL Loss: 168.6858\n",
            "\n",
            "Epoch 1205: Validation loss did not decrease\n",
            "\t Train_Loss: 51.9255 Val_Loss: 174.6213  BEST VAL Loss: 168.6858\n",
            "\n",
            "Epoch 1206: Validation loss did not decrease\n",
            "\t Train_Loss: 51.6651 Val_Loss: 169.3270  BEST VAL Loss: 168.6858\n",
            "\n",
            "Epoch 1207: Validation loss decreased (168.685837 --> 167.357407).\n",
            "\t Train_Loss: 51.2242 Val_Loss: 167.3574  BEST VAL Loss: 167.3574\n",
            "\n",
            "Epoch 1208: Validation loss did not decrease\n",
            "\t Train_Loss: 50.9450 Val_Loss: 167.9090  BEST VAL Loss: 167.3574\n",
            "\n",
            "Epoch 1209: Validation loss decreased (167.357407 --> 166.697098).\n",
            "\t Train_Loss: 51.0886 Val_Loss: 166.6971  BEST VAL Loss: 166.6971\n",
            "\n",
            "Epoch 1210: Validation loss did not decrease\n",
            "\t Train_Loss: 50.4716 Val_Loss: 166.9084  BEST VAL Loss: 166.6971\n",
            "\n",
            "Epoch 1211: Validation loss did not decrease\n",
            "\t Train_Loss: 50.2388 Val_Loss: 169.4531  BEST VAL Loss: 166.6971\n",
            "\n",
            "Epoch 1212: Validation loss did not decrease\n",
            "\t Train_Loss: 50.0412 Val_Loss: 171.3141  BEST VAL Loss: 166.6971\n",
            "\n",
            "Epoch 1213: Validation loss did not decrease\n",
            "\t Train_Loss: 50.1831 Val_Loss: 168.7655  BEST VAL Loss: 166.6971\n",
            "\n",
            "Epoch 1214: Validation loss decreased (166.697098 --> 165.481888).\n",
            "\t Train_Loss: 49.8052 Val_Loss: 165.4819  BEST VAL Loss: 165.4819\n",
            "\n",
            "Epoch 1215: Validation loss decreased (165.481888 --> 164.731277).\n",
            "\t Train_Loss: 49.6172 Val_Loss: 164.7313  BEST VAL Loss: 164.7313\n",
            "\n",
            "Epoch 1216: Validation loss did not decrease\n",
            "\t Train_Loss: 49.4296 Val_Loss: 164.7946  BEST VAL Loss: 164.7313\n",
            "\n",
            "Epoch 1217: Validation loss decreased (164.731277 --> 163.327713).\n",
            "\t Train_Loss: 49.3906 Val_Loss: 163.3277  BEST VAL Loss: 163.3277\n",
            "\n",
            "Epoch 1218: Validation loss did not decrease\n",
            "\t Train_Loss: 49.0651 Val_Loss: 164.8536  BEST VAL Loss: 163.3277\n",
            "\n",
            "Epoch 1219: Validation loss did not decrease\n",
            "\t Train_Loss: 48.8730 Val_Loss: 166.9138  BEST VAL Loss: 163.3277\n",
            "\n",
            "Epoch 1220: Validation loss did not decrease\n",
            "\t Train_Loss: 48.7993 Val_Loss: 166.0691  BEST VAL Loss: 163.3277\n",
            "\n",
            "Epoch 1221: Validation loss decreased (163.327713 --> 163.326889).\n",
            "\t Train_Loss: 48.5592 Val_Loss: 163.3269  BEST VAL Loss: 163.3269\n",
            "\n",
            "Epoch 1222: Validation loss decreased (163.326889 --> 161.719162).\n",
            "\t Train_Loss: 48.2546 Val_Loss: 161.7192  BEST VAL Loss: 161.7192\n",
            "\n",
            "Epoch 1223: Validation loss did not decrease\n",
            "\t Train_Loss: 48.1620 Val_Loss: 161.9679  BEST VAL Loss: 161.7192\n",
            "\n",
            "Epoch 1224: Validation loss did not decrease\n",
            "\t Train_Loss: 48.0607 Val_Loss: 162.1705  BEST VAL Loss: 161.7192\n",
            "\n",
            "Epoch 1225: Validation loss did not decrease\n",
            "\t Train_Loss: 47.7362 Val_Loss: 163.0281  BEST VAL Loss: 161.7192\n",
            "\n",
            "Epoch 1226: Validation loss did not decrease\n",
            "\t Train_Loss: 47.6135 Val_Loss: 163.6586  BEST VAL Loss: 161.7192\n",
            "\n",
            "Epoch 1227: Validation loss did not decrease\n",
            "\t Train_Loss: 47.4562 Val_Loss: 162.9469  BEST VAL Loss: 161.7192\n",
            "\n",
            "Epoch 1228: Validation loss decreased (161.719162 --> 160.812134).\n",
            "\t Train_Loss: 47.2801 Val_Loss: 160.8121  BEST VAL Loss: 160.8121\n",
            "\n",
            "Epoch 1229: Validation loss decreased (160.812134 --> 159.197998).\n",
            "\t Train_Loss: 47.0255 Val_Loss: 159.1980  BEST VAL Loss: 159.1980\n",
            "\n",
            "Epoch 1230: Validation loss decreased (159.197998 --> 159.152847).\n",
            "\t Train_Loss: 46.9527 Val_Loss: 159.1528  BEST VAL Loss: 159.1528\n",
            "\n",
            "Epoch 1231: Validation loss did not decrease\n",
            "\t Train_Loss: 46.7453 Val_Loss: 160.0923  BEST VAL Loss: 159.1528\n",
            "\n",
            "Epoch 1232: Validation loss did not decrease\n",
            "\t Train_Loss: 46.5680 Val_Loss: 160.2479  BEST VAL Loss: 159.1528\n",
            "\n",
            "Epoch 1233: Validation loss did not decrease\n",
            "\t Train_Loss: 46.4293 Val_Loss: 159.9550  BEST VAL Loss: 159.1528\n",
            "\n",
            "Epoch 1234: Validation loss did not decrease\n",
            "\t Train_Loss: 46.2986 Val_Loss: 159.6102  BEST VAL Loss: 159.1528\n",
            "\n",
            "Epoch 1235: Validation loss decreased (159.152847 --> 158.261826).\n",
            "\t Train_Loss: 46.0753 Val_Loss: 158.2618  BEST VAL Loss: 158.2618\n",
            "\n",
            "Epoch 1236: Validation loss decreased (158.261826 --> 158.023087).\n",
            "\t Train_Loss: 45.8107 Val_Loss: 158.0231  BEST VAL Loss: 158.0231\n",
            "\n",
            "Epoch 1237: Validation loss did not decrease\n",
            "\t Train_Loss: 45.0329 Val_Loss: 185.0299  BEST VAL Loss: 158.0231\n",
            "\n",
            "Epoch 1238: Validation loss did not decrease\n",
            "\t Train_Loss: 52.1489 Val_Loss: 163.5542  BEST VAL Loss: 158.0231\n",
            "\n",
            "Epoch 1239: Validation loss decreased (158.023087 --> 152.038788).\n",
            "\t Train_Loss: 52.2289 Val_Loss: 152.0388  BEST VAL Loss: 152.0388\n",
            "\n",
            "Epoch 1240: Validation loss decreased (152.038788 --> 150.270004).\n",
            "\t Train_Loss: 51.2340 Val_Loss: 150.2700  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1241: Validation loss did not decrease\n",
            "\t Train_Loss: 55.6110 Val_Loss: 150.5145  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1242: Validation loss did not decrease\n",
            "\t Train_Loss: 50.8036 Val_Loss: 154.3569  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1243: Validation loss did not decrease\n",
            "\t Train_Loss: 54.4693 Val_Loss: 154.9619  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1244: Validation loss did not decrease\n",
            "\t Train_Loss: 47.6058 Val_Loss: 165.9046  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1245: Validation loss did not decrease\n",
            "\t Train_Loss: 51.8997 Val_Loss: 164.3869  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1246: Validation loss did not decrease\n",
            "\t Train_Loss: 49.0522 Val_Loss: 156.7569  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1247: Validation loss did not decrease\n",
            "\t Train_Loss: 45.7490 Val_Loss: 158.4088  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1248: Validation loss did not decrease\n",
            "\t Train_Loss: 49.0083 Val_Loss: 155.2635  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1249: Validation loss did not decrease\n",
            "\t Train_Loss: 48.1139 Val_Loss: 155.7935  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1250: Validation loss did not decrease\n",
            "\t Train_Loss: 45.8314 Val_Loss: 162.0918  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1251: Validation loss did not decrease\n",
            "\t Train_Loss: 47.5245 Val_Loss: 160.8302  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1252: Validation loss did not decrease\n",
            "\t Train_Loss: 47.3309 Val_Loss: 153.0956  BEST VAL Loss: 150.2700\n",
            "\n",
            "Epoch 1253: Validation loss decreased (150.270004 --> 149.686996).\n",
            "\t Train_Loss: 45.7478 Val_Loss: 149.6870  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1254: Validation loss did not decrease\n",
            "\t Train_Loss: 45.1832 Val_Loss: 149.8331  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1255: Validation loss did not decrease\n",
            "\t Train_Loss: 45.5680 Val_Loss: 151.9458  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1256: Validation loss did not decrease\n",
            "\t Train_Loss: 46.0784 Val_Loss: 153.4010  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1257: Validation loss did not decrease\n",
            "\t Train_Loss: 46.0095 Val_Loss: 154.1649  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1258: Validation loss did not decrease\n",
            "\t Train_Loss: 45.0505 Val_Loss: 156.2956  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1259: Validation loss did not decrease\n",
            "\t Train_Loss: 44.3415 Val_Loss: 158.3719  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1260: Validation loss did not decrease\n",
            "\t Train_Loss: 44.4300 Val_Loss: 156.6929  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1261: Validation loss did not decrease\n",
            "\t Train_Loss: 44.3822 Val_Loss: 152.3863  BEST VAL Loss: 149.6870\n",
            "\n",
            "Epoch 1262: Validation loss decreased (149.686996 --> 148.605469).\n",
            "\t Train_Loss: 44.3408 Val_Loss: 148.6055  BEST VAL Loss: 148.6055\n",
            "\n",
            "Epoch 1263: Validation loss decreased (148.605469 --> 146.933517).\n",
            "\t Train_Loss: 44.3684 Val_Loss: 146.9335  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1264: Validation loss did not decrease\n",
            "\t Train_Loss: 44.3179 Val_Loss: 147.1096  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1265: Validation loss did not decrease\n",
            "\t Train_Loss: 43.7998 Val_Loss: 148.5987  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1266: Validation loss did not decrease\n",
            "\t Train_Loss: 43.4563 Val_Loss: 150.4987  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1267: Validation loss did not decrease\n",
            "\t Train_Loss: 43.4131 Val_Loss: 151.8296  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1268: Validation loss did not decrease\n",
            "\t Train_Loss: 43.4513 Val_Loss: 151.5811  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1269: Validation loss did not decrease\n",
            "\t Train_Loss: 43.2521 Val_Loss: 150.5776  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1270: Validation loss did not decrease\n",
            "\t Train_Loss: 43.1619 Val_Loss: 149.6458  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1271: Validation loss did not decrease\n",
            "\t Train_Loss: 43.0893 Val_Loss: 149.4587  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1272: Validation loss did not decrease\n",
            "\t Train_Loss: 42.9205 Val_Loss: 150.1673  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1273: Validation loss did not decrease\n",
            "\t Train_Loss: 42.6730 Val_Loss: 150.8346  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1274: Validation loss did not decrease\n",
            "\t Train_Loss: 42.5750 Val_Loss: 150.1761  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1275: Validation loss did not decrease\n",
            "\t Train_Loss: 42.5779 Val_Loss: 148.1654  BEST VAL Loss: 146.9335\n",
            "\n",
            "Epoch 1276: Validation loss decreased (146.933517 --> 146.179977).\n",
            "\t Train_Loss: 42.3402 Val_Loss: 146.1800  BEST VAL Loss: 146.1800\n",
            "\n",
            "Epoch 1277: Validation loss decreased (146.179977 --> 145.406433).\n",
            "\t Train_Loss: 42.2543 Val_Loss: 145.4064  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1278: Validation loss did not decrease\n",
            "\t Train_Loss: 42.2088 Val_Loss: 145.7021  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1279: Validation loss did not decrease\n",
            "\t Train_Loss: 42.1220 Val_Loss: 146.5708  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1280: Validation loss did not decrease\n",
            "\t Train_Loss: 41.8987 Val_Loss: 147.9504  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1281: Validation loss did not decrease\n",
            "\t Train_Loss: 41.8547 Val_Loss: 148.9650  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1282: Validation loss did not decrease\n",
            "\t Train_Loss: 41.7927 Val_Loss: 148.7202  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1283: Validation loss did not decrease\n",
            "\t Train_Loss: 41.6816 Val_Loss: 147.5533  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1284: Validation loss did not decrease\n",
            "\t Train_Loss: 41.5678 Val_Loss: 146.3762  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1285: Validation loss did not decrease\n",
            "\t Train_Loss: 41.4944 Val_Loss: 145.7289  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1286: Validation loss did not decrease\n",
            "\t Train_Loss: 41.4233 Val_Loss: 145.5274  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1287: Validation loss did not decrease\n",
            "\t Train_Loss: 41.2706 Val_Loss: 145.5822  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1288: Validation loss did not decrease\n",
            "\t Train_Loss: 41.2195 Val_Loss: 145.5947  BEST VAL Loss: 145.4064\n",
            "\n",
            "Epoch 1289: Validation loss decreased (145.406433 --> 145.236313).\n",
            "\t Train_Loss: 41.1589 Val_Loss: 145.2363  BEST VAL Loss: 145.2363\n",
            "\n",
            "Epoch 1290: Validation loss decreased (145.236313 --> 144.683640).\n",
            "\t Train_Loss: 41.0595 Val_Loss: 144.6836  BEST VAL Loss: 144.6836\n",
            "\n",
            "Epoch 1291: Validation loss decreased (144.683640 --> 144.501968).\n",
            "\t Train_Loss: 40.9625 Val_Loss: 144.5020  BEST VAL Loss: 144.5020\n",
            "\n",
            "Epoch 1292: Validation loss did not decrease\n",
            "\t Train_Loss: 40.8838 Val_Loss: 144.7085  BEST VAL Loss: 144.5020\n",
            "\n",
            "Epoch 1293: Validation loss did not decrease\n",
            "\t Train_Loss: 40.8191 Val_Loss: 145.1044  BEST VAL Loss: 144.5020\n",
            "\n",
            "Epoch 1294: Validation loss did not decrease\n",
            "\t Train_Loss: 40.7261 Val_Loss: 145.5416  BEST VAL Loss: 144.5020\n",
            "\n",
            "Epoch 1295: Validation loss did not decrease\n",
            "\t Train_Loss: 40.6541 Val_Loss: 145.5377  BEST VAL Loss: 144.5020\n",
            "\n",
            "Epoch 1296: Validation loss did not decrease\n",
            "\t Train_Loss: 40.5970 Val_Loss: 144.9169  BEST VAL Loss: 144.5020\n",
            "\n",
            "Epoch 1297: Validation loss decreased (144.501968 --> 143.998901).\n",
            "\t Train_Loss: 40.4980 Val_Loss: 143.9989  BEST VAL Loss: 143.9989\n",
            "\n",
            "Epoch 1298: Validation loss decreased (143.998901 --> 143.236023).\n",
            "\t Train_Loss: 40.4119 Val_Loss: 143.2360  BEST VAL Loss: 143.2360\n",
            "\n",
            "Epoch 1299: Validation loss decreased (143.236023 --> 142.754639).\n",
            "\t Train_Loss: 40.3517 Val_Loss: 142.7546  BEST VAL Loss: 142.7546\n",
            "\n",
            "Epoch 1300: Validation loss decreased (142.754639 --> 142.677597).\n",
            "\t Train_Loss: 40.2795 Val_Loss: 142.6776  BEST VAL Loss: 142.6776\n",
            "\n",
            "Epoch 1301: Validation loss did not decrease\n",
            "\t Train_Loss: 40.2065 Val_Loss: 142.8771  BEST VAL Loss: 142.6776\n",
            "\n",
            "Epoch 1302: Validation loss did not decrease\n",
            "\t Train_Loss: 40.1352 Val_Loss: 142.9946  BEST VAL Loss: 142.6776\n",
            "\n",
            "Epoch 1303: Validation loss did not decrease\n",
            "\t Train_Loss: 40.0588 Val_Loss: 142.9527  BEST VAL Loss: 142.6776\n",
            "\n",
            "Epoch 1304: Validation loss did not decrease\n",
            "\t Train_Loss: 39.9838 Val_Loss: 142.7853  BEST VAL Loss: 142.6776\n",
            "\n",
            "Epoch 1305: Validation loss decreased (142.677597 --> 142.544495).\n",
            "\t Train_Loss: 39.9130 Val_Loss: 142.5445  BEST VAL Loss: 142.5445\n",
            "\n",
            "Epoch 1306: Validation loss decreased (142.544495 --> 142.382370).\n",
            "\t Train_Loss: 39.8450 Val_Loss: 142.3824  BEST VAL Loss: 142.3824\n",
            "\n",
            "Epoch 1307: Validation loss decreased (142.382370 --> 142.321213).\n",
            "\t Train_Loss: 39.7802 Val_Loss: 142.3212  BEST VAL Loss: 142.3212\n",
            "\n",
            "Epoch 1308: Validation loss decreased (142.321213 --> 142.218872).\n",
            "\t Train_Loss: 39.7092 Val_Loss: 142.2189  BEST VAL Loss: 142.2189\n",
            "\n",
            "Epoch 1309: Validation loss decreased (142.218872 --> 142.009766).\n",
            "\t Train_Loss: 39.6343 Val_Loss: 142.0098  BEST VAL Loss: 142.0098\n",
            "\n",
            "Epoch 1310: Validation loss decreased (142.009766 --> 141.658981).\n",
            "\t Train_Loss: 39.5657 Val_Loss: 141.6590  BEST VAL Loss: 141.6590\n",
            "\n",
            "Epoch 1311: Validation loss decreased (141.658981 --> 141.183578).\n",
            "\t Train_Loss: 39.4978 Val_Loss: 141.1836  BEST VAL Loss: 141.1836\n",
            "\n",
            "Epoch 1312: Validation loss decreased (141.183578 --> 140.768723).\n",
            "\t Train_Loss: 39.4274 Val_Loss: 140.7687  BEST VAL Loss: 140.7687\n",
            "\n",
            "Epoch 1313: Validation loss decreased (140.768723 --> 140.523483).\n",
            "\t Train_Loss: 39.3618 Val_Loss: 140.5235  BEST VAL Loss: 140.5235\n",
            "\n",
            "Epoch 1314: Validation loss decreased (140.523483 --> 140.411697).\n",
            "\t Train_Loss: 39.2975 Val_Loss: 140.4117  BEST VAL Loss: 140.4117\n",
            "\n",
            "Epoch 1315: Validation loss did not decrease\n",
            "\t Train_Loss: 39.2277 Val_Loss: 140.4416  BEST VAL Loss: 140.4117\n",
            "\n",
            "Epoch 1316: Validation loss did not decrease\n",
            "\t Train_Loss: 39.1579 Val_Loss: 140.4731  BEST VAL Loss: 140.4117\n",
            "\n",
            "Epoch 1317: Validation loss decreased (140.411697 --> 140.387466).\n",
            "\t Train_Loss: 39.0923 Val_Loss: 140.3875  BEST VAL Loss: 140.3875\n",
            "\n",
            "Epoch 1318: Validation loss decreased (140.387466 --> 140.209961).\n",
            "\t Train_Loss: 39.0260 Val_Loss: 140.2100  BEST VAL Loss: 140.2100\n",
            "\n",
            "Epoch 1319: Validation loss decreased (140.209961 --> 139.967407).\n",
            "\t Train_Loss: 38.9586 Val_Loss: 139.9674  BEST VAL Loss: 139.9674\n",
            "\n",
            "Epoch 1320: Validation loss decreased (139.967407 --> 139.722702).\n",
            "\t Train_Loss: 38.8936 Val_Loss: 139.7227  BEST VAL Loss: 139.7227\n",
            "\n",
            "Epoch 1321: Validation loss decreased (139.722702 --> 139.545700).\n",
            "\t Train_Loss: 38.8291 Val_Loss: 139.5457  BEST VAL Loss: 139.5457\n",
            "\n",
            "Epoch 1322: Validation loss decreased (139.545700 --> 139.391052).\n",
            "\t Train_Loss: 38.7623 Val_Loss: 139.3911  BEST VAL Loss: 139.3911\n",
            "\n",
            "Epoch 1323: Validation loss decreased (139.391052 --> 139.216995).\n",
            "\t Train_Loss: 38.6957 Val_Loss: 139.2170  BEST VAL Loss: 139.2170\n",
            "\n",
            "Epoch 1324: Validation loss decreased (139.216995 --> 139.001404).\n",
            "\t Train_Loss: 38.6309 Val_Loss: 139.0014  BEST VAL Loss: 139.0014\n",
            "\n",
            "Epoch 1325: Validation loss decreased (139.001404 --> 138.719345).\n",
            "\t Train_Loss: 38.5658 Val_Loss: 138.7193  BEST VAL Loss: 138.7193\n",
            "\n",
            "Epoch 1326: Validation loss decreased (138.719345 --> 138.429916).\n",
            "\t Train_Loss: 38.5003 Val_Loss: 138.4299  BEST VAL Loss: 138.4299\n",
            "\n",
            "Epoch 1327: Validation loss decreased (138.429916 --> 138.201355).\n",
            "\t Train_Loss: 38.4359 Val_Loss: 138.2014  BEST VAL Loss: 138.2014\n",
            "\n",
            "Epoch 1328: Validation loss decreased (138.201355 --> 138.036987).\n",
            "\t Train_Loss: 38.3718 Val_Loss: 138.0370  BEST VAL Loss: 138.0370\n",
            "\n",
            "Epoch 1329: Validation loss decreased (138.036987 --> 137.947800).\n",
            "\t Train_Loss: 38.3066 Val_Loss: 137.9478  BEST VAL Loss: 137.9478\n",
            "\n",
            "Epoch 1330: Validation loss decreased (137.947800 --> 137.884308).\n",
            "\t Train_Loss: 38.2413 Val_Loss: 137.8843  BEST VAL Loss: 137.8843\n",
            "\n",
            "Epoch 1331: Validation loss decreased (137.884308 --> 137.780807).\n",
            "\t Train_Loss: 38.1765 Val_Loss: 137.7808  BEST VAL Loss: 137.7808\n",
            "\n",
            "Epoch 1332: Validation loss decreased (137.780807 --> 137.633865).\n",
            "\t Train_Loss: 38.1115 Val_Loss: 137.6339  BEST VAL Loss: 137.6339\n",
            "\n",
            "Epoch 1333: Validation loss decreased (137.633865 --> 137.448517).\n",
            "\t Train_Loss: 38.0459 Val_Loss: 137.4485  BEST VAL Loss: 137.4485\n",
            "\n",
            "Epoch 1334: Validation loss decreased (137.448517 --> 137.252548).\n",
            "\t Train_Loss: 37.9803 Val_Loss: 137.2525  BEST VAL Loss: 137.2525\n",
            "\n",
            "Epoch 1335: Validation loss decreased (137.252548 --> 137.087433).\n",
            "\t Train_Loss: 37.9141 Val_Loss: 137.0874  BEST VAL Loss: 137.0874\n",
            "\n",
            "Epoch 1336: Validation loss decreased (137.087433 --> 136.943878).\n",
            "\t Train_Loss: 37.8466 Val_Loss: 136.9439  BEST VAL Loss: 136.9439\n",
            "\n",
            "Epoch 1337: Validation loss decreased (136.943878 --> 136.802246).\n",
            "\t Train_Loss: 37.7773 Val_Loss: 136.8022  BEST VAL Loss: 136.8022\n",
            "\n",
            "Epoch 1338: Validation loss decreased (136.802246 --> 136.646759).\n",
            "\t Train_Loss: 37.7059 Val_Loss: 136.6468  BEST VAL Loss: 136.6468\n",
            "\n",
            "Epoch 1339: Validation loss decreased (136.646759 --> 136.454208).\n",
            "\t Train_Loss: 37.6304 Val_Loss: 136.4542  BEST VAL Loss: 136.4542\n",
            "\n",
            "Epoch 1340: Validation loss decreased (136.454208 --> 136.242142).\n",
            "\t Train_Loss: 37.5481 Val_Loss: 136.2421  BEST VAL Loss: 136.2421\n",
            "\n",
            "Epoch 1341: Validation loss decreased (136.242142 --> 136.042221).\n",
            "\t Train_Loss: 37.4538 Val_Loss: 136.0422  BEST VAL Loss: 136.0422\n",
            "\n",
            "Epoch 1342: Validation loss decreased (136.042221 --> 135.866562).\n",
            "\t Train_Loss: 37.3354 Val_Loss: 135.8666  BEST VAL Loss: 135.8666\n",
            "\n",
            "Epoch 1343: Validation loss decreased (135.866562 --> 135.729599).\n",
            "\t Train_Loss: 37.1628 Val_Loss: 135.7296  BEST VAL Loss: 135.7296\n",
            "\n",
            "Epoch 1344: Validation loss decreased (135.729599 --> 135.618484).\n",
            "\t Train_Loss: 36.8515 Val_Loss: 135.6185  BEST VAL Loss: 135.6185\n",
            "\n",
            "Epoch 1345: Validation loss decreased (135.618484 --> 135.507477).\n",
            "\t Train_Loss: 36.1376 Val_Loss: 135.5075  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1346: Validation loss did not decrease\n",
            "\t Train_Loss: 34.3566 Val_Loss: 135.6787  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1347: Validation loss did not decrease\n",
            "\t Train_Loss: 31.7597 Val_Loss: 140.3154  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1348: Validation loss did not decrease\n",
            "\t Train_Loss: 31.8305 Val_Loss: 144.2447  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1349: Validation loss did not decrease\n",
            "\t Train_Loss: 32.7707 Val_Loss: 144.5656  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1350: Validation loss did not decrease\n",
            "\t Train_Loss: 33.1293 Val_Loss: 144.1098  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1351: Validation loss did not decrease\n",
            "\t Train_Loss: 33.3366 Val_Loss: 143.5850  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1352: Validation loss did not decrease\n",
            "\t Train_Loss: 33.2915 Val_Loss: 143.2330  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1353: Validation loss did not decrease\n",
            "\t Train_Loss: 33.0171 Val_Loss: 143.0239  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1354: Validation loss did not decrease\n",
            "\t Train_Loss: 32.6700 Val_Loss: 141.1459  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1355: Validation loss did not decrease\n",
            "\t Train_Loss: 32.1644 Val_Loss: 137.3955  BEST VAL Loss: 135.5075\n",
            "\n",
            "Epoch 1356: Validation loss decreased (135.507477 --> 133.174408).\n",
            "\t Train_Loss: 31.3492 Val_Loss: 133.1744  BEST VAL Loss: 133.1744\n",
            "\n",
            "Epoch 1357: Validation loss decreased (133.174408 --> 130.334793).\n",
            "\t Train_Loss: 30.6500 Val_Loss: 130.3348  BEST VAL Loss: 130.3348\n",
            "\n",
            "Epoch 1358: Validation loss decreased (130.334793 --> 129.497742).\n",
            "\t Train_Loss: 30.7711 Val_Loss: 129.4977  BEST VAL Loss: 129.4977\n",
            "\n",
            "Epoch 1359: Validation loss decreased (129.497742 --> 129.426361).\n",
            "\t Train_Loss: 31.1217 Val_Loss: 129.4264  BEST VAL Loss: 129.4264\n",
            "\n",
            "Epoch 1360: Validation loss did not decrease\n",
            "\t Train_Loss: 30.9481 Val_Loss: 130.2269  BEST VAL Loss: 129.4264\n",
            "\n",
            "Epoch 1361: Validation loss did not decrease\n",
            "\t Train_Loss: 30.4089 Val_Loss: 131.7079  BEST VAL Loss: 129.4264\n",
            "\n",
            "Epoch 1362: Validation loss did not decrease\n",
            "\t Train_Loss: 29.9686 Val_Loss: 132.6307  BEST VAL Loss: 129.4264\n",
            "\n",
            "Epoch 1363: Validation loss did not decrease\n",
            "\t Train_Loss: 30.3519 Val_Loss: 132.6612  BEST VAL Loss: 129.4264\n",
            "\n",
            "Epoch 1364: Validation loss did not decrease\n",
            "\t Train_Loss: 30.1634 Val_Loss: 130.9534  BEST VAL Loss: 129.4264\n",
            "\n",
            "Epoch 1365: Validation loss decreased (129.426361 --> 128.520920).\n",
            "\t Train_Loss: 29.8297 Val_Loss: 128.5209  BEST VAL Loss: 128.5209\n",
            "\n",
            "Epoch 1366: Validation loss decreased (128.520920 --> 128.055435).\n",
            "\t Train_Loss: 29.8592 Val_Loss: 128.0554  BEST VAL Loss: 128.0554\n",
            "\n",
            "Epoch 1367: Validation loss did not decrease\n",
            "\t Train_Loss: 29.2837 Val_Loss: 128.1667  BEST VAL Loss: 128.0554\n",
            "\n",
            "Epoch 1368: Validation loss did not decrease\n",
            "\t Train_Loss: 29.4273 Val_Loss: 128.5786  BEST VAL Loss: 128.0554\n",
            "\n",
            "Epoch 1369: Validation loss did not decrease\n",
            "\t Train_Loss: 29.3136 Val_Loss: 128.5425  BEST VAL Loss: 128.0554\n",
            "\n",
            "Epoch 1370: Validation loss decreased (128.055435 --> 127.601982).\n",
            "\t Train_Loss: 29.1718 Val_Loss: 127.6020  BEST VAL Loss: 127.6020\n",
            "\n",
            "Epoch 1371: Validation loss decreased (127.601982 --> 127.239014).\n",
            "\t Train_Loss: 28.7100 Val_Loss: 127.2390  BEST VAL Loss: 127.2390\n",
            "\n",
            "Epoch 1372: Validation loss decreased (127.239014 --> 126.773781).\n",
            "\t Train_Loss: 28.8616 Val_Loss: 126.7738  BEST VAL Loss: 126.7738\n",
            "\n",
            "Epoch 1373: Validation loss decreased (126.773781 --> 126.699028).\n",
            "\t Train_Loss: 28.7208 Val_Loss: 126.6990  BEST VAL Loss: 126.6990\n",
            "\n",
            "Epoch 1374: Validation loss did not decrease\n",
            "\t Train_Loss: 28.6518 Val_Loss: 127.0859  BEST VAL Loss: 126.6990\n",
            "\n",
            "Epoch 1375: Validation loss did not decrease\n",
            "\t Train_Loss: 28.3285 Val_Loss: 127.0211  BEST VAL Loss: 126.6990\n",
            "\n",
            "Epoch 1376: Validation loss did not decrease\n",
            "\t Train_Loss: 28.2594 Val_Loss: 126.9036  BEST VAL Loss: 126.6990\n",
            "\n",
            "Epoch 1377: Validation loss decreased (126.699028 --> 126.024956).\n",
            "\t Train_Loss: 28.2481 Val_Loss: 126.0250  BEST VAL Loss: 126.0250\n",
            "\n",
            "Epoch 1378: Validation loss decreased (126.024956 --> 124.678368).\n",
            "\t Train_Loss: 28.1556 Val_Loss: 124.6784  BEST VAL Loss: 124.6784\n",
            "\n",
            "Epoch 1379: Validation loss decreased (124.678368 --> 123.944435).\n",
            "\t Train_Loss: 27.9583 Val_Loss: 123.9444  BEST VAL Loss: 123.9444\n",
            "\n",
            "Epoch 1380: Validation loss decreased (123.944435 --> 123.650444).\n",
            "\t Train_Loss: 27.8470 Val_Loss: 123.6504  BEST VAL Loss: 123.6504\n",
            "\n",
            "Epoch 1381: Validation loss did not decrease\n",
            "\t Train_Loss: 27.7703 Val_Loss: 123.6943  BEST VAL Loss: 123.6504\n",
            "\n",
            "Epoch 1382: Validation loss did not decrease\n",
            "\t Train_Loss: 27.7413 Val_Loss: 123.8569  BEST VAL Loss: 123.6504\n",
            "\n",
            "Epoch 1383: Validation loss did not decrease\n",
            "\t Train_Loss: 27.6347 Val_Loss: 123.7435  BEST VAL Loss: 123.6504\n",
            "\n",
            "Epoch 1384: Validation loss decreased (123.650444 --> 123.492393).\n",
            "\t Train_Loss: 27.4809 Val_Loss: 123.4924  BEST VAL Loss: 123.4924\n",
            "\n",
            "Epoch 1385: Validation loss decreased (123.492393 --> 123.122429).\n",
            "\t Train_Loss: 27.3341 Val_Loss: 123.1224  BEST VAL Loss: 123.1224\n",
            "\n",
            "Epoch 1386: Validation loss decreased (123.122429 --> 122.410088).\n",
            "\t Train_Loss: 27.6367 Val_Loss: 122.4101  BEST VAL Loss: 122.4101\n",
            "\n",
            "Epoch 1387: Validation loss decreased (122.410088 --> 121.740356).\n",
            "\t Train_Loss: 27.5503 Val_Loss: 121.7404  BEST VAL Loss: 121.7404\n",
            "\n",
            "Epoch 1388: Validation loss did not decrease\n",
            "\t Train_Loss: 27.2285 Val_Loss: 121.8999  BEST VAL Loss: 121.7404\n",
            "\n",
            "Epoch 1389: Validation loss did not decrease\n",
            "\t Train_Loss: 27.3407 Val_Loss: 122.2744  BEST VAL Loss: 121.7404\n",
            "\n",
            "Epoch 1390: Validation loss did not decrease\n",
            "\t Train_Loss: 26.9904 Val_Loss: 122.4591  BEST VAL Loss: 121.7404\n",
            "\n",
            "Epoch 1391: Validation loss did not decrease\n",
            "\t Train_Loss: 27.2464 Val_Loss: 121.9111  BEST VAL Loss: 121.7404\n",
            "\n",
            "Epoch 1392: Validation loss decreased (121.740356 --> 121.118652).\n",
            "\t Train_Loss: 26.8512 Val_Loss: 121.1187  BEST VAL Loss: 121.1187\n",
            "\n",
            "Epoch 1393: Validation loss decreased (121.118652 --> 120.550247).\n",
            "\t Train_Loss: 26.9104 Val_Loss: 120.5502  BEST VAL Loss: 120.5502\n",
            "\n",
            "Epoch 1394: Validation loss decreased (120.550247 --> 120.414894).\n",
            "\t Train_Loss: 26.7094 Val_Loss: 120.4149  BEST VAL Loss: 120.4149\n",
            "\n",
            "Epoch 1395: Validation loss decreased (120.414894 --> 120.334671).\n",
            "\t Train_Loss: 26.8039 Val_Loss: 120.3347  BEST VAL Loss: 120.3347\n",
            "\n",
            "Epoch 1396: Validation loss did not decrease\n",
            "\t Train_Loss: 26.5295 Val_Loss: 120.4377  BEST VAL Loss: 120.3347\n",
            "\n",
            "Epoch 1397: Validation loss did not decrease\n",
            "\t Train_Loss: 26.5615 Val_Loss: 120.6106  BEST VAL Loss: 120.3347\n",
            "\n",
            "Epoch 1398: Validation loss decreased (120.334671 --> 120.260643).\n",
            "\t Train_Loss: 26.4151 Val_Loss: 120.2606  BEST VAL Loss: 120.2606\n",
            "\n",
            "Epoch 1399: Validation loss decreased (120.260643 --> 119.493462).\n",
            "\t Train_Loss: 26.4184 Val_Loss: 119.4935  BEST VAL Loss: 119.4935\n",
            "\n",
            "Epoch 1400: Validation loss decreased (119.493462 --> 118.896805).\n",
            "\t Train_Loss: 26.2366 Val_Loss: 118.8968  BEST VAL Loss: 118.8968\n",
            "\n",
            "Epoch 1401: Validation loss decreased (118.896805 --> 118.642876).\n",
            "\t Train_Loss: 26.2351 Val_Loss: 118.6429  BEST VAL Loss: 118.6429\n",
            "\n",
            "Epoch 1402: Validation loss decreased (118.642876 --> 118.642387).\n",
            "\t Train_Loss: 26.1275 Val_Loss: 118.6424  BEST VAL Loss: 118.6424\n",
            "\n",
            "Epoch 1403: Validation loss decreased (118.642387 --> 118.563721).\n",
            "\t Train_Loss: 26.0851 Val_Loss: 118.5637  BEST VAL Loss: 118.5637\n",
            "\n",
            "Epoch 1404: Validation loss decreased (118.563721 --> 118.428749).\n",
            "\t Train_Loss: 25.9638 Val_Loss: 118.4287  BEST VAL Loss: 118.4287\n",
            "\n",
            "Epoch 1405: Validation loss decreased (118.428749 --> 118.220848).\n",
            "\t Train_Loss: 25.9445 Val_Loss: 118.2208  BEST VAL Loss: 118.2208\n",
            "\n",
            "Epoch 1406: Validation loss decreased (118.220848 --> 117.729958).\n",
            "\t Train_Loss: 25.8458 Val_Loss: 117.7300  BEST VAL Loss: 117.7300\n",
            "\n",
            "Epoch 1407: Validation loss decreased (117.729958 --> 117.147926).\n",
            "\t Train_Loss: 25.7993 Val_Loss: 117.1479  BEST VAL Loss: 117.1479\n",
            "\n",
            "Epoch 1408: Validation loss decreased (117.147926 --> 116.786537).\n",
            "\t Train_Loss: 25.7030 Val_Loss: 116.7865  BEST VAL Loss: 116.7865\n",
            "\n",
            "Epoch 1409: Validation loss decreased (116.786537 --> 116.734085).\n",
            "\t Train_Loss: 25.6737 Val_Loss: 116.7341  BEST VAL Loss: 116.7341\n",
            "\n",
            "Epoch 1410: Validation loss did not decrease\n",
            "\t Train_Loss: 25.5829 Val_Loss: 116.7842  BEST VAL Loss: 116.7341\n",
            "\n",
            "Epoch 1411: Validation loss decreased (116.734085 --> 116.665703).\n",
            "\t Train_Loss: 25.5327 Val_Loss: 116.6657  BEST VAL Loss: 116.6657\n",
            "\n",
            "Epoch 1412: Validation loss decreased (116.665703 --> 116.435387).\n",
            "\t Train_Loss: 25.4571 Val_Loss: 116.4354  BEST VAL Loss: 116.4354\n",
            "\n",
            "Epoch 1413: Validation loss decreased (116.435387 --> 116.155052).\n",
            "\t Train_Loss: 25.4156 Val_Loss: 116.1551  BEST VAL Loss: 116.1551\n",
            "\n",
            "Epoch 1414: Validation loss decreased (116.155052 --> 115.744728).\n",
            "\t Train_Loss: 25.3340 Val_Loss: 115.7447  BEST VAL Loss: 115.7447\n",
            "\n",
            "Epoch 1415: Validation loss decreased (115.744728 --> 115.316467).\n",
            "\t Train_Loss: 25.2843 Val_Loss: 115.3165  BEST VAL Loss: 115.3165\n",
            "\n",
            "Epoch 1416: Validation loss decreased (115.316467 --> 115.084473).\n",
            "\t Train_Loss: 25.2192 Val_Loss: 115.0845  BEST VAL Loss: 115.0845\n",
            "\n",
            "Epoch 1417: Validation loss decreased (115.084473 --> 115.061867).\n",
            "\t Train_Loss: 25.1681 Val_Loss: 115.0619  BEST VAL Loss: 115.0619\n",
            "\n",
            "Epoch 1418: Validation loss decreased (115.061867 --> 114.986671).\n",
            "\t Train_Loss: 25.1014 Val_Loss: 114.9867  BEST VAL Loss: 114.9867\n",
            "\n",
            "Epoch 1419: Validation loss decreased (114.986671 --> 114.750198).\n",
            "\t Train_Loss: 25.0446 Val_Loss: 114.7502  BEST VAL Loss: 114.7502\n",
            "\n",
            "Epoch 1420: Validation loss decreased (114.750198 --> 114.482048).\n",
            "\t Train_Loss: 24.9923 Val_Loss: 114.4820  BEST VAL Loss: 114.4820\n",
            "\n",
            "Epoch 1421: Validation loss decreased (114.482048 --> 114.194679).\n",
            "\t Train_Loss: 24.9315 Val_Loss: 114.1947  BEST VAL Loss: 114.1947\n",
            "\n",
            "Epoch 1422: Validation loss decreased (114.194679 --> 113.847816).\n",
            "\t Train_Loss: 24.8757 Val_Loss: 113.8478  BEST VAL Loss: 113.8478\n",
            "\n",
            "Epoch 1423: Validation loss decreased (113.847816 --> 113.556084).\n",
            "\t Train_Loss: 24.8167 Val_Loss: 113.5561  BEST VAL Loss: 113.5561\n",
            "\n",
            "Epoch 1424: Validation loss decreased (113.556084 --> 113.444191).\n",
            "\t Train_Loss: 24.7701 Val_Loss: 113.4442  BEST VAL Loss: 113.4442\n",
            "\n",
            "Epoch 1425: Validation loss decreased (113.444191 --> 113.384277).\n",
            "\t Train_Loss: 24.7051 Val_Loss: 113.3843  BEST VAL Loss: 113.3843\n",
            "\n",
            "Epoch 1426: Validation loss decreased (113.384277 --> 113.200584).\n",
            "\t Train_Loss: 24.6562 Val_Loss: 113.2006  BEST VAL Loss: 113.2006\n",
            "\n",
            "Epoch 1427: Validation loss decreased (113.200584 --> 112.960976).\n",
            "\t Train_Loss: 24.5979 Val_Loss: 112.9610  BEST VAL Loss: 112.9610\n",
            "\n",
            "Epoch 1428: Validation loss decreased (112.960976 --> 112.737389).\n",
            "\t Train_Loss: 24.5491 Val_Loss: 112.7374  BEST VAL Loss: 112.7374\n",
            "\n",
            "Epoch 1429: Validation loss decreased (112.737389 --> 112.446922).\n",
            "\t Train_Loss: 24.4903 Val_Loss: 112.4469  BEST VAL Loss: 112.4469\n",
            "\n",
            "Epoch 1430: Validation loss decreased (112.446922 --> 112.138672).\n",
            "\t Train_Loss: 24.4385 Val_Loss: 112.1387  BEST VAL Loss: 112.1387\n",
            "\n",
            "Epoch 1431: Validation loss decreased (112.138672 --> 111.963333).\n",
            "\t Train_Loss: 24.3874 Val_Loss: 111.9633  BEST VAL Loss: 111.9633\n",
            "\n",
            "Epoch 1432: Validation loss decreased (111.963333 --> 111.860832).\n",
            "\t Train_Loss: 24.3315 Val_Loss: 111.8608  BEST VAL Loss: 111.8608\n",
            "\n",
            "Epoch 1433: Validation loss decreased (111.860832 --> 111.680717).\n",
            "\t Train_Loss: 24.2810 Val_Loss: 111.6807  BEST VAL Loss: 111.6807\n",
            "\n",
            "Epoch 1434: Validation loss decreased (111.680717 --> 111.469780).\n",
            "\t Train_Loss: 24.2262 Val_Loss: 111.4698  BEST VAL Loss: 111.4698\n",
            "\n",
            "Epoch 1435: Validation loss decreased (111.469780 --> 111.284668).\n",
            "\t Train_Loss: 24.1774 Val_Loss: 111.2847  BEST VAL Loss: 111.2847\n",
            "\n",
            "Epoch 1436: Validation loss decreased (111.284668 --> 111.022171).\n",
            "\t Train_Loss: 24.1232 Val_Loss: 111.0222  BEST VAL Loss: 111.0222\n",
            "\n",
            "Epoch 1437: Validation loss decreased (111.022171 --> 110.730362).\n",
            "\t Train_Loss: 24.0710 Val_Loss: 110.7304  BEST VAL Loss: 110.7304\n",
            "\n",
            "Epoch 1438: Validation loss decreased (110.730362 --> 110.551018).\n",
            "\t Train_Loss: 24.0213 Val_Loss: 110.5510  BEST VAL Loss: 110.5510\n",
            "\n",
            "Epoch 1439: Validation loss decreased (110.551018 --> 110.407959).\n",
            "\t Train_Loss: 23.9679 Val_Loss: 110.4080  BEST VAL Loss: 110.4080\n",
            "\n",
            "Epoch 1440: Validation loss decreased (110.407959 --> 110.217667).\n",
            "\t Train_Loss: 23.9178 Val_Loss: 110.2177  BEST VAL Loss: 110.2177\n",
            "\n",
            "Epoch 1441: Validation loss decreased (110.217667 --> 110.060204).\n",
            "\t Train_Loss: 23.8658 Val_Loss: 110.0602  BEST VAL Loss: 110.0602\n",
            "\n",
            "Epoch 1442: Validation loss decreased (110.060204 --> 109.903076).\n",
            "\t Train_Loss: 23.8147 Val_Loss: 109.9031  BEST VAL Loss: 109.9031\n",
            "\n",
            "Epoch 1443: Validation loss decreased (109.903076 --> 109.646767).\n",
            "\t Train_Loss: 23.7649 Val_Loss: 109.6468  BEST VAL Loss: 109.6468\n",
            "\n",
            "Epoch 1444: Validation loss decreased (109.646767 --> 109.409279).\n",
            "\t Train_Loss: 23.7127 Val_Loss: 109.4093  BEST VAL Loss: 109.4093\n",
            "\n",
            "Epoch 1445: Validation loss decreased (109.409279 --> 109.237061).\n",
            "\t Train_Loss: 23.6625 Val_Loss: 109.2371  BEST VAL Loss: 109.2371\n",
            "\n",
            "Epoch 1446: Validation loss decreased (109.237061 --> 109.028313).\n",
            "\t Train_Loss: 23.6126 Val_Loss: 109.0283  BEST VAL Loss: 109.0283\n",
            "\n",
            "Epoch 1447: Validation loss decreased (109.028313 --> 108.847290).\n",
            "\t Train_Loss: 23.5611 Val_Loss: 108.8473  BEST VAL Loss: 108.8473\n",
            "\n",
            "Epoch 1448: Validation loss decreased (108.847290 --> 108.720047).\n",
            "\t Train_Loss: 23.5112 Val_Loss: 108.7200  BEST VAL Loss: 108.7200\n",
            "\n",
            "Epoch 1449: Validation loss decreased (108.720047 --> 108.512718).\n",
            "\t Train_Loss: 23.4611 Val_Loss: 108.5127  BEST VAL Loss: 108.5127\n",
            "\n",
            "Epoch 1450: Validation loss decreased (108.512718 --> 108.296219).\n",
            "\t Train_Loss: 23.4103 Val_Loss: 108.2962  BEST VAL Loss: 108.2962\n",
            "\n",
            "Epoch 1451: Validation loss decreased (108.296219 --> 108.119949).\n",
            "\t Train_Loss: 23.3606 Val_Loss: 108.1199  BEST VAL Loss: 108.1199\n",
            "\n",
            "Epoch 1452: Validation loss decreased (108.119949 --> 107.881126).\n",
            "\t Train_Loss: 23.3108 Val_Loss: 107.8811  BEST VAL Loss: 107.8811\n",
            "\n",
            "Epoch 1453: Validation loss decreased (107.881126 --> 107.685463).\n",
            "\t Train_Loss: 23.2604 Val_Loss: 107.6855  BEST VAL Loss: 107.6855\n",
            "\n",
            "Epoch 1454: Validation loss decreased (107.685463 --> 107.536674).\n",
            "\t Train_Loss: 23.2106 Val_Loss: 107.5367  BEST VAL Loss: 107.5367\n",
            "\n",
            "Epoch 1455: Validation loss decreased (107.536674 --> 107.328751).\n",
            "\t Train_Loss: 23.1613 Val_Loss: 107.3288  BEST VAL Loss: 107.3288\n",
            "\n",
            "Epoch 1456: Validation loss decreased (107.328751 --> 107.172142).\n",
            "\t Train_Loss: 23.1115 Val_Loss: 107.1721  BEST VAL Loss: 107.1721\n",
            "\n",
            "Epoch 1457: Validation loss decreased (107.172142 --> 106.991066).\n",
            "\t Train_Loss: 23.0616 Val_Loss: 106.9911  BEST VAL Loss: 106.9911\n",
            "\n",
            "Epoch 1458: Validation loss decreased (106.991066 --> 106.762268).\n",
            "\t Train_Loss: 23.0123 Val_Loss: 106.7623  BEST VAL Loss: 106.7623\n",
            "\n",
            "Epoch 1459: Validation loss decreased (106.762268 --> 106.596642).\n",
            "\t Train_Loss: 22.9633 Val_Loss: 106.5966  BEST VAL Loss: 106.5966\n",
            "\n",
            "Epoch 1460: Validation loss decreased (106.596642 --> 106.372704).\n",
            "\t Train_Loss: 22.9140 Val_Loss: 106.3727  BEST VAL Loss: 106.3727\n",
            "\n",
            "Epoch 1461: Validation loss decreased (106.372704 --> 106.200920).\n",
            "\t Train_Loss: 22.8647 Val_Loss: 106.2009  BEST VAL Loss: 106.2009\n",
            "\n",
            "Epoch 1462: Validation loss decreased (106.200920 --> 106.035500).\n",
            "\t Train_Loss: 22.8155 Val_Loss: 106.0355  BEST VAL Loss: 106.0355\n",
            "\n",
            "Epoch 1463: Validation loss decreased (106.035500 --> 105.837036).\n",
            "\t Train_Loss: 22.7667 Val_Loss: 105.8370  BEST VAL Loss: 105.8370\n",
            "\n",
            "Epoch 1464: Validation loss decreased (105.837036 --> 105.695976).\n",
            "\t Train_Loss: 22.7182 Val_Loss: 105.6960  BEST VAL Loss: 105.6960\n",
            "\n",
            "Epoch 1465: Validation loss decreased (105.695976 --> 105.459816).\n",
            "\t Train_Loss: 22.6698 Val_Loss: 105.4598  BEST VAL Loss: 105.4598\n",
            "\n",
            "Epoch 1466: Validation loss decreased (105.459816 --> 105.322746).\n",
            "\t Train_Loss: 22.6214 Val_Loss: 105.3227  BEST VAL Loss: 105.3227\n",
            "\n",
            "Epoch 1467: Validation loss decreased (105.322746 --> 105.068069).\n",
            "\t Train_Loss: 22.5733 Val_Loss: 105.0681  BEST VAL Loss: 105.0681\n",
            "\n",
            "Epoch 1468: Validation loss decreased (105.068069 --> 104.973854).\n",
            "\t Train_Loss: 22.5256 Val_Loss: 104.9739  BEST VAL Loss: 104.9739\n",
            "\n",
            "Epoch 1469: Validation loss decreased (104.973854 --> 104.681824).\n",
            "\t Train_Loss: 22.4787 Val_Loss: 104.6818  BEST VAL Loss: 104.6818\n",
            "\n",
            "Epoch 1470: Validation loss did not decrease\n",
            "\t Train_Loss: 22.4340 Val_Loss: 104.7060  BEST VAL Loss: 104.6818\n",
            "\n",
            "Epoch 1471: Validation loss decreased (104.681824 --> 104.216431).\n",
            "\t Train_Loss: 22.3962 Val_Loss: 104.2164  BEST VAL Loss: 104.2164\n",
            "\n",
            "Epoch 1472: Validation loss did not decrease\n",
            "\t Train_Loss: 22.3808 Val_Loss: 104.6543  BEST VAL Loss: 104.2164\n",
            "\n",
            "Epoch 1473: Validation loss decreased (104.216431 --> 103.682823).\n",
            "\t Train_Loss: 22.4513 Val_Loss: 103.6828  BEST VAL Loss: 103.6828\n",
            "\n",
            "Epoch 1474: Validation loss did not decrease\n",
            "\t Train_Loss: 22.7326 Val_Loss: 104.9706  BEST VAL Loss: 103.6828\n",
            "\n",
            "Epoch 1475: Validation loss did not decrease\n",
            "\t Train_Loss: 23.7737 Val_Loss: 103.7114  BEST VAL Loss: 103.6828\n",
            "\n",
            "Epoch 1476: Validation loss did not decrease\n",
            "\t Train_Loss: 23.0195 Val_Loss: 103.7401  BEST VAL Loss: 103.6828\n",
            "\n",
            "Epoch 1477: Validation loss decreased (103.682823 --> 103.184731).\n",
            "\t Train_Loss: 22.6455 Val_Loss: 103.1847  BEST VAL Loss: 103.1847\n",
            "\n",
            "Epoch 1478: Validation loss decreased (103.184731 --> 103.120018).\n",
            "\t Train_Loss: 22.1256 Val_Loss: 103.1200  BEST VAL Loss: 103.1200\n",
            "\n",
            "Epoch 1479: Validation loss did not decrease\n",
            "\t Train_Loss: 22.4934 Val_Loss: 104.2867  BEST VAL Loss: 103.1200\n",
            "\n",
            "Epoch 1480: Validation loss decreased (103.120018 --> 102.825806).\n",
            "\t Train_Loss: 23.5895 Val_Loss: 102.8258  BEST VAL Loss: 102.8258\n",
            "\n",
            "Epoch 1481: Validation loss decreased (102.825806 --> 102.696510).\n",
            "\t Train_Loss: 22.3021 Val_Loss: 102.6965  BEST VAL Loss: 102.6965\n",
            "\n",
            "Epoch 1482: Validation loss did not decrease\n",
            "\t Train_Loss: 21.9654 Val_Loss: 103.0059  BEST VAL Loss: 102.6965\n",
            "\n",
            "Epoch 1483: Validation loss decreased (102.696510 --> 102.454361).\n",
            "\t Train_Loss: 22.3867 Val_Loss: 102.4544  BEST VAL Loss: 102.4544\n",
            "\n",
            "Epoch 1484: Validation loss did not decrease\n",
            "\t Train_Loss: 22.1081 Val_Loss: 102.6956  BEST VAL Loss: 102.4544\n",
            "\n",
            "Epoch 1485: Validation loss decreased (102.454361 --> 102.411209).\n",
            "\t Train_Loss: 21.9071 Val_Loss: 102.4112  BEST VAL Loss: 102.4112\n",
            "\n",
            "Epoch 1486: Validation loss decreased (102.411209 --> 102.203583).\n",
            "\t Train_Loss: 21.7842 Val_Loss: 102.2036  BEST VAL Loss: 102.2036\n",
            "\n",
            "Epoch 1487: Validation loss did not decrease\n",
            "\t Train_Loss: 21.7831 Val_Loss: 102.5984  BEST VAL Loss: 102.2036\n",
            "\n",
            "Epoch 1488: Validation loss decreased (102.203583 --> 101.716698).\n",
            "\t Train_Loss: 21.8908 Val_Loss: 101.7167  BEST VAL Loss: 101.7167\n",
            "\n",
            "Epoch 1489: Validation loss did not decrease\n",
            "\t Train_Loss: 21.9457 Val_Loss: 102.2349  BEST VAL Loss: 101.7167\n",
            "\n",
            "Epoch 1490: Validation loss decreased (101.716698 --> 101.194023).\n",
            "\t Train_Loss: 22.1139 Val_Loss: 101.1940  BEST VAL Loss: 101.1940\n",
            "\n",
            "Epoch 1491: Validation loss decreased (101.194023 --> 101.069901).\n",
            "\t Train_Loss: 21.6555 Val_Loss: 101.0699  BEST VAL Loss: 101.0699\n",
            "\n",
            "Epoch 1492: Validation loss did not decrease\n",
            "\t Train_Loss: 21.5609 Val_Loss: 101.4484  BEST VAL Loss: 101.0699\n",
            "\n",
            "Epoch 1493: Validation loss decreased (101.069901 --> 100.697701).\n",
            "\t Train_Loss: 21.7340 Val_Loss: 100.6977  BEST VAL Loss: 100.6977\n",
            "\n",
            "Epoch 1494: Validation loss did not decrease\n",
            "\t Train_Loss: 21.8193 Val_Loss: 101.3578  BEST VAL Loss: 100.6977\n",
            "\n",
            "Epoch 1495: Validation loss decreased (100.697701 --> 100.331444).\n",
            "\t Train_Loss: 21.9520 Val_Loss: 100.3314  BEST VAL Loss: 100.3314\n",
            "\n",
            "Epoch 1496: Validation loss decreased (100.331444 --> 100.077087).\n",
            "\t Train_Loss: 21.4197 Val_Loss: 100.0771  BEST VAL Loss: 100.0771\n",
            "\n",
            "Epoch 1497: Validation loss did not decrease\n",
            "\t Train_Loss: 21.4899 Val_Loss: 100.8199  BEST VAL Loss: 100.0771\n",
            "\n",
            "Epoch 1498: Validation loss decreased (100.077087 --> 99.876884).\n",
            "\t Train_Loss: 21.9658 Val_Loss: 99.8769  BEST VAL Loss: 99.8769\n",
            "\n",
            "Epoch 1499: Validation loss decreased (99.876884 --> 99.846085).\n",
            "\t Train_Loss: 21.3929 Val_Loss: 99.8461  BEST VAL Loss: 99.8461\n",
            "\n",
            "Epoch 1500: Validation loss did not decrease\n",
            "\t Train_Loss: 21.2446 Val_Loss: 100.3437  BEST VAL Loss: 99.8461\n",
            "\n",
            "Epoch 1501: Validation loss decreased (99.846085 --> 99.441338).\n",
            "\t Train_Loss: 21.4623 Val_Loss: 99.4413  BEST VAL Loss: 99.4413\n",
            "\n",
            "Epoch 1502: Validation loss did not decrease\n",
            "\t Train_Loss: 21.6530 Val_Loss: 100.5593  BEST VAL Loss: 99.4413\n",
            "\n",
            "Epoch 1503: Validation loss decreased (99.441338 --> 99.367615).\n",
            "\t Train_Loss: 21.9195 Val_Loss: 99.3676  BEST VAL Loss: 99.3676\n",
            "\n",
            "Epoch 1504: Validation loss decreased (99.367615 --> 99.168289).\n",
            "\t Train_Loss: 21.0943 Val_Loss: 99.1683  BEST VAL Loss: 99.1683\n",
            "\n",
            "Epoch 1505: Validation loss did not decrease\n",
            "\t Train_Loss: 21.3995 Val_Loss: 100.3440  BEST VAL Loss: 99.1683\n",
            "\n",
            "Epoch 1506: Validation loss decreased (99.168289 --> 98.920753).\n",
            "\t Train_Loss: 22.3971 Val_Loss: 98.9208  BEST VAL Loss: 98.9208\n",
            "\n",
            "Epoch 1507: Validation loss decreased (98.920753 --> 98.885216).\n",
            "\t Train_Loss: 21.0726 Val_Loss: 98.8852  BEST VAL Loss: 98.8852\n",
            "\n",
            "Epoch 1508: Validation loss did not decrease\n",
            "\t Train_Loss: 21.3698 Val_Loss: 99.8346  BEST VAL Loss: 98.8852\n",
            "\n",
            "Epoch 1509: Validation loss decreased (98.885216 --> 98.570694).\n",
            "\t Train_Loss: 22.5375 Val_Loss: 98.5707  BEST VAL Loss: 98.5707\n",
            "\n",
            "Epoch 1510: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9147 Val_Loss: 99.0404  BEST VAL Loss: 98.5707\n",
            "\n",
            "Epoch 1511: Validation loss did not decrease\n",
            "\t Train_Loss: 21.9017 Val_Loss: 99.4260  BEST VAL Loss: 98.5707\n",
            "\n",
            "Epoch 1512: Validation loss decreased (98.570694 --> 98.468285).\n",
            "\t Train_Loss: 22.8530 Val_Loss: 98.4683  BEST VAL Loss: 98.4683\n",
            "\n",
            "Epoch 1513: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9780 Val_Loss: 101.1859  BEST VAL Loss: 98.4683\n",
            "\n",
            "Epoch 1514: Validation loss decreased (98.468285 --> 98.394821).\n",
            "\t Train_Loss: 24.3529 Val_Loss: 98.3948  BEST VAL Loss: 98.3948\n",
            "\n",
            "Epoch 1515: Validation loss did not decrease\n",
            "\t Train_Loss: 21.3470 Val_Loss: 98.6348  BEST VAL Loss: 98.3948\n",
            "\n",
            "Epoch 1516: Validation loss decreased (98.394821 --> 97.750259).\n",
            "\t Train_Loss: 22.5092 Val_Loss: 97.7503  BEST VAL Loss: 97.7503\n",
            "\n",
            "Epoch 1517: Validation loss decreased (97.750259 --> 97.212425).\n",
            "\t Train_Loss: 21.3599 Val_Loss: 97.2124  BEST VAL Loss: 97.2124\n",
            "\n",
            "Epoch 1518: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9156 Val_Loss: 97.5327  BEST VAL Loss: 97.2124\n",
            "\n",
            "Epoch 1519: Validation loss did not decrease\n",
            "\t Train_Loss: 21.8945 Val_Loss: 97.3337  BEST VAL Loss: 97.2124\n",
            "\n",
            "Epoch 1520: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9831 Val_Loss: 98.5429  BEST VAL Loss: 97.2124\n",
            "\n",
            "Epoch 1521: Validation loss did not decrease\n",
            "\t Train_Loss: 22.7578 Val_Loss: 97.2871  BEST VAL Loss: 97.2124\n",
            "\n",
            "Epoch 1522: Validation loss did not decrease\n",
            "\t Train_Loss: 20.7653 Val_Loss: 97.5508  BEST VAL Loss: 97.2124\n",
            "\n",
            "Epoch 1523: Validation loss decreased (97.212425 --> 96.548035).\n",
            "\t Train_Loss: 21.5188 Val_Loss: 96.5480  BEST VAL Loss: 96.5480\n",
            "\n",
            "Epoch 1524: Validation loss decreased (96.548035 --> 96.493568).\n",
            "\t Train_Loss: 20.6908 Val_Loss: 96.4936  BEST VAL Loss: 96.4936\n",
            "\n",
            "Epoch 1525: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9994 Val_Loss: 96.6962  BEST VAL Loss: 96.4936\n",
            "\n",
            "Epoch 1526: Validation loss did not decrease\n",
            "\t Train_Loss: 21.2318 Val_Loss: 96.5715  BEST VAL Loss: 96.4936\n",
            "\n",
            "Epoch 1527: Validation loss did not decrease\n",
            "\t Train_Loss: 20.8368 Val_Loss: 96.6887  BEST VAL Loss: 96.4936\n",
            "\n",
            "Epoch 1528: Validation loss did not decrease\n",
            "\t Train_Loss: 21.8362 Val_Loss: 96.5234  BEST VAL Loss: 96.4936\n",
            "\n",
            "Epoch 1529: Validation loss did not decrease\n",
            "\t Train_Loss: 20.5764 Val_Loss: 96.7166  BEST VAL Loss: 96.4936\n",
            "\n",
            "Epoch 1530: Validation loss decreased (96.493568 --> 95.805214).\n",
            "\t Train_Loss: 20.8845 Val_Loss: 95.8052  BEST VAL Loss: 95.8052\n",
            "\n",
            "Epoch 1531: Validation loss decreased (95.805214 --> 95.470665).\n",
            "\t Train_Loss: 20.6429 Val_Loss: 95.4707  BEST VAL Loss: 95.4707\n",
            "\n",
            "Epoch 1532: Validation loss decreased (95.470665 --> 95.364822).\n",
            "\t Train_Loss: 20.3387 Val_Loss: 95.3648  BEST VAL Loss: 95.3648\n",
            "\n",
            "Epoch 1533: Validation loss decreased (95.364822 --> 95.050362).\n",
            "\t Train_Loss: 20.5408 Val_Loss: 95.0504  BEST VAL Loss: 95.0504\n",
            "\n",
            "Epoch 1534: Validation loss decreased (95.050362 --> 95.044411).\n",
            "\t Train_Loss: 20.3177 Val_Loss: 95.0444  BEST VAL Loss: 95.0444\n",
            "\n",
            "Epoch 1535: Validation loss did not decrease\n",
            "\t Train_Loss: 20.2772 Val_Loss: 95.5438  BEST VAL Loss: 95.0444\n",
            "\n",
            "Epoch 1536: Validation loss did not decrease\n",
            "\t Train_Loss: 20.3667 Val_Loss: 95.1414  BEST VAL Loss: 95.0444\n",
            "\n",
            "Epoch 1537: Validation loss did not decrease\n",
            "\t Train_Loss: 20.1487 Val_Loss: 95.0490  BEST VAL Loss: 95.0444\n",
            "\n",
            "Epoch 1538: Validation loss did not decrease\n",
            "\t Train_Loss: 20.1827 Val_Loss: 95.3654  BEST VAL Loss: 95.0444\n",
            "\n",
            "Epoch 1539: Validation loss decreased (95.044411 --> 94.669357).\n",
            "\t Train_Loss: 20.2692 Val_Loss: 94.6694  BEST VAL Loss: 94.6694\n",
            "\n",
            "Epoch 1540: Validation loss decreased (94.669357 --> 94.440346).\n",
            "\t Train_Loss: 20.0206 Val_Loss: 94.4403  BEST VAL Loss: 94.4403\n",
            "\n",
            "Epoch 1541: Validation loss did not decrease\n",
            "\t Train_Loss: 20.0545 Val_Loss: 94.7607  BEST VAL Loss: 94.4403\n",
            "\n",
            "Epoch 1542: Validation loss decreased (94.440346 --> 94.283012).\n",
            "\t Train_Loss: 20.1722 Val_Loss: 94.2830  BEST VAL Loss: 94.2830\n",
            "\n",
            "Epoch 1543: Validation loss did not decrease\n",
            "\t Train_Loss: 19.9709 Val_Loss: 94.3922  BEST VAL Loss: 94.2830\n",
            "\n",
            "Epoch 1544: Validation loss did not decrease\n",
            "\t Train_Loss: 19.8666 Val_Loss: 94.7547  BEST VAL Loss: 94.2830\n",
            "\n",
            "Epoch 1545: Validation loss decreased (94.283012 --> 94.114983).\n",
            "\t Train_Loss: 19.9472 Val_Loss: 94.1150  BEST VAL Loss: 94.1150\n",
            "\n",
            "Epoch 1546: Validation loss did not decrease\n",
            "\t Train_Loss: 19.9183 Val_Loss: 94.2700  BEST VAL Loss: 94.1150\n",
            "\n",
            "Epoch 1547: Validation loss decreased (94.114983 --> 93.892639).\n",
            "\t Train_Loss: 19.8028 Val_Loss: 93.8926  BEST VAL Loss: 93.8926\n",
            "\n",
            "Epoch 1548: Validation loss decreased (93.892639 --> 93.647102).\n",
            "\t Train_Loss: 19.7213 Val_Loss: 93.6471  BEST VAL Loss: 93.6471\n",
            "\n",
            "Epoch 1549: Validation loss did not decrease\n",
            "\t Train_Loss: 19.7483 Val_Loss: 93.8424  BEST VAL Loss: 93.6471\n",
            "\n",
            "Epoch 1550: Validation loss decreased (93.647102 --> 93.433006).\n",
            "\t Train_Loss: 19.7887 Val_Loss: 93.4330  BEST VAL Loss: 93.4330\n",
            "\n",
            "Epoch 1551: Validation loss did not decrease\n",
            "\t Train_Loss: 19.7376 Val_Loss: 93.8026  BEST VAL Loss: 93.4330\n",
            "\n",
            "Epoch 1552: Validation loss did not decrease\n",
            "\t Train_Loss: 19.6505 Val_Loss: 93.4725  BEST VAL Loss: 93.4330\n",
            "\n",
            "Epoch 1553: Validation loss decreased (93.433006 --> 93.339775).\n",
            "\t Train_Loss: 19.5683 Val_Loss: 93.3398  BEST VAL Loss: 93.3398\n",
            "\n",
            "Epoch 1554: Validation loss did not decrease\n",
            "\t Train_Loss: 19.5406 Val_Loss: 93.4018  BEST VAL Loss: 93.3398\n",
            "\n",
            "Epoch 1555: Validation loss decreased (93.339775 --> 92.847733).\n",
            "\t Train_Loss: 19.5467 Val_Loss: 92.8477  BEST VAL Loss: 92.8477\n",
            "\n",
            "Epoch 1556: Validation loss did not decrease\n",
            "\t Train_Loss: 19.5667 Val_Loss: 93.1644  BEST VAL Loss: 92.8477\n",
            "\n",
            "Epoch 1557: Validation loss decreased (92.847733 --> 92.629021).\n",
            "\t Train_Loss: 19.5832 Val_Loss: 92.6290  BEST VAL Loss: 92.6290\n",
            "\n",
            "Epoch 1558: Validation loss did not decrease\n",
            "\t Train_Loss: 19.5355 Val_Loss: 93.0565  BEST VAL Loss: 92.6290\n",
            "\n",
            "Epoch 1559: Validation loss decreased (92.629021 --> 92.472466).\n",
            "\t Train_Loss: 19.5050 Val_Loss: 92.4725  BEST VAL Loss: 92.4725\n",
            "\n",
            "Epoch 1560: Validation loss did not decrease\n",
            "\t Train_Loss: 19.4213 Val_Loss: 92.7842  BEST VAL Loss: 92.4725\n",
            "\n",
            "Epoch 1561: Validation loss decreased (92.472466 --> 92.269608).\n",
            "\t Train_Loss: 19.3696 Val_Loss: 92.2696  BEST VAL Loss: 92.2696\n",
            "\n",
            "Epoch 1562: Validation loss did not decrease\n",
            "\t Train_Loss: 19.2998 Val_Loss: 92.3120  BEST VAL Loss: 92.2696\n",
            "\n",
            "Epoch 1563: Validation loss decreased (92.269608 --> 92.000992).\n",
            "\t Train_Loss: 19.2550 Val_Loss: 92.0010  BEST VAL Loss: 92.0010\n",
            "\n",
            "Epoch 1564: Validation loss did not decrease\n",
            "\t Train_Loss: 19.2231 Val_Loss: 92.0089  BEST VAL Loss: 92.0010\n",
            "\n",
            "Epoch 1565: Validation loss decreased (92.000992 --> 91.740135).\n",
            "\t Train_Loss: 19.1959 Val_Loss: 91.7401  BEST VAL Loss: 91.7401\n",
            "\n",
            "Epoch 1566: Validation loss did not decrease\n",
            "\t Train_Loss: 19.1726 Val_Loss: 92.0005  BEST VAL Loss: 91.7401\n",
            "\n",
            "Epoch 1567: Validation loss decreased (91.740135 --> 91.504005).\n",
            "\t Train_Loss: 19.1730 Val_Loss: 91.5040  BEST VAL Loss: 91.5040\n",
            "\n",
            "Epoch 1568: Validation loss did not decrease\n",
            "\t Train_Loss: 19.2010 Val_Loss: 92.1659  BEST VAL Loss: 91.5040\n",
            "\n",
            "Epoch 1569: Validation loss decreased (91.504005 --> 91.107597).\n",
            "\t Train_Loss: 19.3347 Val_Loss: 91.1076  BEST VAL Loss: 91.1076\n",
            "\n",
            "Epoch 1570: Validation loss did not decrease\n",
            "\t Train_Loss: 19.3270 Val_Loss: 92.0701  BEST VAL Loss: 91.1076\n",
            "\n",
            "Epoch 1571: Validation loss decreased (91.107597 --> 90.936440).\n",
            "\t Train_Loss: 19.4696 Val_Loss: 90.9364  BEST VAL Loss: 90.9364\n",
            "\n",
            "Epoch 1572: Validation loss did not decrease\n",
            "\t Train_Loss: 19.1907 Val_Loss: 91.4894  BEST VAL Loss: 90.9364\n",
            "\n",
            "Epoch 1573: Validation loss decreased (90.936440 --> 90.760986).\n",
            "\t Train_Loss: 19.1150 Val_Loss: 90.7610  BEST VAL Loss: 90.7610\n",
            "\n",
            "Epoch 1574: Validation loss did not decrease\n",
            "\t Train_Loss: 19.0076 Val_Loss: 91.1625  BEST VAL Loss: 90.7610\n",
            "\n",
            "Epoch 1575: Validation loss decreased (90.760986 --> 90.538643).\n",
            "\t Train_Loss: 18.9879 Val_Loss: 90.5386  BEST VAL Loss: 90.5386\n",
            "\n",
            "Epoch 1576: Validation loss did not decrease\n",
            "\t Train_Loss: 18.9563 Val_Loss: 91.0495  BEST VAL Loss: 90.5386\n",
            "\n",
            "Epoch 1577: Validation loss decreased (90.538643 --> 90.220589).\n",
            "\t Train_Loss: 19.0167 Val_Loss: 90.2206  BEST VAL Loss: 90.2206\n",
            "\n",
            "Epoch 1578: Validation loss did not decrease\n",
            "\t Train_Loss: 19.0092 Val_Loss: 91.1146  BEST VAL Loss: 90.2206\n",
            "\n",
            "Epoch 1579: Validation loss decreased (90.220589 --> 89.942749).\n",
            "\t Train_Loss: 19.1737 Val_Loss: 89.9427  BEST VAL Loss: 89.9427\n",
            "\n",
            "Epoch 1580: Validation loss did not decrease\n",
            "\t Train_Loss: 18.9911 Val_Loss: 90.8016  BEST VAL Loss: 89.9427\n",
            "\n",
            "Epoch 1581: Validation loss decreased (89.942749 --> 89.818962).\n",
            "\t Train_Loss: 18.9914 Val_Loss: 89.8190  BEST VAL Loss: 89.8190\n",
            "\n",
            "Epoch 1582: Validation loss did not decrease\n",
            "\t Train_Loss: 18.8074 Val_Loss: 90.2521  BEST VAL Loss: 89.8190\n",
            "\n",
            "Epoch 1583: Validation loss decreased (89.818962 --> 89.689598).\n",
            "\t Train_Loss: 18.7454 Val_Loss: 89.6896  BEST VAL Loss: 89.6896\n",
            "\n",
            "Epoch 1584: Validation loss did not decrease\n",
            "\t Train_Loss: 18.6731 Val_Loss: 89.8892  BEST VAL Loss: 89.6896\n",
            "\n",
            "Epoch 1585: Validation loss decreased (89.689598 --> 89.518005).\n",
            "\t Train_Loss: 18.6395 Val_Loss: 89.5180  BEST VAL Loss: 89.5180\n",
            "\n",
            "Epoch 1586: Validation loss did not decrease\n",
            "\t Train_Loss: 18.6075 Val_Loss: 89.7424  BEST VAL Loss: 89.5180\n",
            "\n",
            "Epoch 1587: Validation loss decreased (89.518005 --> 89.272720).\n",
            "\t Train_Loss: 18.5968 Val_Loss: 89.2727  BEST VAL Loss: 89.2727\n",
            "\n",
            "Epoch 1588: Validation loss did not decrease\n",
            "\t Train_Loss: 18.5845 Val_Loss: 89.7757  BEST VAL Loss: 89.2727\n",
            "\n",
            "Epoch 1589: Validation loss decreased (89.272720 --> 88.995155).\n",
            "\t Train_Loss: 18.6296 Val_Loss: 88.9952  BEST VAL Loss: 88.9952\n",
            "\n",
            "Epoch 1590: Validation loss did not decrease\n",
            "\t Train_Loss: 18.6165 Val_Loss: 89.8462  BEST VAL Loss: 88.9952\n",
            "\n",
            "Epoch 1591: Validation loss decreased (88.995155 --> 88.711906).\n",
            "\t Train_Loss: 18.7541 Val_Loss: 88.7119  BEST VAL Loss: 88.7119\n",
            "\n",
            "Epoch 1592: Validation loss did not decrease\n",
            "\t Train_Loss: 18.6601 Val_Loss: 89.7612  BEST VAL Loss: 88.7119\n",
            "\n",
            "Epoch 1593: Validation loss decreased (88.711906 --> 88.508911).\n",
            "\t Train_Loss: 18.8092 Val_Loss: 88.5089  BEST VAL Loss: 88.5089\n",
            "\n",
            "Epoch 1594: Validation loss did not decrease\n",
            "\t Train_Loss: 18.6019 Val_Loss: 89.4388  BEST VAL Loss: 88.5089\n",
            "\n",
            "Epoch 1595: Validation loss decreased (88.508911 --> 88.354179).\n",
            "\t Train_Loss: 18.6473 Val_Loss: 88.3542  BEST VAL Loss: 88.3542\n",
            "\n",
            "Epoch 1596: Validation loss did not decrease\n",
            "\t Train_Loss: 18.4470 Val_Loss: 88.9645  BEST VAL Loss: 88.3542\n",
            "\n",
            "Epoch 1597: Validation loss decreased (88.354179 --> 88.276100).\n",
            "\t Train_Loss: 18.4063 Val_Loss: 88.2761  BEST VAL Loss: 88.2761\n",
            "\n",
            "Epoch 1598: Validation loss did not decrease\n",
            "\t Train_Loss: 18.3040 Val_Loss: 88.5237  BEST VAL Loss: 88.2761\n",
            "\n",
            "Epoch 1599: Validation loss decreased (88.276100 --> 88.205772).\n",
            "\t Train_Loss: 18.2566 Val_Loss: 88.2058  BEST VAL Loss: 88.2058\n",
            "\n",
            "Epoch 1600: Validation loss did not decrease\n",
            "\t Train_Loss: 18.2142 Val_Loss: 88.2399  BEST VAL Loss: 88.2058\n",
            "\n",
            "Epoch 1601: Validation loss decreased (88.205772 --> 88.124084).\n",
            "\t Train_Loss: 18.1826 Val_Loss: 88.1241  BEST VAL Loss: 88.1241\n",
            "\n",
            "Epoch 1602: Validation loss decreased (88.124084 --> 88.030319).\n",
            "\t Train_Loss: 18.1544 Val_Loss: 88.0303  BEST VAL Loss: 88.0303\n",
            "\n",
            "Epoch 1603: Validation loss did not decrease\n",
            "\t Train_Loss: 18.1282 Val_Loss: 88.0444  BEST VAL Loss: 88.0303\n",
            "\n",
            "Epoch 1604: Validation loss decreased (88.030319 --> 87.822220).\n",
            "\t Train_Loss: 18.1044 Val_Loss: 87.8222  BEST VAL Loss: 87.8222\n",
            "\n",
            "Epoch 1605: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0837 Val_Loss: 88.0067  BEST VAL Loss: 87.8222\n",
            "\n",
            "Epoch 1606: Validation loss decreased (87.822220 --> 87.549736).\n",
            "\t Train_Loss: 18.0735 Val_Loss: 87.5497  BEST VAL Loss: 87.5497\n",
            "\n",
            "Epoch 1607: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0690 Val_Loss: 88.1231  BEST VAL Loss: 87.5497\n",
            "\n",
            "Epoch 1608: Validation loss decreased (87.549736 --> 87.200829).\n",
            "\t Train_Loss: 18.1416 Val_Loss: 87.2008  BEST VAL Loss: 87.2008\n",
            "\n",
            "Epoch 1609: Validation loss did not decrease\n",
            "\t Train_Loss: 18.1870 Val_Loss: 88.7280  BEST VAL Loss: 87.2008\n",
            "\n",
            "Epoch 1610: Validation loss decreased (87.200829 --> 86.977150).\n",
            "\t Train_Loss: 18.7144 Val_Loss: 86.9771  BEST VAL Loss: 86.9771\n",
            "\n",
            "Epoch 1611: Validation loss did not decrease\n",
            "\t Train_Loss: 19.0429 Val_Loss: 89.3405  BEST VAL Loss: 86.9771\n",
            "\n",
            "Epoch 1612: Validation loss did not decrease\n",
            "\t Train_Loss: 20.1905 Val_Loss: 88.1605  BEST VAL Loss: 86.9771\n",
            "\n",
            "Epoch 1613: Validation loss did not decrease\n",
            "\t Train_Loss: 19.5252 Val_Loss: 87.6953  BEST VAL Loss: 86.9771\n",
            "\n",
            "Epoch 1614: Validation loss did not decrease\n",
            "\t Train_Loss: 18.2806 Val_Loss: 87.5231  BEST VAL Loss: 86.9771\n",
            "\n",
            "Epoch 1615: Validation loss did not decrease\n",
            "\t Train_Loss: 18.1077 Val_Loss: 87.7170  BEST VAL Loss: 86.9771\n",
            "\n",
            "Epoch 1616: Validation loss did not decrease\n",
            "\t Train_Loss: 19.0385 Val_Loss: 88.3300  BEST VAL Loss: 86.9771\n",
            "\n",
            "Epoch 1617: Validation loss decreased (86.977150 --> 86.566460).\n",
            "\t Train_Loss: 18.7789 Val_Loss: 86.5665  BEST VAL Loss: 86.5665\n",
            "\n",
            "Epoch 1618: Validation loss decreased (86.566460 --> 86.502129).\n",
            "\t Train_Loss: 17.8898 Val_Loss: 86.5021  BEST VAL Loss: 86.5021\n",
            "\n",
            "Epoch 1619: Validation loss did not decrease\n",
            "\t Train_Loss: 19.3381 Val_Loss: 88.8057  BEST VAL Loss: 86.5021\n",
            "\n",
            "Epoch 1620: Validation loss decreased (86.502129 --> 86.412453).\n",
            "\t Train_Loss: 21.1029 Val_Loss: 86.4125  BEST VAL Loss: 86.4125\n",
            "\n",
            "Epoch 1621: Validation loss did not decrease\n",
            "\t Train_Loss: 17.8744 Val_Loss: 88.7271  BEST VAL Loss: 86.4125\n",
            "\n",
            "Epoch 1622: Validation loss did not decrease\n",
            "\t Train_Loss: 21.2456 Val_Loss: 86.6308  BEST VAL Loss: 86.4125\n",
            "\n",
            "Epoch 1623: Validation loss did not decrease\n",
            "\t Train_Loss: 18.2766 Val_Loss: 86.4995  BEST VAL Loss: 86.4125\n",
            "\n",
            "Epoch 1624: Validation loss did not decrease\n",
            "\t Train_Loss: 19.1994 Val_Loss: 86.8478  BEST VAL Loss: 86.4125\n",
            "\n",
            "Epoch 1625: Validation loss decreased (86.412453 --> 86.203514).\n",
            "\t Train_Loss: 18.6672 Val_Loss: 86.2035  BEST VAL Loss: 86.2035\n",
            "\n",
            "Epoch 1626: Validation loss did not decrease\n",
            "\t Train_Loss: 17.9187 Val_Loss: 86.7581  BEST VAL Loss: 86.2035\n",
            "\n",
            "Epoch 1627: Validation loss did not decrease\n",
            "\t Train_Loss: 18.3240 Val_Loss: 86.5531  BEST VAL Loss: 86.2035\n",
            "\n",
            "Epoch 1628: Validation loss decreased (86.203514 --> 86.045464).\n",
            "\t Train_Loss: 18.0178 Val_Loss: 86.0455  BEST VAL Loss: 86.0455\n",
            "\n",
            "Epoch 1629: Validation loss did not decrease\n",
            "\t Train_Loss: 18.1384 Val_Loss: 86.8475  BEST VAL Loss: 86.0455\n",
            "\n",
            "Epoch 1630: Validation loss did not decrease\n",
            "\t Train_Loss: 17.8017 Val_Loss: 86.3950  BEST VAL Loss: 86.0455\n",
            "\n",
            "Epoch 1631: Validation loss decreased (86.045464 --> 85.434822).\n",
            "\t Train_Loss: 17.8335 Val_Loss: 85.4348  BEST VAL Loss: 85.4348\n",
            "\n",
            "Epoch 1632: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0072 Val_Loss: 86.0634  BEST VAL Loss: 85.4348\n",
            "\n",
            "Epoch 1633: Validation loss did not decrease\n",
            "\t Train_Loss: 17.8540 Val_Loss: 85.8444  BEST VAL Loss: 85.4348\n",
            "\n",
            "Epoch 1634: Validation loss decreased (85.434822 --> 84.976906).\n",
            "\t Train_Loss: 17.6162 Val_Loss: 84.9769  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1635: Validation loss did not decrease\n",
            "\t Train_Loss: 17.9360 Val_Loss: 87.0356  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1636: Validation loss did not decrease\n",
            "\t Train_Loss: 17.7831 Val_Loss: 87.0579  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1637: Validation loss did not decrease\n",
            "\t Train_Loss: 17.5237 Val_Loss: 86.7976  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1638: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0587 Val_Loss: 88.0603  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1639: Validation loss did not decrease\n",
            "\t Train_Loss: 17.4502 Val_Loss: 88.7110  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1640: Validation loss did not decrease\n",
            "\t Train_Loss: 17.6264 Val_Loss: 88.1405  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1641: Validation loss did not decrease\n",
            "\t Train_Loss: 17.5071 Val_Loss: 87.3589  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1642: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1530 Val_Loss: 87.0314  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1643: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2595 Val_Loss: 85.4031  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1644: Validation loss did not decrease\n",
            "\t Train_Loss: 17.0961 Val_Loss: 85.0578  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1645: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1092 Val_Loss: 86.0245  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1646: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1742 Val_Loss: 85.0564  BEST VAL Loss: 84.9769\n",
            "\n",
            "Epoch 1647: Validation loss decreased (84.976906 --> 84.420158).\n",
            "\t Train_Loss: 16.9752 Val_Loss: 84.4202  BEST VAL Loss: 84.4202\n",
            "\n",
            "Epoch 1648: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1244 Val_Loss: 85.8639  BEST VAL Loss: 84.4202\n",
            "\n",
            "Epoch 1649: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1217 Val_Loss: 85.6607  BEST VAL Loss: 84.4202\n",
            "\n",
            "Epoch 1650: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9066 Val_Loss: 85.3863  BEST VAL Loss: 84.4202\n",
            "\n",
            "Epoch 1651: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2260 Val_Loss: 87.0432  BEST VAL Loss: 84.4202\n",
            "\n",
            "Epoch 1652: Validation loss did not decrease\n",
            "\t Train_Loss: 17.3287 Val_Loss: 86.7034  BEST VAL Loss: 84.4202\n",
            "\n",
            "Epoch 1653: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8082 Val_Loss: 86.0558  BEST VAL Loss: 84.4202\n",
            "\n",
            "Epoch 1654: Validation loss did not decrease\n",
            "\t Train_Loss: 17.5507 Val_Loss: 87.7875  BEST VAL Loss: 84.4202\n",
            "\n",
            "Epoch 1655: Validation loss decreased (84.420158 --> 83.152023).\n",
            "\t Train_Loss: 18.0002 Val_Loss: 83.1520  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1656: Validation loss did not decrease\n",
            "\t Train_Loss: 17.6678 Val_Loss: 85.2056  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1657: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8039 Val_Loss: 85.7265  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1658: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2077 Val_Loss: 85.5891  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1659: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9019 Val_Loss: 85.3507  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1660: Validation loss did not decrease\n",
            "\t Train_Loss: 17.0467 Val_Loss: 85.8702  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1661: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5991 Val_Loss: 85.4603  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1662: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9348 Val_Loss: 84.8431  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1663: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5861 Val_Loss: 83.8258  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1664: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7223 Val_Loss: 85.0409  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1665: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5058 Val_Loss: 83.5297  BEST VAL Loss: 83.1520\n",
            "\n",
            "Epoch 1666: Validation loss decreased (83.152023 --> 82.935158).\n",
            "\t Train_Loss: 19.2178 Val_Loss: 82.9352  BEST VAL Loss: 82.9352\n",
            "\n",
            "Epoch 1667: Validation loss decreased (82.935158 --> 82.919159).\n",
            "\t Train_Loss: 17.7947 Val_Loss: 82.9192  BEST VAL Loss: 82.9192\n",
            "\n",
            "Epoch 1668: Validation loss decreased (82.919159 --> 82.910439).\n",
            "\t Train_Loss: 17.7550 Val_Loss: 82.9104  BEST VAL Loss: 82.9104\n",
            "\n",
            "Epoch 1669: Validation loss decreased (82.910439 --> 79.193542).\n",
            "\t Train_Loss: 17.5841 Val_Loss: 79.1935  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1670: Validation loss did not decrease\n",
            "\t Train_Loss: 18.2404 Val_Loss: 83.2131  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1671: Validation loss did not decrease\n",
            "\t Train_Loss: 17.3503 Val_Loss: 82.4398  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1672: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1469 Val_Loss: 82.4715  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1673: Validation loss did not decrease\n",
            "\t Train_Loss: 17.4248 Val_Loss: 82.5309  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1674: Validation loss did not decrease\n",
            "\t Train_Loss: 18.1254 Val_Loss: 81.7761  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1675: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2425 Val_Loss: 81.2642  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1676: Validation loss did not decrease\n",
            "\t Train_Loss: 17.0357 Val_Loss: 83.1247  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1677: Validation loss did not decrease\n",
            "\t Train_Loss: 17.9083 Val_Loss: 82.2428  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1678: Validation loss did not decrease\n",
            "\t Train_Loss: 18.8701 Val_Loss: 82.1543  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1679: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1223 Val_Loss: 82.1450  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1680: Validation loss did not decrease\n",
            "\t Train_Loss: 17.0110 Val_Loss: 80.7257  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1681: Validation loss did not decrease\n",
            "\t Train_Loss: 17.5977 Val_Loss: 82.5378  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1682: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2843 Val_Loss: 81.6270  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1683: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5620 Val_Loss: 80.1321  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1684: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1517 Val_Loss: 82.2940  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1685: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9459 Val_Loss: 80.8197  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1686: Validation loss did not decrease\n",
            "\t Train_Loss: 15.5334 Val_Loss: 81.8875  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1687: Validation loss did not decrease\n",
            "\t Train_Loss: 16.7352 Val_Loss: 83.5666  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1688: Validation loss did not decrease\n",
            "\t Train_Loss: 15.8875 Val_Loss: 81.3034  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1689: Validation loss did not decrease\n",
            "\t Train_Loss: 14.6906 Val_Loss: 82.2102  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1690: Validation loss did not decrease\n",
            "\t Train_Loss: 15.9009 Val_Loss: 83.2605  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1691: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7866 Val_Loss: 83.0616  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1692: Validation loss did not decrease\n",
            "\t Train_Loss: 14.9801 Val_Loss: 80.3379  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1693: Validation loss did not decrease\n",
            "\t Train_Loss: 14.5892 Val_Loss: 79.4399  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1694: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1865 Val_Loss: 81.0404  BEST VAL Loss: 79.1935\n",
            "\n",
            "Epoch 1695: Validation loss decreased (79.193542 --> 78.891685).\n",
            "\t Train_Loss: 14.5449 Val_Loss: 78.8917  BEST VAL Loss: 78.8917\n",
            "\n",
            "Epoch 1696: Validation loss decreased (78.891685 --> 78.770935).\n",
            "\t Train_Loss: 14.1345 Val_Loss: 78.7709  BEST VAL Loss: 78.7709\n",
            "\n",
            "Epoch 1697: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3043 Val_Loss: 80.6750  BEST VAL Loss: 78.7709\n",
            "\n",
            "Epoch 1698: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4874 Val_Loss: 80.2455  BEST VAL Loss: 78.7709\n",
            "\n",
            "Epoch 1699: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9111 Val_Loss: 80.9752  BEST VAL Loss: 78.7709\n",
            "\n",
            "Epoch 1700: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8835 Val_Loss: 80.3780  BEST VAL Loss: 78.7709\n",
            "\n",
            "Epoch 1701: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0696 Val_Loss: 80.1455  BEST VAL Loss: 78.7709\n",
            "\n",
            "Epoch 1702: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0362 Val_Loss: 78.9612  BEST VAL Loss: 78.7709\n",
            "\n",
            "Epoch 1703: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0143 Val_Loss: 79.0122  BEST VAL Loss: 78.7709\n",
            "\n",
            "Epoch 1704: Validation loss decreased (78.770935 --> 78.346924).\n",
            "\t Train_Loss: 13.7568 Val_Loss: 78.3469  BEST VAL Loss: 78.3469\n",
            "\n",
            "Epoch 1705: Validation loss decreased (78.346924 --> 76.430725).\n",
            "\t Train_Loss: 13.6163 Val_Loss: 76.4307  BEST VAL Loss: 76.4307\n",
            "\n",
            "Epoch 1706: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9487 Val_Loss: 78.9613  BEST VAL Loss: 76.4307\n",
            "\n",
            "Epoch 1707: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6914 Val_Loss: 78.6123  BEST VAL Loss: 76.4307\n",
            "\n",
            "Epoch 1708: Validation loss did not decrease\n",
            "\t Train_Loss: 13.4859 Val_Loss: 78.3771  BEST VAL Loss: 76.4307\n",
            "\n",
            "Epoch 1709: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8389 Val_Loss: 79.1008  BEST VAL Loss: 76.4307\n",
            "\n",
            "Epoch 1710: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0218 Val_Loss: 78.3362  BEST VAL Loss: 76.4307\n",
            "\n",
            "Epoch 1711: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3058 Val_Loss: 77.7657  BEST VAL Loss: 76.4307\n",
            "\n",
            "Epoch 1712: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8651 Val_Loss: 78.2673  BEST VAL Loss: 76.4307\n",
            "\n",
            "Epoch 1713: Validation loss decreased (76.430725 --> 76.215096).\n",
            "\t Train_Loss: 13.7327 Val_Loss: 76.2151  BEST VAL Loss: 76.2151\n",
            "\n",
            "Epoch 1714: Validation loss decreased (76.215096 --> 75.581284).\n",
            "\t Train_Loss: 13.4151 Val_Loss: 75.5813  BEST VAL Loss: 75.5813\n",
            "\n",
            "Epoch 1715: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8143 Val_Loss: 78.6604  BEST VAL Loss: 75.5813\n",
            "\n",
            "Epoch 1716: Validation loss did not decrease\n",
            "\t Train_Loss: 14.5584 Val_Loss: 77.0323  BEST VAL Loss: 75.5813\n",
            "\n",
            "Epoch 1717: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2510 Val_Loss: 76.1957  BEST VAL Loss: 75.5813\n",
            "\n",
            "Epoch 1718: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5877 Val_Loss: 78.4065  BEST VAL Loss: 75.5813\n",
            "\n",
            "Epoch 1719: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1413 Val_Loss: 77.3422  BEST VAL Loss: 75.5813\n",
            "\n",
            "Epoch 1720: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2688 Val_Loss: 76.8257  BEST VAL Loss: 75.5813\n",
            "\n",
            "Epoch 1721: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7922 Val_Loss: 77.5772  BEST VAL Loss: 75.5813\n",
            "\n",
            "Epoch 1722: Validation loss decreased (75.581284 --> 75.261246).\n",
            "\t Train_Loss: 13.9977 Val_Loss: 75.2612  BEST VAL Loss: 75.2612\n",
            "\n",
            "Epoch 1723: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0991 Val_Loss: 76.0346  BEST VAL Loss: 75.2612\n",
            "\n",
            "Epoch 1724: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1666 Val_Loss: 77.1806  BEST VAL Loss: 75.2612\n",
            "\n",
            "Epoch 1725: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9916 Val_Loss: 76.3295  BEST VAL Loss: 75.2612\n",
            "\n",
            "Epoch 1726: Validation loss decreased (75.261246 --> 74.292580).\n",
            "\t Train_Loss: 12.8962 Val_Loss: 74.2926  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1727: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4805 Val_Loss: 77.2004  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1728: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5767 Val_Loss: 76.1410  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1729: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5228 Val_Loss: 76.9304  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1730: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9388 Val_Loss: 75.6731  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1731: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1035 Val_Loss: 76.3070  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1732: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5494 Val_Loss: 75.0602  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1733: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2905 Val_Loss: 74.9062  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1734: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0510 Val_Loss: 75.1735  BEST VAL Loss: 74.2926\n",
            "\n",
            "Epoch 1735: Validation loss decreased (74.292580 --> 73.006348).\n",
            "\t Train_Loss: 13.1563 Val_Loss: 73.0063  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1736: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1380 Val_Loss: 74.6359  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1737: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9331 Val_Loss: 75.4301  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1738: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0871 Val_Loss: 74.9036  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1739: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6839 Val_Loss: 74.1233  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1740: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9378 Val_Loss: 74.7487  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1741: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6561 Val_Loss: 74.5937  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1742: Validation loss did not decrease\n",
            "\t Train_Loss: 12.8347 Val_Loss: 74.1837  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1743: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6356 Val_Loss: 73.9099  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1744: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6838 Val_Loss: 73.0315  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1745: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5921 Val_Loss: 74.1325  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1746: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5698 Val_Loss: 74.0706  BEST VAL Loss: 73.0063\n",
            "\n",
            "Epoch 1747: Validation loss decreased (73.006348 --> 72.760612).\n",
            "\t Train_Loss: 12.5844 Val_Loss: 72.7606  BEST VAL Loss: 72.7606\n",
            "\n",
            "Epoch 1748: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4369 Val_Loss: 73.0283  BEST VAL Loss: 72.7606\n",
            "\n",
            "Epoch 1749: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5392 Val_Loss: 73.4050  BEST VAL Loss: 72.7606\n",
            "\n",
            "Epoch 1750: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3821 Val_Loss: 73.5590  BEST VAL Loss: 72.7606\n",
            "\n",
            "Epoch 1751: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4487 Val_Loss: 73.4990  BEST VAL Loss: 72.7606\n",
            "\n",
            "Epoch 1752: Validation loss decreased (72.760612 --> 72.574730).\n",
            "\t Train_Loss: 12.3634 Val_Loss: 72.5747  BEST VAL Loss: 72.5747\n",
            "\n",
            "Epoch 1753: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4226 Val_Loss: 73.3884  BEST VAL Loss: 72.5747\n",
            "\n",
            "Epoch 1754: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2931 Val_Loss: 73.1889  BEST VAL Loss: 72.5747\n",
            "\n",
            "Epoch 1755: Validation loss decreased (72.574730 --> 72.290062).\n",
            "\t Train_Loss: 12.3308 Val_Loss: 72.2901  BEST VAL Loss: 72.2901\n",
            "\n",
            "Epoch 1756: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2497 Val_Loss: 72.3549  BEST VAL Loss: 72.2901\n",
            "\n",
            "Epoch 1757: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3070 Val_Loss: 72.3224  BEST VAL Loss: 72.2901\n",
            "\n",
            "Epoch 1758: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2490 Val_Loss: 72.7196  BEST VAL Loss: 72.2901\n",
            "\n",
            "Epoch 1759: Validation loss decreased (72.290062 --> 72.037926).\n",
            "\t Train_Loss: 12.2437 Val_Loss: 72.0379  BEST VAL Loss: 72.0379\n",
            "\n",
            "Epoch 1760: Validation loss decreased (72.037926 --> 71.957649).\n",
            "\t Train_Loss: 12.1782 Val_Loss: 71.9576  BEST VAL Loss: 71.9576\n",
            "\n",
            "Epoch 1761: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1770 Val_Loss: 72.6417  BEST VAL Loss: 71.9576\n",
            "\n",
            "Epoch 1762: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1683 Val_Loss: 72.2796  BEST VAL Loss: 71.9576\n",
            "\n",
            "Epoch 1763: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1617 Val_Loss: 72.0928  BEST VAL Loss: 71.9576\n",
            "\n",
            "Epoch 1764: Validation loss decreased (71.957649 --> 71.607964).\n",
            "\t Train_Loss: 12.1409 Val_Loss: 71.6080  BEST VAL Loss: 71.6080\n",
            "\n",
            "Epoch 1765: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1032 Val_Loss: 72.0416  BEST VAL Loss: 71.6080\n",
            "\n",
            "Epoch 1766: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0974 Val_Loss: 71.6425  BEST VAL Loss: 71.6080\n",
            "\n",
            "Epoch 1767: Validation loss decreased (71.607964 --> 71.207977).\n",
            "\t Train_Loss: 12.0608 Val_Loss: 71.2080  BEST VAL Loss: 71.2080\n",
            "\n",
            "Epoch 1768: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0663 Val_Loss: 71.5380  BEST VAL Loss: 71.2080\n",
            "\n",
            "Epoch 1769: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0295 Val_Loss: 71.5025  BEST VAL Loss: 71.2080\n",
            "\n",
            "Epoch 1770: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0463 Val_Loss: 71.6664  BEST VAL Loss: 71.2080\n",
            "\n",
            "Epoch 1771: Validation loss decreased (71.207977 --> 71.053581).\n",
            "\t Train_Loss: 12.0147 Val_Loss: 71.0536  BEST VAL Loss: 71.0536\n",
            "\n",
            "Epoch 1772: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0166 Val_Loss: 71.5004  BEST VAL Loss: 71.0536\n",
            "\n",
            "Epoch 1773: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9802 Val_Loss: 71.2655  BEST VAL Loss: 71.0536\n",
            "\n",
            "Epoch 1774: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9734 Val_Loss: 71.0764  BEST VAL Loss: 71.0536\n",
            "\n",
            "Epoch 1775: Validation loss decreased (71.053581 --> 70.793175).\n",
            "\t Train_Loss: 11.9419 Val_Loss: 70.7932  BEST VAL Loss: 70.7932\n",
            "\n",
            "Epoch 1776: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9378 Val_Loss: 70.8624  BEST VAL Loss: 70.7932\n",
            "\n",
            "Epoch 1777: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9182 Val_Loss: 70.9563  BEST VAL Loss: 70.7932\n",
            "\n",
            "Epoch 1778: Validation loss decreased (70.793175 --> 70.526566).\n",
            "\t Train_Loss: 11.9111 Val_Loss: 70.5266  BEST VAL Loss: 70.5266\n",
            "\n",
            "Epoch 1779: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8983 Val_Loss: 70.7606  BEST VAL Loss: 70.5266\n",
            "\n",
            "Epoch 1780: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8880 Val_Loss: 70.6154  BEST VAL Loss: 70.5266\n",
            "\n",
            "Epoch 1781: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8821 Val_Loss: 70.8301  BEST VAL Loss: 70.5266\n",
            "\n",
            "Epoch 1782: Validation loss decreased (70.526566 --> 70.269142).\n",
            "\t Train_Loss: 11.8674 Val_Loss: 70.2691  BEST VAL Loss: 70.2691\n",
            "\n",
            "Epoch 1783: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8586 Val_Loss: 70.5890  BEST VAL Loss: 70.2691\n",
            "\n",
            "Epoch 1784: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8407 Val_Loss: 70.2822  BEST VAL Loss: 70.2691\n",
            "\n",
            "Epoch 1785: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8319 Val_Loss: 70.3934  BEST VAL Loss: 70.2691\n",
            "\n",
            "Epoch 1786: Validation loss decreased (70.269142 --> 69.937431).\n",
            "\t Train_Loss: 11.8120 Val_Loss: 69.9374  BEST VAL Loss: 69.9374\n",
            "\n",
            "Epoch 1787: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8021 Val_Loss: 70.2443  BEST VAL Loss: 69.9374\n",
            "\n",
            "Epoch 1788: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7865 Val_Loss: 69.9862  BEST VAL Loss: 69.9374\n",
            "\n",
            "Epoch 1789: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7778 Val_Loss: 70.1040  BEST VAL Loss: 69.9374\n",
            "\n",
            "Epoch 1790: Validation loss decreased (69.937431 --> 69.735283).\n",
            "\t Train_Loss: 11.7645 Val_Loss: 69.7353  BEST VAL Loss: 69.7353\n",
            "\n",
            "Epoch 1791: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7570 Val_Loss: 70.1170  BEST VAL Loss: 69.7353\n",
            "\n",
            "Epoch 1792: Validation loss decreased (69.735283 --> 69.675621).\n",
            "\t Train_Loss: 11.7494 Val_Loss: 69.6756  BEST VAL Loss: 69.6756\n",
            "\n",
            "Epoch 1793: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7438 Val_Loss: 69.9466  BEST VAL Loss: 69.6756\n",
            "\n",
            "Epoch 1794: Validation loss decreased (69.675621 --> 69.402527).\n",
            "\t Train_Loss: 11.7372 Val_Loss: 69.4025  BEST VAL Loss: 69.4025\n",
            "\n",
            "Epoch 1795: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7366 Val_Loss: 69.9636  BEST VAL Loss: 69.4025\n",
            "\n",
            "Epoch 1796: Validation loss decreased (69.402527 --> 69.244461).\n",
            "\t Train_Loss: 11.7365 Val_Loss: 69.2445  BEST VAL Loss: 69.2445\n",
            "\n",
            "Epoch 1797: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7395 Val_Loss: 69.8000  BEST VAL Loss: 69.2445\n",
            "\n",
            "Epoch 1798: Validation loss decreased (69.244461 --> 69.085670).\n",
            "\t Train_Loss: 11.7306 Val_Loss: 69.0857  BEST VAL Loss: 69.0857\n",
            "\n",
            "Epoch 1799: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7294 Val_Loss: 69.7118  BEST VAL Loss: 69.0857\n",
            "\n",
            "Epoch 1800: Validation loss decreased (69.085670 --> 69.069389).\n",
            "\t Train_Loss: 11.6964 Val_Loss: 69.0694  BEST VAL Loss: 69.0694\n",
            "\n",
            "Epoch 1801: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6700 Val_Loss: 69.3832  BEST VAL Loss: 69.0694\n",
            "\n",
            "Epoch 1802: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6351 Val_Loss: 69.1311  BEST VAL Loss: 69.0694\n",
            "\n",
            "Epoch 1803: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6123 Val_Loss: 69.1698  BEST VAL Loss: 69.0694\n",
            "\n",
            "Epoch 1804: Validation loss decreased (69.069389 --> 69.021233).\n",
            "\t Train_Loss: 11.5981 Val_Loss: 69.0212  BEST VAL Loss: 69.0212\n",
            "\n",
            "Epoch 1805: Validation loss decreased (69.021233 --> 68.800011).\n",
            "\t Train_Loss: 11.5875 Val_Loss: 68.8000  BEST VAL Loss: 68.8000\n",
            "\n",
            "Epoch 1806: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5807 Val_Loss: 69.0091  BEST VAL Loss: 68.8000\n",
            "\n",
            "Epoch 1807: Validation loss decreased (68.800011 --> 68.575867).\n",
            "\t Train_Loss: 11.5788 Val_Loss: 68.5759  BEST VAL Loss: 68.5759\n",
            "\n",
            "Epoch 1808: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5868 Val_Loss: 68.9590  BEST VAL Loss: 68.5759\n",
            "\n",
            "Epoch 1809: Validation loss decreased (68.575867 --> 68.478310).\n",
            "\t Train_Loss: 11.5821 Val_Loss: 68.4783  BEST VAL Loss: 68.4783\n",
            "\n",
            "Epoch 1810: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5751 Val_Loss: 68.7512  BEST VAL Loss: 68.4783\n",
            "\n",
            "Epoch 1811: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5343 Val_Loss: 68.7346  BEST VAL Loss: 68.4783\n",
            "\n",
            "Epoch 1812: Validation loss decreased (68.478310 --> 68.395409).\n",
            "\t Train_Loss: 11.5173 Val_Loss: 68.3954  BEST VAL Loss: 68.3954\n",
            "\n",
            "Epoch 1813: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5526 Val_Loss: 69.3894  BEST VAL Loss: 68.3954\n",
            "\n",
            "Epoch 1814: Validation loss decreased (68.395409 --> 68.066399).\n",
            "\t Train_Loss: 11.7340 Val_Loss: 68.0664  BEST VAL Loss: 68.0664\n",
            "\n",
            "Epoch 1815: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9245 Val_Loss: 69.4672  BEST VAL Loss: 68.0664\n",
            "\n",
            "Epoch 1816: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7573 Val_Loss: 68.0926  BEST VAL Loss: 68.0664\n",
            "\n",
            "Epoch 1817: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5640 Val_Loss: 68.1955  BEST VAL Loss: 68.0664\n",
            "\n",
            "Epoch 1818: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4636 Val_Loss: 68.0790  BEST VAL Loss: 68.0664\n",
            "\n",
            "Epoch 1819: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4497 Val_Loss: 68.3804  BEST VAL Loss: 68.0664\n",
            "\n",
            "Epoch 1820: Validation loss decreased (68.066399 --> 67.679031).\n",
            "\t Train_Loss: 11.4475 Val_Loss: 67.6790  BEST VAL Loss: 67.6790\n",
            "\n",
            "Epoch 1821: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5243 Val_Loss: 69.1499  BEST VAL Loss: 67.6790\n",
            "\n",
            "Epoch 1822: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8044 Val_Loss: 67.8515  BEST VAL Loss: 67.6790\n",
            "\n",
            "Epoch 1823: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0463 Val_Loss: 68.8198  BEST VAL Loss: 67.6790\n",
            "\n",
            "Epoch 1824: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5426 Val_Loss: 68.1810  BEST VAL Loss: 67.6790\n",
            "\n",
            "Epoch 1825: Validation loss decreased (67.679031 --> 67.243179).\n",
            "\t Train_Loss: 11.4343 Val_Loss: 67.2432  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1826: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0612 Val_Loss: 69.3406  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1827: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1469 Val_Loss: 67.8293  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1828: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6668 Val_Loss: 67.5611  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1829: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4210 Val_Loss: 69.0301  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1830: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0771 Val_Loss: 67.9830  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1831: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5214 Val_Loss: 68.0290  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1832: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4344 Val_Loss: 68.2591  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1833: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8687 Val_Loss: 67.3259  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1834: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7029 Val_Loss: 68.3283  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1835: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6173 Val_Loss: 69.3699  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1836: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7808 Val_Loss: 68.6978  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1837: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2097 Val_Loss: 67.9269  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1838: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5000 Val_Loss: 69.0845  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1839: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1074 Val_Loss: 68.1866  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1840: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1221 Val_Loss: 67.4586  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1841: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1201 Val_Loss: 68.4108  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1842: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3078 Val_Loss: 67.2912  BEST VAL Loss: 67.2432\n",
            "\n",
            "Epoch 1843: Validation loss decreased (67.243179 --> 66.914787).\n",
            "\t Train_Loss: 11.4044 Val_Loss: 66.9148  BEST VAL Loss: 66.9148\n",
            "\n",
            "Epoch 1844: Validation loss decreased (66.914787 --> 66.859673).\n",
            "\t Train_Loss: 11.8480 Val_Loss: 66.8597  BEST VAL Loss: 66.8597\n",
            "\n",
            "Epoch 1845: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3694 Val_Loss: 66.9522  BEST VAL Loss: 66.8597\n",
            "\n",
            "Epoch 1846: Validation loss decreased (66.859673 --> 66.407982).\n",
            "\t Train_Loss: 11.5855 Val_Loss: 66.4080  BEST VAL Loss: 66.4080\n",
            "\n",
            "Epoch 1847: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6976 Val_Loss: 66.7478  BEST VAL Loss: 66.4080\n",
            "\n",
            "Epoch 1848: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3934 Val_Loss: 67.6441  BEST VAL Loss: 66.4080\n",
            "\n",
            "Epoch 1849: Validation loss decreased (66.407982 --> 66.025200).\n",
            "\t Train_Loss: 11.7088 Val_Loss: 66.0252  BEST VAL Loss: 66.0252\n",
            "\n",
            "Epoch 1850: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4538 Val_Loss: 66.5594  BEST VAL Loss: 66.0252\n",
            "\n",
            "Epoch 1851: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3368 Val_Loss: 67.3930  BEST VAL Loss: 66.0252\n",
            "\n",
            "Epoch 1852: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4645 Val_Loss: 66.6001  BEST VAL Loss: 66.0252\n",
            "\n",
            "Epoch 1853: Validation loss decreased (66.025200 --> 65.718079).\n",
            "\t Train_Loss: 11.2274 Val_Loss: 65.7181  BEST VAL Loss: 65.7181\n",
            "\n",
            "Epoch 1854: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5394 Val_Loss: 66.9320  BEST VAL Loss: 65.7181\n",
            "\n",
            "Epoch 1855: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3206 Val_Loss: 66.7422  BEST VAL Loss: 65.7181\n",
            "\n",
            "Epoch 1856: Validation loss decreased (65.718079 --> 65.448059).\n",
            "\t Train_Loss: 11.2878 Val_Loss: 65.4481  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1857: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3966 Val_Loss: 66.1054  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1858: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1435 Val_Loss: 66.4595  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1859: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1959 Val_Loss: 65.8301  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1860: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4305 Val_Loss: 66.4383  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1861: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1027 Val_Loss: 66.7852  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1862: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2512 Val_Loss: 65.7887  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1863: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3895 Val_Loss: 66.0214  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1864: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0568 Val_Loss: 66.6589  BEST VAL Loss: 65.4481\n",
            "\n",
            "Epoch 1865: Validation loss decreased (65.448059 --> 65.030190).\n",
            "\t Train_Loss: 11.4442 Val_Loss: 65.0302  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1866: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4345 Val_Loss: 65.4605  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1867: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0536 Val_Loss: 66.6838  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1868: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5434 Val_Loss: 65.2334  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1869: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3170 Val_Loss: 65.2542  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1870: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0742 Val_Loss: 66.6311  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1871: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5840 Val_Loss: 65.3322  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1872: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1707 Val_Loss: 65.1139  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1873: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1258 Val_Loss: 66.1838  BEST VAL Loss: 65.0302\n",
            "\n",
            "Epoch 1874: Validation loss decreased (65.030190 --> 64.966103).\n",
            "\t Train_Loss: 11.4336 Val_Loss: 64.9661  BEST VAL Loss: 64.9661\n",
            "\n",
            "Epoch 1875: Validation loss decreased (64.966103 --> 64.826462).\n",
            "\t Train_Loss: 11.0087 Val_Loss: 64.8265  BEST VAL Loss: 64.8265\n",
            "\n",
            "Epoch 1876: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1329 Val_Loss: 65.8013  BEST VAL Loss: 64.8265\n",
            "\n",
            "Epoch 1877: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2692 Val_Loss: 64.8464  BEST VAL Loss: 64.8265\n",
            "\n",
            "Epoch 1878: Validation loss decreased (64.826462 --> 64.647011).\n",
            "\t Train_Loss: 10.9399 Val_Loss: 64.6470  BEST VAL Loss: 64.6470\n",
            "\n",
            "Epoch 1879: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1413 Val_Loss: 65.6405  BEST VAL Loss: 64.6470\n",
            "\n",
            "Epoch 1880: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1497 Val_Loss: 65.1117  BEST VAL Loss: 64.6470\n",
            "\n",
            "Epoch 1881: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8995 Val_Loss: 64.6641  BEST VAL Loss: 64.6470\n",
            "\n",
            "Epoch 1882: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1380 Val_Loss: 65.3354  BEST VAL Loss: 64.6470\n",
            "\n",
            "Epoch 1883: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0265 Val_Loss: 64.9049  BEST VAL Loss: 64.6470\n",
            "\n",
            "Epoch 1884: Validation loss decreased (64.647011 --> 64.490105).\n",
            "\t Train_Loss: 10.8542 Val_Loss: 64.4901  BEST VAL Loss: 64.4901\n",
            "\n",
            "Epoch 1885: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9863 Val_Loss: 65.0385  BEST VAL Loss: 64.4901\n",
            "\n",
            "Epoch 1886: Validation loss decreased (64.490105 --> 64.423355).\n",
            "\t Train_Loss: 10.9788 Val_Loss: 64.4234  BEST VAL Loss: 64.4234\n",
            "\n",
            "Epoch 1887: Validation loss decreased (64.423355 --> 64.223282).\n",
            "\t Train_Loss: 10.8262 Val_Loss: 64.2233  BEST VAL Loss: 64.2233\n",
            "\n",
            "Epoch 1888: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8771 Val_Loss: 64.8853  BEST VAL Loss: 64.2233\n",
            "\n",
            "Epoch 1889: Validation loss decreased (64.223282 --> 64.160080).\n",
            "\t Train_Loss: 10.9619 Val_Loss: 64.1601  BEST VAL Loss: 64.1601\n",
            "\n",
            "Epoch 1890: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8266 Val_Loss: 64.2469  BEST VAL Loss: 64.1601\n",
            "\n",
            "Epoch 1891: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7917 Val_Loss: 64.6753  BEST VAL Loss: 64.1601\n",
            "\n",
            "Epoch 1892: Validation loss decreased (64.160080 --> 64.024651).\n",
            "\t Train_Loss: 10.8642 Val_Loss: 64.0247  BEST VAL Loss: 64.0247\n",
            "\n",
            "Epoch 1893: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9346 Val_Loss: 64.5045  BEST VAL Loss: 64.0247\n",
            "\n",
            "Epoch 1894: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7952 Val_Loss: 64.2233  BEST VAL Loss: 64.0247\n",
            "\n",
            "Epoch 1895: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7416 Val_Loss: 64.1080  BEST VAL Loss: 64.0247\n",
            "\n",
            "Epoch 1896: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7481 Val_Loss: 64.3750  BEST VAL Loss: 64.0247\n",
            "\n",
            "Epoch 1897: Validation loss decreased (64.024651 --> 63.640156).\n",
            "\t Train_Loss: 10.7755 Val_Loss: 63.6402  BEST VAL Loss: 63.6402\n",
            "\n",
            "Epoch 1898: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8561 Val_Loss: 64.1657  BEST VAL Loss: 63.6402\n",
            "\n",
            "Epoch 1899: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7583 Val_Loss: 63.8755  BEST VAL Loss: 63.6402\n",
            "\n",
            "Epoch 1900: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7023 Val_Loss: 63.9449  BEST VAL Loss: 63.6402\n",
            "\n",
            "Epoch 1901: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6871 Val_Loss: 63.9577  BEST VAL Loss: 63.6402\n",
            "\n",
            "Epoch 1902: Validation loss decreased (63.640156 --> 63.497620).\n",
            "\t Train_Loss: 10.6846 Val_Loss: 63.4976  BEST VAL Loss: 63.4976\n",
            "\n",
            "Epoch 1903: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7249 Val_Loss: 64.0069  BEST VAL Loss: 63.4976\n",
            "\n",
            "Epoch 1904: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7187 Val_Loss: 63.5133  BEST VAL Loss: 63.4976\n",
            "\n",
            "Epoch 1905: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7425 Val_Loss: 63.9205  BEST VAL Loss: 63.4976\n",
            "\n",
            "Epoch 1906: Validation loss decreased (63.497620 --> 63.411316).\n",
            "\t Train_Loss: 10.7093 Val_Loss: 63.4113  BEST VAL Loss: 63.4113\n",
            "\n",
            "Epoch 1907: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6560 Val_Loss: 63.6577  BEST VAL Loss: 63.4113\n",
            "\n",
            "Epoch 1908: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6271 Val_Loss: 63.4272  BEST VAL Loss: 63.4113\n",
            "\n",
            "Epoch 1909: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6001 Val_Loss: 63.4153  BEST VAL Loss: 63.4113\n",
            "\n",
            "Epoch 1910: Validation loss decreased (63.411316 --> 63.406368).\n",
            "\t Train_Loss: 10.5916 Val_Loss: 63.4064  BEST VAL Loss: 63.4064\n",
            "\n",
            "Epoch 1911: Validation loss decreased (63.406368 --> 63.273998).\n",
            "\t Train_Loss: 10.5808 Val_Loss: 63.2740  BEST VAL Loss: 63.2740\n",
            "\n",
            "Epoch 1912: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5744 Val_Loss: 63.3676  BEST VAL Loss: 63.2740\n",
            "\n",
            "Epoch 1913: Validation loss decreased (63.273998 --> 63.050716).\n",
            "\t Train_Loss: 10.5754 Val_Loss: 63.0507  BEST VAL Loss: 63.0507\n",
            "\n",
            "Epoch 1914: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5860 Val_Loss: 63.4768  BEST VAL Loss: 63.0507\n",
            "\n",
            "Epoch 1915: Validation loss decreased (63.050716 --> 63.008766).\n",
            "\t Train_Loss: 10.6059 Val_Loss: 63.0088  BEST VAL Loss: 63.0088\n",
            "\n",
            "Epoch 1916: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6847 Val_Loss: 63.5287  BEST VAL Loss: 63.0088\n",
            "\n",
            "Epoch 1917: Validation loss decreased (63.008766 --> 62.926575).\n",
            "\t Train_Loss: 10.6764 Val_Loss: 62.9266  BEST VAL Loss: 62.9266\n",
            "\n",
            "Epoch 1918: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7148 Val_Loss: 63.4181  BEST VAL Loss: 62.9266\n",
            "\n",
            "Epoch 1919: Validation loss decreased (62.926575 --> 62.881897).\n",
            "\t Train_Loss: 10.6006 Val_Loss: 62.8819  BEST VAL Loss: 62.8819\n",
            "\n",
            "Epoch 1920: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5645 Val_Loss: 63.1360  BEST VAL Loss: 62.8819\n",
            "\n",
            "Epoch 1921: Validation loss decreased (62.881897 --> 62.751888).\n",
            "\t Train_Loss: 10.5138 Val_Loss: 62.7519  BEST VAL Loss: 62.7519\n",
            "\n",
            "Epoch 1922: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4917 Val_Loss: 62.9032  BEST VAL Loss: 62.7519\n",
            "\n",
            "Epoch 1923: Validation loss decreased (62.751888 --> 62.630970).\n",
            "\t Train_Loss: 10.4733 Val_Loss: 62.6310  BEST VAL Loss: 62.6310\n",
            "\n",
            "Epoch 1924: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4656 Val_Loss: 62.7933  BEST VAL Loss: 62.6310\n",
            "\n",
            "Epoch 1925: Validation loss decreased (62.630970 --> 62.477943).\n",
            "\t Train_Loss: 10.4613 Val_Loss: 62.4779  BEST VAL Loss: 62.4779\n",
            "\n",
            "Epoch 1926: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4756 Val_Loss: 62.8856  BEST VAL Loss: 62.4779\n",
            "\n",
            "Epoch 1927: Validation loss decreased (62.477943 --> 62.442402).\n",
            "\t Train_Loss: 10.4905 Val_Loss: 62.4424  BEST VAL Loss: 62.4424\n",
            "\n",
            "Epoch 1928: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5519 Val_Loss: 62.9970  BEST VAL Loss: 62.4424\n",
            "\n",
            "Epoch 1929: Validation loss decreased (62.442402 --> 62.418934).\n",
            "\t Train_Loss: 10.5659 Val_Loss: 62.4189  BEST VAL Loss: 62.4189\n",
            "\n",
            "Epoch 1930: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5711 Val_Loss: 62.8416  BEST VAL Loss: 62.4189\n",
            "\n",
            "Epoch 1931: Validation loss decreased (62.418934 --> 62.250343).\n",
            "\t Train_Loss: 10.5065 Val_Loss: 62.2503  BEST VAL Loss: 62.2503\n",
            "\n",
            "Epoch 1932: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4869 Val_Loss: 62.6226  BEST VAL Loss: 62.2503\n",
            "\n",
            "Epoch 1933: Validation loss decreased (62.250343 --> 62.161793).\n",
            "\t Train_Loss: 10.4351 Val_Loss: 62.1618  BEST VAL Loss: 62.1618\n",
            "\n",
            "Epoch 1934: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4187 Val_Loss: 62.4701  BEST VAL Loss: 62.1618\n",
            "\n",
            "Epoch 1935: Validation loss decreased (62.161793 --> 62.058704).\n",
            "\t Train_Loss: 10.3964 Val_Loss: 62.0587  BEST VAL Loss: 62.0587\n",
            "\n",
            "Epoch 1936: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3958 Val_Loss: 62.4264  BEST VAL Loss: 62.0587\n",
            "\n",
            "Epoch 1937: Validation loss decreased (62.058704 --> 62.018307).\n",
            "\t Train_Loss: 10.3884 Val_Loss: 62.0183  BEST VAL Loss: 62.0183\n",
            "\n",
            "Epoch 1938: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3984 Val_Loss: 62.4053  BEST VAL Loss: 62.0183\n",
            "\n",
            "Epoch 1939: Validation loss decreased (62.018307 --> 61.941505).\n",
            "\t Train_Loss: 10.3972 Val_Loss: 61.9415  BEST VAL Loss: 61.9415\n",
            "\n",
            "Epoch 1940: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4068 Val_Loss: 62.3485  BEST VAL Loss: 61.9415\n",
            "\n",
            "Epoch 1941: Validation loss decreased (61.941505 --> 61.864319).\n",
            "\t Train_Loss: 10.3945 Val_Loss: 61.8643  BEST VAL Loss: 61.8643\n",
            "\n",
            "Epoch 1942: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3763 Val_Loss: 62.1818  BEST VAL Loss: 61.8643\n",
            "\n",
            "Epoch 1943: Validation loss decreased (61.864319 --> 61.754616).\n",
            "\t Train_Loss: 10.3478 Val_Loss: 61.7546  BEST VAL Loss: 61.7546\n",
            "\n",
            "Epoch 1944: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3120 Val_Loss: 61.9788  BEST VAL Loss: 61.7546\n",
            "\n",
            "Epoch 1945: Validation loss decreased (61.754616 --> 61.717384).\n",
            "\t Train_Loss: 10.2779 Val_Loss: 61.7174  BEST VAL Loss: 61.7174\n",
            "\n",
            "Epoch 1946: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2444 Val_Loss: 61.7584  BEST VAL Loss: 61.7174\n",
            "\n",
            "Epoch 1947: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2227 Val_Loss: 61.7566  BEST VAL Loss: 61.7174\n",
            "\n",
            "Epoch 1948: Validation loss decreased (61.717384 --> 61.649727).\n",
            "\t Train_Loss: 10.2128 Val_Loss: 61.6497  BEST VAL Loss: 61.6497\n",
            "\n",
            "Epoch 1949: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2107 Val_Loss: 61.7778  BEST VAL Loss: 61.6497\n",
            "\n",
            "Epoch 1950: Validation loss decreased (61.649727 --> 61.570446).\n",
            "\t Train_Loss: 10.2093 Val_Loss: 61.5704  BEST VAL Loss: 61.5704\n",
            "\n",
            "Epoch 1951: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1974 Val_Loss: 61.6825  BEST VAL Loss: 61.5704\n",
            "\n",
            "Epoch 1952: Validation loss decreased (61.570446 --> 61.515491).\n",
            "\t Train_Loss: 10.1793 Val_Loss: 61.5155  BEST VAL Loss: 61.5155\n",
            "\n",
            "Epoch 1953: Validation loss decreased (61.515491 --> 61.478931).\n",
            "\t Train_Loss: 10.1565 Val_Loss: 61.4789  BEST VAL Loss: 61.4789\n",
            "\n",
            "Epoch 1954: Validation loss decreased (61.478931 --> 61.459461).\n",
            "\t Train_Loss: 10.1415 Val_Loss: 61.4595  BEST VAL Loss: 61.4595\n",
            "\n",
            "Epoch 1955: Validation loss decreased (61.459461 --> 61.353859).\n",
            "\t Train_Loss: 10.1349 Val_Loss: 61.3539  BEST VAL Loss: 61.3539\n",
            "\n",
            "Epoch 1956: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1296 Val_Loss: 61.3778  BEST VAL Loss: 61.3539\n",
            "\n",
            "Epoch 1957: Validation loss decreased (61.353859 --> 61.277863).\n",
            "\t Train_Loss: 10.1180 Val_Loss: 61.2779  BEST VAL Loss: 61.2779\n",
            "\n",
            "Epoch 1958: Validation loss decreased (61.277863 --> 61.255032).\n",
            "\t Train_Loss: 10.1017 Val_Loss: 61.2550  BEST VAL Loss: 61.2550\n",
            "\n",
            "Epoch 1959: Validation loss decreased (61.255032 --> 61.253223).\n",
            "\t Train_Loss: 10.0878 Val_Loss: 61.2532  BEST VAL Loss: 61.2532\n",
            "\n",
            "Epoch 1960: Validation loss decreased (61.253223 --> 61.169842).\n",
            "\t Train_Loss: 10.0777 Val_Loss: 61.1698  BEST VAL Loss: 61.1698\n",
            "\n",
            "Epoch 1961: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0663 Val_Loss: 61.1711  BEST VAL Loss: 61.1698\n",
            "\n",
            "Epoch 1962: Validation loss decreased (61.169842 --> 61.067287).\n",
            "\t Train_Loss: 10.0521 Val_Loss: 61.0673  BEST VAL Loss: 61.0673\n",
            "\n",
            "Epoch 1963: Validation loss decreased (61.067287 --> 61.014668).\n",
            "\t Train_Loss: 10.0367 Val_Loss: 61.0147  BEST VAL Loss: 61.0147\n",
            "\n",
            "Epoch 1964: Validation loss decreased (61.014668 --> 60.971020).\n",
            "\t Train_Loss: 10.0240 Val_Loss: 60.9710  BEST VAL Loss: 60.9710\n",
            "\n",
            "Epoch 1965: Validation loss decreased (60.971020 --> 60.871487).\n",
            "\t Train_Loss: 10.0143 Val_Loss: 60.8715  BEST VAL Loss: 60.8715\n",
            "\n",
            "Epoch 1966: Validation loss decreased (60.871487 --> 60.849220).\n",
            "\t Train_Loss: 10.0046 Val_Loss: 60.8492  BEST VAL Loss: 60.8492\n",
            "\n",
            "Epoch 1967: Validation loss decreased (60.849220 --> 60.771156).\n",
            "\t Train_Loss: 9.9928 Val_Loss: 60.7712  BEST VAL Loss: 60.7712\n",
            "\n",
            "Epoch 1968: Validation loss decreased (60.771156 --> 60.746777).\n",
            "\t Train_Loss: 9.9789 Val_Loss: 60.7468  BEST VAL Loss: 60.7468\n",
            "\n",
            "Epoch 1969: Validation loss decreased (60.746777 --> 60.718239).\n",
            "\t Train_Loss: 9.9646 Val_Loss: 60.7182  BEST VAL Loss: 60.7182\n",
            "\n",
            "Epoch 1970: Validation loss decreased (60.718239 --> 60.661945).\n",
            "\t Train_Loss: 9.9504 Val_Loss: 60.6619  BEST VAL Loss: 60.6619\n",
            "\n",
            "Epoch 1971: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9352 Val_Loss: 60.6796  BEST VAL Loss: 60.6619\n",
            "\n",
            "Epoch 1972: Validation loss decreased (60.661945 --> 60.605141).\n",
            "\t Train_Loss: 9.9182 Val_Loss: 60.6051  BEST VAL Loss: 60.6051\n",
            "\n",
            "Epoch 1973: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9008 Val_Loss: 60.6338  BEST VAL Loss: 60.6051\n",
            "\n",
            "Epoch 1974: Validation loss decreased (60.605141 --> 60.558693).\n",
            "\t Train_Loss: 9.8850 Val_Loss: 60.5587  BEST VAL Loss: 60.5587\n",
            "\n",
            "Epoch 1975: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8715 Val_Loss: 60.5975  BEST VAL Loss: 60.5587\n",
            "\n",
            "Epoch 1976: Validation loss decreased (60.558693 --> 60.523651).\n",
            "\t Train_Loss: 9.8597 Val_Loss: 60.5237  BEST VAL Loss: 60.5237\n",
            "\n",
            "Epoch 1977: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8486 Val_Loss: 60.6022  BEST VAL Loss: 60.5237\n",
            "\n",
            "Epoch 1978: Validation loss decreased (60.523651 --> 60.469807).\n",
            "\t Train_Loss: 9.8387 Val_Loss: 60.4698  BEST VAL Loss: 60.4698\n",
            "\n",
            "Epoch 1979: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8334 Val_Loss: 60.6409  BEST VAL Loss: 60.4698\n",
            "\n",
            "Epoch 1980: Validation loss decreased (60.469807 --> 60.310219).\n",
            "\t Train_Loss: 9.8400 Val_Loss: 60.3102  BEST VAL Loss: 60.3102\n",
            "\n",
            "Epoch 1981: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8848 Val_Loss: 60.8255  BEST VAL Loss: 60.3102\n",
            "\n",
            "Epoch 1982: Validation loss decreased (60.310219 --> 60.136219).\n",
            "\t Train_Loss: 9.9946 Val_Loss: 60.1362  BEST VAL Loss: 60.1362\n",
            "\n",
            "Epoch 1983: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1649 Val_Loss: 61.3165  BEST VAL Loss: 60.1362\n",
            "\n",
            "Epoch 1984: Validation loss decreased (60.136219 --> 59.863991).\n",
            "\t Train_Loss: 10.4901 Val_Loss: 59.8640  BEST VAL Loss: 59.8640\n",
            "\n",
            "Epoch 1985: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3982 Val_Loss: 61.1304  BEST VAL Loss: 59.8640\n",
            "\n",
            "Epoch 1986: Validation loss decreased (59.863991 --> 59.593788).\n",
            "\t Train_Loss: 10.4546 Val_Loss: 59.5938  BEST VAL Loss: 59.5938\n",
            "\n",
            "Epoch 1987: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0632 Val_Loss: 60.1577  BEST VAL Loss: 59.5938\n",
            "\n",
            "Epoch 1988: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8090 Val_Loss: 59.9808  BEST VAL Loss: 59.5938\n",
            "\n",
            "Epoch 1989: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7025 Val_Loss: 59.9556  BEST VAL Loss: 59.5938\n",
            "\n",
            "Epoch 1990: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7136 Val_Loss: 60.4004  BEST VAL Loss: 59.5938\n",
            "\n",
            "Epoch 1991: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8553 Val_Loss: 59.9302  BEST VAL Loss: 59.5938\n",
            "\n",
            "Epoch 1992: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0804 Val_Loss: 60.9373  BEST VAL Loss: 59.5938\n",
            "\n",
            "Epoch 1993: Validation loss decreased (59.593788 --> 59.426113).\n",
            "\t Train_Loss: 10.3413 Val_Loss: 59.4261  BEST VAL Loss: 59.4261\n",
            "\n",
            "Epoch 1994: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1100 Val_Loss: 59.9376  BEST VAL Loss: 59.4261\n",
            "\n",
            "Epoch 1995: Validation loss decreased (59.426113 --> 59.146301).\n",
            "\t Train_Loss: 9.8946 Val_Loss: 59.1463  BEST VAL Loss: 59.1463\n",
            "\n",
            "Epoch 1996: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7082 Val_Loss: 59.3120  BEST VAL Loss: 59.1463\n",
            "\n",
            "Epoch 1997: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6701 Val_Loss: 60.0458  BEST VAL Loss: 59.1463\n",
            "\n",
            "Epoch 1998: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8763 Val_Loss: 59.5390  BEST VAL Loss: 59.1463\n",
            "\n",
            "Epoch 1999: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0820 Val_Loss: 60.9020  BEST VAL Loss: 59.1463\n",
            "\n"
          ]
        }
      ],
      "source": [
        "GRU_best_model, train_losses, val_losses = trainer(GRU_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jaUYwd9vFHuC",
        "outputId": "95cab696-7e85-464e-cb5a-6ac771b3e84d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jcdsct5Y9a8"
      },
      "source": [
        "---\n",
        "### 5.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHWWy8uCY7Vg"
      },
      "outputs": [],
      "source": [
        "val_predict_GRU = GRU_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "wThiG7D-ZBA_",
        "outputId": "0caeb71a-b13f-47b3-d7e4-af73ce756636"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_GRU.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by GRU RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMYeUTH5Ki7"
      },
      "source": [
        "---\n",
        "---\n",
        "## 6 LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1qXDXQ9VfIW"
      },
      "source": [
        "---\n",
        "### 6.1 Define single RNN cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVP9ju7m39TL"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A simple LSTM cell network for educational AI-summer purposes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_length=10, hidden_size=20, bias=True):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.forget_gate_layer = nn.Linear(\n",
        "            input_length + hidden_size, hidden_size, bias=bias\n",
        "        )\n",
        "        self.input_gate_layer = nn.Linear(\n",
        "            input_length + hidden_size, hidden_size, bias=bias\n",
        "        )\n",
        "        self.cell_gate_layer = nn.Linear(\n",
        "            input_length + hidden_size, hidden_size, bias=bias\n",
        "        )\n",
        "        self.output_gate_layer = nn.Linear(\n",
        "            input_length + hidden_size, hidden_size, bias=bias\n",
        "        )\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "    def forget(self, x, h):\n",
        "        combined = torch.cat((x, h), 1)\n",
        "        fg = torch.sigmoid(self.forget_gate_layer(combined))\n",
        "        return fg\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "        combined = torch.cat((x, h), 1)\n",
        "        ig = torch.sigmoid(self.input_gate_layer(combined))\n",
        "        return ig\n",
        "\n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        combined = torch.cat((x, h), 1)\n",
        "        g = torch.tanh(self.cell_gate_layer(combined))\n",
        "        c_next = f * c_prev + i * g\n",
        "        return c_next\n",
        "\n",
        "    def output_gate(self, x, h, c):\n",
        "        combined = torch.cat((x, h), 1)\n",
        "        og = torch.sigmoid(self.output_gate_layer(combined))\n",
        "        return og\n",
        "\n",
        "    def forward(self, x, hx=None):\n",
        "        h_prev, c_prev = hx\n",
        "        i = self.input_gate(x, h_prev)\n",
        "        f = self.forget(x, h_prev)\n",
        "        c_next = self.cell_memory_gate(i, f, x, h_prev, c_prev)\n",
        "        o = self.output_gate(x, h_prev, c_next)\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "        return h_next, c_next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQpH0TuoVpVf"
      },
      "source": [
        "---\n",
        "### 6.2 LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PshO9vv35REY"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers, bias=bias, batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7G5_9S7VxJ0"
      },
      "source": [
        "---\n",
        "### 6.3 Train LSTM model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQTMa2Xj6maS",
        "outputId": "6039457e-cc99-4bb3-f80b-cf61f17db3fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (rnn_cell_list): ModuleList(\n",
              "    (0): LSTMCell(\n",
              "      (forget_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (forget_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "      (input_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (input_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "      (cell_memory_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (cell_memory_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "      (output_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (output_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LSTM_model = LSTM(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "LSTM_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuzlCGTrV_xH"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(LSTM_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwpdUK6Z6skV",
        "outputId": "ca3ca71a-1aff-4313-9c79-ece01a35f71f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 77: Validation loss decreased (6336.466309 --> 6289.252441).\n",
            "\t Train_Loss: 4316.1445 Val_Loss: 6289.2524  BEST VAL Loss: 6289.2524\n",
            "\n",
            "Epoch 78: Validation loss decreased (6289.252441 --> 6242.225586).\n",
            "\t Train_Loss: 4277.3428 Val_Loss: 6242.2256  BEST VAL Loss: 6242.2256\n",
            "\n",
            "Epoch 79: Validation loss decreased (6242.225586 --> 6194.985352).\n",
            "\t Train_Loss: 4238.4707 Val_Loss: 6194.9854  BEST VAL Loss: 6194.9854\n",
            "\n",
            "Epoch 80: Validation loss decreased (6194.985352 --> 6146.422852).\n",
            "\t Train_Loss: 4199.0459 Val_Loss: 6146.4229  BEST VAL Loss: 6146.4229\n",
            "\n",
            "Epoch 81: Validation loss decreased (6146.422852 --> 6095.354980).\n",
            "\t Train_Loss: 4158.5137 Val_Loss: 6095.3550  BEST VAL Loss: 6095.3550\n",
            "\n",
            "Epoch 82: Validation loss decreased (6095.354980 --> 6044.214844).\n",
            "\t Train_Loss: 4117.1777 Val_Loss: 6044.2148  BEST VAL Loss: 6044.2148\n",
            "\n",
            "Epoch 83: Validation loss decreased (6044.214844 --> 5995.688477).\n",
            "\t Train_Loss: 4076.7991 Val_Loss: 5995.6885  BEST VAL Loss: 5995.6885\n",
            "\n",
            "Epoch 84: Validation loss decreased (5995.688477 --> 5947.200684).\n",
            "\t Train_Loss: 4037.6560 Val_Loss: 5947.2007  BEST VAL Loss: 5947.2007\n",
            "\n",
            "Epoch 85: Validation loss decreased (5947.200684 --> 5897.847656).\n",
            "\t Train_Loss: 3998.1184 Val_Loss: 5897.8477  BEST VAL Loss: 5897.8477\n",
            "\n",
            "Epoch 86: Validation loss decreased (5897.847656 --> 5851.274902).\n",
            "\t Train_Loss: 3958.4668 Val_Loss: 5851.2749  BEST VAL Loss: 5851.2749\n",
            "\n",
            "Epoch 87: Validation loss decreased (5851.274902 --> 5805.321777).\n",
            "\t Train_Loss: 3920.7830 Val_Loss: 5805.3218  BEST VAL Loss: 5805.3218\n",
            "\n",
            "Epoch 88: Validation loss decreased (5805.321777 --> 5759.660156).\n",
            "\t Train_Loss: 3883.5234 Val_Loss: 5759.6602  BEST VAL Loss: 5759.6602\n",
            "\n",
            "Epoch 89: Validation loss decreased (5759.660156 --> 5714.301758).\n",
            "\t Train_Loss: 3846.5352 Val_Loss: 5714.3018  BEST VAL Loss: 5714.3018\n",
            "\n",
            "Epoch 90: Validation loss decreased (5714.301758 --> 5669.257324).\n",
            "\t Train_Loss: 3809.8269 Val_Loss: 5669.2573  BEST VAL Loss: 5669.2573\n",
            "\n",
            "Epoch 91: Validation loss decreased (5669.257324 --> 5624.532715).\n",
            "\t Train_Loss: 3773.4070 Val_Loss: 5624.5327  BEST VAL Loss: 5624.5327\n",
            "\n",
            "Epoch 92: Validation loss decreased (5624.532715 --> 5580.136230).\n",
            "\t Train_Loss: 3737.2812 Val_Loss: 5580.1362  BEST VAL Loss: 5580.1362\n",
            "\n",
            "Epoch 93: Validation loss decreased (5580.136230 --> 5536.069824).\n",
            "\t Train_Loss: 3701.4536 Val_Loss: 5536.0698  BEST VAL Loss: 5536.0698\n",
            "\n",
            "Epoch 94: Validation loss decreased (5536.069824 --> 5492.338379).\n",
            "\t Train_Loss: 3665.9277 Val_Loss: 5492.3384  BEST VAL Loss: 5492.3384\n",
            "\n",
            "Epoch 95: Validation loss decreased (5492.338379 --> 5448.943359).\n",
            "\t Train_Loss: 3630.7063 Val_Loss: 5448.9434  BEST VAL Loss: 5448.9434\n",
            "\n",
            "Epoch 96: Validation loss decreased (5448.943359 --> 5405.886230).\n",
            "\t Train_Loss: 3595.7898 Val_Loss: 5405.8862  BEST VAL Loss: 5405.8862\n",
            "\n",
            "Epoch 97: Validation loss decreased (5405.886230 --> 5363.166992).\n",
            "\t Train_Loss: 3561.1792 Val_Loss: 5363.1670  BEST VAL Loss: 5363.1670\n",
            "\n",
            "Epoch 98: Validation loss decreased (5363.166992 --> 5320.784668).\n",
            "\t Train_Loss: 3526.8740 Val_Loss: 5320.7847  BEST VAL Loss: 5320.7847\n",
            "\n",
            "Epoch 99: Validation loss decreased (5320.784668 --> 5278.739746).\n",
            "\t Train_Loss: 3492.8735 Val_Loss: 5278.7397  BEST VAL Loss: 5278.7397\n",
            "\n",
            "Epoch 100: Validation loss decreased (5278.739746 --> 5237.030762).\n",
            "\t Train_Loss: 3459.1772 Val_Loss: 5237.0308  BEST VAL Loss: 5237.0308\n",
            "\n",
            "Epoch 101: Validation loss decreased (5237.030762 --> 5195.655273).\n",
            "\t Train_Loss: 3425.7837 Val_Loss: 5195.6553  BEST VAL Loss: 5195.6553\n",
            "\n",
            "Epoch 102: Validation loss decreased (5195.655273 --> 5154.610840).\n",
            "\t Train_Loss: 3392.6904 Val_Loss: 5154.6108  BEST VAL Loss: 5154.6108\n",
            "\n",
            "Epoch 103: Validation loss decreased (5154.610840 --> 5113.898438).\n",
            "\t Train_Loss: 3359.8960 Val_Loss: 5113.8984  BEST VAL Loss: 5113.8984\n",
            "\n",
            "Epoch 104: Validation loss decreased (5113.898438 --> 5073.513672).\n",
            "\t Train_Loss: 3327.3997 Val_Loss: 5073.5137  BEST VAL Loss: 5073.5137\n",
            "\n",
            "Epoch 105: Validation loss decreased (5073.513672 --> 5033.453613).\n",
            "\t Train_Loss: 3295.1970 Val_Loss: 5033.4536  BEST VAL Loss: 5033.4536\n",
            "\n",
            "Epoch 106: Validation loss decreased (5033.453613 --> 4993.716309).\n",
            "\t Train_Loss: 3263.2869 Val_Loss: 4993.7163  BEST VAL Loss: 4993.7163\n",
            "\n",
            "Epoch 107: Validation loss decreased (4993.716309 --> 4954.300293).\n",
            "\t Train_Loss: 3231.6667 Val_Loss: 4954.3003  BEST VAL Loss: 4954.3003\n",
            "\n",
            "Epoch 108: Validation loss decreased (4954.300293 --> 4915.200195).\n",
            "\t Train_Loss: 3200.3342 Val_Loss: 4915.2002  BEST VAL Loss: 4915.2002\n",
            "\n",
            "Epoch 109: Validation loss decreased (4915.200195 --> 4876.416504).\n",
            "\t Train_Loss: 3169.2866 Val_Loss: 4876.4165  BEST VAL Loss: 4876.4165\n",
            "\n",
            "Epoch 110: Validation loss decreased (4876.416504 --> 4837.943848).\n",
            "\t Train_Loss: 3138.5217 Val_Loss: 4837.9438  BEST VAL Loss: 4837.9438\n",
            "\n",
            "Epoch 111: Validation loss decreased (4837.943848 --> 4799.781250).\n",
            "\t Train_Loss: 3108.0361 Val_Loss: 4799.7812  BEST VAL Loss: 4799.7812\n",
            "\n",
            "Epoch 112: Validation loss decreased (4799.781250 --> 4761.924805).\n",
            "\t Train_Loss: 3077.8279 Val_Loss: 4761.9248  BEST VAL Loss: 4761.9248\n",
            "\n",
            "Epoch 113: Validation loss decreased (4761.924805 --> 4724.371582).\n",
            "\t Train_Loss: 3047.8943 Val_Loss: 4724.3716  BEST VAL Loss: 4724.3716\n",
            "\n",
            "Epoch 114: Validation loss decreased (4724.371582 --> 4687.119629).\n",
            "\t Train_Loss: 3018.2322 Val_Loss: 4687.1196  BEST VAL Loss: 4687.1196\n",
            "\n",
            "Epoch 115: Validation loss decreased (4687.119629 --> 4650.166016).\n",
            "\t Train_Loss: 2988.8398 Val_Loss: 4650.1660  BEST VAL Loss: 4650.1660\n",
            "\n",
            "Epoch 116: Validation loss decreased (4650.166016 --> 4613.507324).\n",
            "\t Train_Loss: 2959.7144 Val_Loss: 4613.5073  BEST VAL Loss: 4613.5073\n",
            "\n",
            "Epoch 117: Validation loss decreased (4613.507324 --> 4577.140625).\n",
            "\t Train_Loss: 2930.8518 Val_Loss: 4577.1406  BEST VAL Loss: 4577.1406\n",
            "\n",
            "Epoch 118: Validation loss decreased (4577.140625 --> 4541.063965).\n",
            "\t Train_Loss: 2902.2520 Val_Loss: 4541.0640  BEST VAL Loss: 4541.0640\n",
            "\n",
            "Epoch 119: Validation loss decreased (4541.063965 --> 4505.274902).\n",
            "\t Train_Loss: 2873.9106 Val_Loss: 4505.2749  BEST VAL Loss: 4505.2749\n",
            "\n",
            "Epoch 120: Validation loss decreased (4505.274902 --> 4469.769043).\n",
            "\t Train_Loss: 2845.8259 Val_Loss: 4469.7690  BEST VAL Loss: 4469.7690\n",
            "\n",
            "Epoch 121: Validation loss decreased (4469.769043 --> 4434.546387).\n",
            "\t Train_Loss: 2817.9949 Val_Loss: 4434.5464  BEST VAL Loss: 4434.5464\n",
            "\n",
            "Epoch 122: Validation loss decreased (4434.546387 --> 4399.600098).\n",
            "\t Train_Loss: 2790.4155 Val_Loss: 4399.6001  BEST VAL Loss: 4399.6001\n",
            "\n",
            "Epoch 123: Validation loss decreased (4399.600098 --> 4364.930664).\n",
            "\t Train_Loss: 2763.0845 Val_Loss: 4364.9307  BEST VAL Loss: 4364.9307\n",
            "\n",
            "Epoch 124: Validation loss decreased (4364.930664 --> 4330.536133).\n",
            "\t Train_Loss: 2736.0005 Val_Loss: 4330.5361  BEST VAL Loss: 4330.5361\n",
            "\n",
            "Epoch 125: Validation loss decreased (4330.536133 --> 4296.412109).\n",
            "\t Train_Loss: 2709.1602 Val_Loss: 4296.4121  BEST VAL Loss: 4296.4121\n",
            "\n",
            "Epoch 126: Validation loss decreased (4296.412109 --> 4262.556152).\n",
            "\t Train_Loss: 2682.5610 Val_Loss: 4262.5562  BEST VAL Loss: 4262.5562\n",
            "\n",
            "Epoch 127: Validation loss decreased (4262.556152 --> 4228.966309).\n",
            "\t Train_Loss: 2656.2009 Val_Loss: 4228.9663  BEST VAL Loss: 4228.9663\n",
            "\n",
            "Epoch 128: Validation loss decreased (4228.966309 --> 4195.638184).\n",
            "\t Train_Loss: 2630.0759 Val_Loss: 4195.6382  BEST VAL Loss: 4195.6382\n",
            "\n",
            "Epoch 129: Validation loss decreased (4195.638184 --> 4162.568359).\n",
            "\t Train_Loss: 2604.1831 Val_Loss: 4162.5684  BEST VAL Loss: 4162.5684\n",
            "\n",
            "Epoch 130: Validation loss decreased (4162.568359 --> 4129.752441).\n",
            "\t Train_Loss: 2578.5188 Val_Loss: 4129.7524  BEST VAL Loss: 4129.7524\n",
            "\n",
            "Epoch 131: Validation loss decreased (4129.752441 --> 4097.182617).\n",
            "\t Train_Loss: 2553.0774 Val_Loss: 4097.1826  BEST VAL Loss: 4097.1826\n",
            "\n",
            "Epoch 132: Validation loss decreased (4097.182617 --> 4064.846191).\n",
            "\t Train_Loss: 2527.8503 Val_Loss: 4064.8462  BEST VAL Loss: 4064.8462\n",
            "\n",
            "Epoch 133: Validation loss decreased (4064.846191 --> 4032.714111).\n",
            "\t Train_Loss: 2502.8228 Val_Loss: 4032.7141  BEST VAL Loss: 4032.7141\n",
            "\n",
            "Epoch 134: Validation loss decreased (4032.714111 --> 4000.708984).\n",
            "\t Train_Loss: 2477.9619 Val_Loss: 4000.7090  BEST VAL Loss: 4000.7090\n",
            "\n",
            "Epoch 135: Validation loss decreased (4000.708984 --> 3968.593506).\n",
            "\t Train_Loss: 2453.1919 Val_Loss: 3968.5935  BEST VAL Loss: 3968.5935\n",
            "\n",
            "Epoch 136: Validation loss decreased (3968.593506 --> 3935.744629).\n",
            "\t Train_Loss: 2428.3171 Val_Loss: 3935.7446  BEST VAL Loss: 3935.7446\n",
            "\n",
            "Epoch 137: Validation loss decreased (3935.744629 --> 3902.056396).\n",
            "\t Train_Loss: 2402.9629 Val_Loss: 3902.0564  BEST VAL Loss: 3902.0564\n",
            "\n",
            "Epoch 138: Validation loss decreased (3902.056396 --> 3869.551270).\n",
            "\t Train_Loss: 2377.3169 Val_Loss: 3869.5513  BEST VAL Loss: 3869.5513\n",
            "\n",
            "Epoch 139: Validation loss decreased (3869.551270 --> 3838.253906).\n",
            "\t Train_Loss: 2352.5811 Val_Loss: 3838.2539  BEST VAL Loss: 3838.2539\n",
            "\n",
            "Epoch 140: Validation loss decreased (3838.253906 --> 3807.326172).\n",
            "\t Train_Loss: 2328.6282 Val_Loss: 3807.3262  BEST VAL Loss: 3807.3262\n",
            "\n",
            "Epoch 141: Validation loss decreased (3807.326172 --> 3776.568848).\n",
            "\t Train_Loss: 2304.9592 Val_Loss: 3776.5688  BEST VAL Loss: 3776.5688\n",
            "\n",
            "Epoch 142: Validation loss decreased (3776.568848 --> 3745.959717).\n",
            "\t Train_Loss: 2281.4495 Val_Loss: 3745.9597  BEST VAL Loss: 3745.9597\n",
            "\n",
            "Epoch 143: Validation loss decreased (3745.959717 --> 3715.505859).\n",
            "\t Train_Loss: 2258.0842 Val_Loss: 3715.5059  BEST VAL Loss: 3715.5059\n",
            "\n",
            "Epoch 144: Validation loss decreased (3715.505859 --> 3685.215576).\n",
            "\t Train_Loss: 2234.8674 Val_Loss: 3685.2156  BEST VAL Loss: 3685.2156\n",
            "\n",
            "Epoch 145: Validation loss decreased (3685.215576 --> 3655.098389).\n",
            "\t Train_Loss: 2211.8064 Val_Loss: 3655.0984  BEST VAL Loss: 3655.0984\n",
            "\n",
            "Epoch 146: Validation loss decreased (3655.098389 --> 3625.163818).\n",
            "\t Train_Loss: 2188.9077 Val_Loss: 3625.1638  BEST VAL Loss: 3625.1638\n",
            "\n",
            "Epoch 147: Validation loss decreased (3625.163818 --> 3595.416504).\n",
            "\t Train_Loss: 2166.1777 Val_Loss: 3595.4165  BEST VAL Loss: 3595.4165\n",
            "\n",
            "Epoch 148: Validation loss decreased (3595.416504 --> 3565.864502).\n",
            "\t Train_Loss: 2143.6206 Val_Loss: 3565.8645  BEST VAL Loss: 3565.8645\n",
            "\n",
            "Epoch 149: Validation loss decreased (3565.864502 --> 3536.512451).\n",
            "\t Train_Loss: 2121.2422 Val_Loss: 3536.5125  BEST VAL Loss: 3536.5125\n",
            "\n",
            "Epoch 150: Validation loss decreased (3536.512451 --> 3507.364014).\n",
            "\t Train_Loss: 2099.0449 Val_Loss: 3507.3640  BEST VAL Loss: 3507.3640\n",
            "\n",
            "Epoch 151: Validation loss decreased (3507.364014 --> 3478.423584).\n",
            "\t Train_Loss: 2077.0330 Val_Loss: 3478.4236  BEST VAL Loss: 3478.4236\n",
            "\n",
            "Epoch 152: Validation loss decreased (3478.423584 --> 3449.692139).\n",
            "\t Train_Loss: 2055.2075 Val_Loss: 3449.6921  BEST VAL Loss: 3449.6921\n",
            "\n",
            "Epoch 153: Validation loss decreased (3449.692139 --> 3421.174561).\n",
            "\t Train_Loss: 2033.5706 Val_Loss: 3421.1746  BEST VAL Loss: 3421.1746\n",
            "\n",
            "Epoch 154: Validation loss decreased (3421.174561 --> 3392.872314).\n",
            "\t Train_Loss: 2012.1246 Val_Loss: 3392.8723  BEST VAL Loss: 3392.8723\n",
            "\n",
            "Epoch 155: Validation loss decreased (3392.872314 --> 3364.785156).\n",
            "\t Train_Loss: 1990.8701 Val_Loss: 3364.7852  BEST VAL Loss: 3364.7852\n",
            "\n",
            "Epoch 156: Validation loss decreased (3364.785156 --> 3336.916016).\n",
            "\t Train_Loss: 1969.8082 Val_Loss: 3336.9160  BEST VAL Loss: 3336.9160\n",
            "\n",
            "Epoch 157: Validation loss decreased (3336.916016 --> 3309.263672).\n",
            "\t Train_Loss: 1948.9385 Val_Loss: 3309.2637  BEST VAL Loss: 3309.2637\n",
            "\n",
            "Epoch 158: Validation loss decreased (3309.263672 --> 3281.829834).\n",
            "\t Train_Loss: 1928.2621 Val_Loss: 3281.8298  BEST VAL Loss: 3281.8298\n",
            "\n",
            "Epoch 159: Validation loss decreased (3281.829834 --> 3254.614746).\n",
            "\t Train_Loss: 1907.7787 Val_Loss: 3254.6147  BEST VAL Loss: 3254.6147\n",
            "\n",
            "Epoch 160: Validation loss decreased (3254.614746 --> 3227.618408).\n",
            "\t Train_Loss: 1887.4883 Val_Loss: 3227.6184  BEST VAL Loss: 3227.6184\n",
            "\n",
            "Epoch 161: Validation loss decreased (3227.618408 --> 3200.839844).\n",
            "\t Train_Loss: 1867.3899 Val_Loss: 3200.8398  BEST VAL Loss: 3200.8398\n",
            "\n",
            "Epoch 162: Validation loss decreased (3200.839844 --> 3174.278564).\n",
            "\t Train_Loss: 1847.4836 Val_Loss: 3174.2786  BEST VAL Loss: 3174.2786\n",
            "\n",
            "Epoch 163: Validation loss decreased (3174.278564 --> 3147.934814).\n",
            "\t Train_Loss: 1827.7686 Val_Loss: 3147.9348  BEST VAL Loss: 3147.9348\n",
            "\n",
            "Epoch 164: Validation loss decreased (3147.934814 --> 3121.807861).\n",
            "\t Train_Loss: 1808.2444 Val_Loss: 3121.8079  BEST VAL Loss: 3121.8079\n",
            "\n",
            "Epoch 165: Validation loss decreased (3121.807861 --> 3095.897217).\n",
            "\t Train_Loss: 1788.9094 Val_Loss: 3095.8972  BEST VAL Loss: 3095.8972\n",
            "\n",
            "Epoch 166: Validation loss decreased (3095.897217 --> 3070.200928).\n",
            "\t Train_Loss: 1769.7633 Val_Loss: 3070.2009  BEST VAL Loss: 3070.2009\n",
            "\n",
            "Epoch 167: Validation loss decreased (3070.200928 --> 3044.717529).\n",
            "\t Train_Loss: 1750.8053 Val_Loss: 3044.7175  BEST VAL Loss: 3044.7175\n",
            "\n",
            "Epoch 168: Validation loss decreased (3044.717529 --> 3019.447021).\n",
            "\t Train_Loss: 1732.0334 Val_Loss: 3019.4470  BEST VAL Loss: 3019.4470\n",
            "\n",
            "Epoch 169: Validation loss decreased (3019.447021 --> 2994.388428).\n",
            "\t Train_Loss: 1713.4471 Val_Loss: 2994.3884  BEST VAL Loss: 2994.3884\n",
            "\n",
            "Epoch 170: Validation loss decreased (2994.388428 --> 2969.539551).\n",
            "\t Train_Loss: 1695.0450 Val_Loss: 2969.5396  BEST VAL Loss: 2969.5396\n",
            "\n",
            "Epoch 171: Validation loss decreased (2969.539551 --> 2944.900635).\n",
            "\t Train_Loss: 1676.8257 Val_Loss: 2944.9006  BEST VAL Loss: 2944.9006\n",
            "\n",
            "Epoch 172: Validation loss decreased (2944.900635 --> 2920.469971).\n",
            "\t Train_Loss: 1658.7887 Val_Loss: 2920.4700  BEST VAL Loss: 2920.4700\n",
            "\n",
            "Epoch 173: Validation loss decreased (2920.469971 --> 2896.243896).\n",
            "\t Train_Loss: 1640.9316 Val_Loss: 2896.2439  BEST VAL Loss: 2896.2439\n",
            "\n",
            "Epoch 174: Validation loss decreased (2896.243896 --> 2872.224365).\n",
            "\t Train_Loss: 1623.2537 Val_Loss: 2872.2244  BEST VAL Loss: 2872.2244\n",
            "\n",
            "Epoch 175: Validation loss decreased (2872.224365 --> 2848.408203).\n",
            "\t Train_Loss: 1605.7539 Val_Loss: 2848.4082  BEST VAL Loss: 2848.4082\n",
            "\n",
            "Epoch 176: Validation loss decreased (2848.408203 --> 2824.794678).\n",
            "\t Train_Loss: 1588.4303 Val_Loss: 2824.7947  BEST VAL Loss: 2824.7947\n",
            "\n",
            "Epoch 177: Validation loss decreased (2824.794678 --> 2801.382324).\n",
            "\t Train_Loss: 1571.2821 Val_Loss: 2801.3823  BEST VAL Loss: 2801.3823\n",
            "\n",
            "Epoch 178: Validation loss decreased (2801.382324 --> 2778.168457).\n",
            "\t Train_Loss: 1554.3071 Val_Loss: 2778.1685  BEST VAL Loss: 2778.1685\n",
            "\n",
            "Epoch 179: Validation loss decreased (2778.168457 --> 2755.153564).\n",
            "\t Train_Loss: 1537.5046 Val_Loss: 2755.1536  BEST VAL Loss: 2755.1536\n",
            "\n",
            "Epoch 180: Validation loss decreased (2755.153564 --> 2732.334473).\n",
            "\t Train_Loss: 1520.8727 Val_Loss: 2732.3345  BEST VAL Loss: 2732.3345\n",
            "\n",
            "Epoch 181: Validation loss decreased (2732.334473 --> 2709.711670).\n",
            "\t Train_Loss: 1504.4106 Val_Loss: 2709.7117  BEST VAL Loss: 2709.7117\n",
            "\n",
            "Epoch 182: Validation loss decreased (2709.711670 --> 2687.281982).\n",
            "\t Train_Loss: 1488.1167 Val_Loss: 2687.2820  BEST VAL Loss: 2687.2820\n",
            "\n",
            "Epoch 183: Validation loss decreased (2687.281982 --> 2665.045166).\n",
            "\t Train_Loss: 1471.9894 Val_Loss: 2665.0452  BEST VAL Loss: 2665.0452\n",
            "\n",
            "Epoch 184: Validation loss decreased (2665.045166 --> 2642.997803).\n",
            "\t Train_Loss: 1456.0273 Val_Loss: 2642.9978  BEST VAL Loss: 2642.9978\n",
            "\n",
            "Epoch 185: Validation loss decreased (2642.997803 --> 2621.141357).\n",
            "\t Train_Loss: 1440.2294 Val_Loss: 2621.1414  BEST VAL Loss: 2621.1414\n",
            "\n",
            "Epoch 186: Validation loss decreased (2621.141357 --> 2599.472656).\n",
            "\t Train_Loss: 1424.5941 Val_Loss: 2599.4727  BEST VAL Loss: 2599.4727\n",
            "\n",
            "Epoch 187: Validation loss decreased (2599.472656 --> 2577.989990).\n",
            "\t Train_Loss: 1409.1198 Val_Loss: 2577.9900  BEST VAL Loss: 2577.9900\n",
            "\n",
            "Epoch 188: Validation loss decreased (2577.989990 --> 2556.692139).\n",
            "\t Train_Loss: 1393.8053 Val_Loss: 2556.6921  BEST VAL Loss: 2556.6921\n",
            "\n",
            "Epoch 189: Validation loss decreased (2556.692139 --> 2535.578857).\n",
            "\t Train_Loss: 1378.6494 Val_Loss: 2535.5789  BEST VAL Loss: 2535.5789\n",
            "\n",
            "Epoch 190: Validation loss decreased (2535.578857 --> 2514.646729).\n",
            "\t Train_Loss: 1363.6505 Val_Loss: 2514.6467  BEST VAL Loss: 2514.6467\n",
            "\n",
            "Epoch 191: Validation loss decreased (2514.646729 --> 2493.896240).\n",
            "\t Train_Loss: 1348.8071 Val_Loss: 2493.8962  BEST VAL Loss: 2493.8962\n",
            "\n",
            "Epoch 192: Validation loss decreased (2493.896240 --> 2473.324951).\n",
            "\t Train_Loss: 1334.1185 Val_Loss: 2473.3250  BEST VAL Loss: 2473.3250\n",
            "\n",
            "Epoch 193: Validation loss decreased (2473.324951 --> 2452.932129).\n",
            "\t Train_Loss: 1319.5826 Val_Loss: 2452.9321  BEST VAL Loss: 2452.9321\n",
            "\n",
            "Epoch 194: Validation loss decreased (2452.932129 --> 2432.715332).\n",
            "\t Train_Loss: 1305.1984 Val_Loss: 2432.7153  BEST VAL Loss: 2432.7153\n",
            "\n",
            "Epoch 195: Validation loss decreased (2432.715332 --> 2412.673584).\n",
            "\t Train_Loss: 1290.9650 Val_Loss: 2412.6736  BEST VAL Loss: 2412.6736\n",
            "\n",
            "Epoch 196: Validation loss decreased (2412.673584 --> 2392.807129).\n",
            "\t Train_Loss: 1276.8801 Val_Loss: 2392.8071  BEST VAL Loss: 2392.8071\n",
            "\n",
            "Epoch 197: Validation loss decreased (2392.807129 --> 2373.111328).\n",
            "\t Train_Loss: 1262.9432 Val_Loss: 2373.1113  BEST VAL Loss: 2373.1113\n",
            "\n",
            "Epoch 198: Validation loss decreased (2373.111328 --> 2353.587891).\n",
            "\t Train_Loss: 1249.1530 Val_Loss: 2353.5879  BEST VAL Loss: 2353.5879\n",
            "\n",
            "Epoch 199: Validation loss decreased (2353.587891 --> 2334.234131).\n",
            "\t Train_Loss: 1235.5078 Val_Loss: 2334.2341  BEST VAL Loss: 2334.2341\n",
            "\n",
            "Epoch 200: Validation loss decreased (2334.234131 --> 2315.049561).\n",
            "\t Train_Loss: 1222.0067 Val_Loss: 2315.0496  BEST VAL Loss: 2315.0496\n",
            "\n",
            "Epoch 201: Validation loss decreased (2315.049561 --> 2296.031250).\n",
            "\t Train_Loss: 1208.6483 Val_Loss: 2296.0312  BEST VAL Loss: 2296.0312\n",
            "\n",
            "Epoch 202: Validation loss decreased (2296.031250 --> 2277.179443).\n",
            "\t Train_Loss: 1195.4313 Val_Loss: 2277.1794  BEST VAL Loss: 2277.1794\n",
            "\n",
            "Epoch 203: Validation loss decreased (2277.179443 --> 2258.491943).\n",
            "\t Train_Loss: 1182.3544 Val_Loss: 2258.4919  BEST VAL Loss: 2258.4919\n",
            "\n",
            "Epoch 204: Validation loss decreased (2258.491943 --> 2239.967285).\n",
            "\t Train_Loss: 1169.4164 Val_Loss: 2239.9673  BEST VAL Loss: 2239.9673\n",
            "\n",
            "Epoch 205: Validation loss decreased (2239.967285 --> 2221.605225).\n",
            "\t Train_Loss: 1156.6162 Val_Loss: 2221.6052  BEST VAL Loss: 2221.6052\n",
            "\n",
            "Epoch 206: Validation loss decreased (2221.605225 --> 2203.403809).\n",
            "\t Train_Loss: 1143.9523 Val_Loss: 2203.4038  BEST VAL Loss: 2203.4038\n",
            "\n",
            "Epoch 207: Validation loss decreased (2203.403809 --> 2185.361328).\n",
            "\t Train_Loss: 1131.4240 Val_Loss: 2185.3613  BEST VAL Loss: 2185.3613\n",
            "\n",
            "Epoch 208: Validation loss decreased (2185.361328 --> 2167.478271).\n",
            "\t Train_Loss: 1119.0291 Val_Loss: 2167.4783  BEST VAL Loss: 2167.4783\n",
            "\n",
            "Epoch 209: Validation loss decreased (2167.478271 --> 2149.751221).\n",
            "\t Train_Loss: 1106.7676 Val_Loss: 2149.7512  BEST VAL Loss: 2149.7512\n",
            "\n",
            "Epoch 210: Validation loss decreased (2149.751221 --> 2132.179688).\n",
            "\t Train_Loss: 1094.6379 Val_Loss: 2132.1797  BEST VAL Loss: 2132.1797\n",
            "\n",
            "Epoch 211: Validation loss decreased (2132.179688 --> 2114.763428).\n",
            "\t Train_Loss: 1082.6387 Val_Loss: 2114.7634  BEST VAL Loss: 2114.7634\n",
            "\n",
            "Epoch 212: Validation loss decreased (2114.763428 --> 2097.500732).\n",
            "\t Train_Loss: 1070.7688 Val_Loss: 2097.5007  BEST VAL Loss: 2097.5007\n",
            "\n",
            "Epoch 213: Validation loss decreased (2097.500732 --> 2080.389404).\n",
            "\t Train_Loss: 1059.0273 Val_Loss: 2080.3894  BEST VAL Loss: 2080.3894\n",
            "\n",
            "Epoch 214: Validation loss decreased (2080.389404 --> 2063.428467).\n",
            "\t Train_Loss: 1047.4126 Val_Loss: 2063.4285  BEST VAL Loss: 2063.4285\n",
            "\n",
            "Epoch 215: Validation loss decreased (2063.428467 --> 2046.618408).\n",
            "\t Train_Loss: 1035.9244 Val_Loss: 2046.6184  BEST VAL Loss: 2046.6184\n",
            "\n",
            "Epoch 216: Validation loss decreased (2046.618408 --> 2029.956421).\n",
            "\t Train_Loss: 1024.5604 Val_Loss: 2029.9564  BEST VAL Loss: 2029.9564\n",
            "\n",
            "Epoch 217: Validation loss decreased (2029.956421 --> 2013.441406).\n",
            "\t Train_Loss: 1013.3205 Val_Loss: 2013.4414  BEST VAL Loss: 2013.4414\n",
            "\n",
            "Epoch 218: Validation loss decreased (2013.441406 --> 1997.073242).\n",
            "\t Train_Loss: 1002.2029 Val_Loss: 1997.0732  BEST VAL Loss: 1997.0732\n",
            "\n",
            "Epoch 219: Validation loss decreased (1997.073242 --> 1980.848999).\n",
            "\t Train_Loss: 991.2070 Val_Loss: 1980.8490  BEST VAL Loss: 1980.8490\n",
            "\n",
            "Epoch 220: Validation loss decreased (1980.848999 --> 1964.770142).\n",
            "\t Train_Loss: 980.3312 Val_Loss: 1964.7701  BEST VAL Loss: 1964.7701\n",
            "\n",
            "Epoch 221: Validation loss decreased (1964.770142 --> 1948.833252).\n",
            "\t Train_Loss: 969.5748 Val_Loss: 1948.8333  BEST VAL Loss: 1948.8333\n",
            "\n",
            "Epoch 222: Validation loss decreased (1948.833252 --> 1933.037964).\n",
            "\t Train_Loss: 958.9368 Val_Loss: 1933.0380  BEST VAL Loss: 1933.0380\n",
            "\n",
            "Epoch 223: Validation loss decreased (1933.037964 --> 1917.383667).\n",
            "\t Train_Loss: 948.4160 Val_Loss: 1917.3837  BEST VAL Loss: 1917.3837\n",
            "\n",
            "Epoch 224: Validation loss decreased (1917.383667 --> 1901.868408).\n",
            "\t Train_Loss: 938.0113 Val_Loss: 1901.8684  BEST VAL Loss: 1901.8684\n",
            "\n",
            "Epoch 225: Validation loss decreased (1901.868408 --> 1886.491577).\n",
            "\t Train_Loss: 927.7214 Val_Loss: 1886.4916  BEST VAL Loss: 1886.4916\n",
            "\n",
            "Epoch 226: Validation loss decreased (1886.491577 --> 1871.251343).\n",
            "\t Train_Loss: 917.5460 Val_Loss: 1871.2513  BEST VAL Loss: 1871.2513\n",
            "\n",
            "Epoch 227: Validation loss decreased (1871.251343 --> 1856.148438).\n",
            "\t Train_Loss: 907.4830 Val_Loss: 1856.1484  BEST VAL Loss: 1856.1484\n",
            "\n",
            "Epoch 228: Validation loss decreased (1856.148438 --> 1841.179688).\n",
            "\t Train_Loss: 897.5323 Val_Loss: 1841.1797  BEST VAL Loss: 1841.1797\n",
            "\n",
            "Epoch 229: Validation loss decreased (1841.179688 --> 1826.344971).\n",
            "\t Train_Loss: 887.6922 Val_Loss: 1826.3450  BEST VAL Loss: 1826.3450\n",
            "\n",
            "Epoch 230: Validation loss decreased (1826.344971 --> 1811.643799).\n",
            "\t Train_Loss: 877.9624 Val_Loss: 1811.6438  BEST VAL Loss: 1811.6438\n",
            "\n",
            "Epoch 231: Validation loss decreased (1811.643799 --> 1797.073242).\n",
            "\t Train_Loss: 868.3412 Val_Loss: 1797.0732  BEST VAL Loss: 1797.0732\n",
            "\n",
            "Epoch 232: Validation loss decreased (1797.073242 --> 1782.634644).\n",
            "\t Train_Loss: 858.8282 Val_Loss: 1782.6346  BEST VAL Loss: 1782.6346\n",
            "\n",
            "Epoch 233: Validation loss decreased (1782.634644 --> 1768.325195).\n",
            "\t Train_Loss: 849.4218 Val_Loss: 1768.3252  BEST VAL Loss: 1768.3252\n",
            "\n",
            "Epoch 234: Validation loss decreased (1768.325195 --> 1754.144409).\n",
            "\t Train_Loss: 840.1213 Val_Loss: 1754.1444  BEST VAL Loss: 1754.1444\n",
            "\n",
            "Epoch 235: Validation loss decreased (1754.144409 --> 1740.091797).\n",
            "\t Train_Loss: 830.9257 Val_Loss: 1740.0918  BEST VAL Loss: 1740.0918\n",
            "\n",
            "Epoch 236: Validation loss decreased (1740.091797 --> 1726.165283).\n",
            "\t Train_Loss: 821.8344 Val_Loss: 1726.1653  BEST VAL Loss: 1726.1653\n",
            "\n",
            "Epoch 237: Validation loss decreased (1726.165283 --> 1712.364136).\n",
            "\t Train_Loss: 812.8459 Val_Loss: 1712.3641  BEST VAL Loss: 1712.3641\n",
            "\n",
            "Epoch 238: Validation loss decreased (1712.364136 --> 1698.689087).\n",
            "\t Train_Loss: 803.9594 Val_Loss: 1698.6891  BEST VAL Loss: 1698.6891\n",
            "\n",
            "Epoch 239: Validation loss decreased (1698.689087 --> 1685.136597).\n",
            "\t Train_Loss: 795.1741 Val_Loss: 1685.1366  BEST VAL Loss: 1685.1366\n",
            "\n",
            "Epoch 240: Validation loss decreased (1685.136597 --> 1671.707031).\n",
            "\t Train_Loss: 786.4891 Val_Loss: 1671.7070  BEST VAL Loss: 1671.7070\n",
            "\n",
            "Epoch 241: Validation loss decreased (1671.707031 --> 1658.399048).\n",
            "\t Train_Loss: 777.9030 Val_Loss: 1658.3990  BEST VAL Loss: 1658.3990\n",
            "\n",
            "Epoch 242: Validation loss decreased (1658.399048 --> 1645.211792).\n",
            "\t Train_Loss: 769.4155 Val_Loss: 1645.2118  BEST VAL Loss: 1645.2118\n",
            "\n",
            "Epoch 243: Validation loss decreased (1645.211792 --> 1632.144165).\n",
            "\t Train_Loss: 761.0252 Val_Loss: 1632.1442  BEST VAL Loss: 1632.1442\n",
            "\n",
            "Epoch 244: Validation loss decreased (1632.144165 --> 1619.196167).\n",
            "\t Train_Loss: 752.7313 Val_Loss: 1619.1962  BEST VAL Loss: 1619.1962\n",
            "\n",
            "Epoch 245: Validation loss decreased (1619.196167 --> 1606.363647).\n",
            "\t Train_Loss: 744.5331 Val_Loss: 1606.3636  BEST VAL Loss: 1606.3636\n",
            "\n",
            "Epoch 246: Validation loss decreased (1606.363647 --> 1593.650635).\n",
            "\t Train_Loss: 736.4291 Val_Loss: 1593.6506  BEST VAL Loss: 1593.6506\n",
            "\n",
            "Epoch 247: Validation loss decreased (1593.650635 --> 1581.052246).\n",
            "\t Train_Loss: 728.4196 Val_Loss: 1581.0522  BEST VAL Loss: 1581.0522\n",
            "\n",
            "Epoch 248: Validation loss decreased (1581.052246 --> 1568.568726).\n",
            "\t Train_Loss: 720.5024 Val_Loss: 1568.5687  BEST VAL Loss: 1568.5687\n",
            "\n",
            "Epoch 249: Validation loss decreased (1568.568726 --> 1556.199219).\n",
            "\t Train_Loss: 712.6772 Val_Loss: 1556.1992  BEST VAL Loss: 1556.1992\n",
            "\n",
            "Epoch 250: Validation loss decreased (1556.199219 --> 1543.943115).\n",
            "\t Train_Loss: 704.9434 Val_Loss: 1543.9431  BEST VAL Loss: 1543.9431\n",
            "\n",
            "Epoch 251: Validation loss decreased (1543.943115 --> 1531.799194).\n",
            "\t Train_Loss: 697.2993 Val_Loss: 1531.7992  BEST VAL Loss: 1531.7992\n",
            "\n",
            "Epoch 252: Validation loss decreased (1531.799194 --> 1519.753174).\n",
            "\t Train_Loss: 689.7416 Val_Loss: 1519.7532  BEST VAL Loss: 1519.7532\n",
            "\n",
            "Epoch 253: Validation loss decreased (1519.753174 --> 1507.690186).\n",
            "\t Train_Loss: 682.2387 Val_Loss: 1507.6902  BEST VAL Loss: 1507.6902\n",
            "\n",
            "Epoch 254: Validation loss decreased (1507.690186 --> 1494.983276).\n",
            "\t Train_Loss: 674.6403 Val_Loss: 1494.9833  BEST VAL Loss: 1494.9833\n",
            "\n",
            "Epoch 255: Validation loss decreased (1494.983276 --> 1480.410034).\n",
            "\t Train_Loss: 666.4908 Val_Loss: 1480.4100  BEST VAL Loss: 1480.4100\n",
            "\n",
            "Epoch 256: Validation loss decreased (1480.410034 --> 1466.167847).\n",
            "\t Train_Loss: 657.5773 Val_Loss: 1466.1678  BEST VAL Loss: 1466.1678\n",
            "\n",
            "Epoch 257: Validation loss decreased (1466.167847 --> 1453.414429).\n",
            "\t Train_Loss: 649.2098 Val_Loss: 1453.4144  BEST VAL Loss: 1453.4144\n",
            "\n",
            "Epoch 258: Validation loss decreased (1453.414429 --> 1440.711792).\n",
            "\t Train_Loss: 641.4456 Val_Loss: 1440.7118  BEST VAL Loss: 1440.7118\n",
            "\n",
            "Epoch 259: Validation loss decreased (1440.711792 --> 1427.086304).\n",
            "\t Train_Loss: 633.6363 Val_Loss: 1427.0863  BEST VAL Loss: 1427.0863\n",
            "\n",
            "Epoch 260: Validation loss decreased (1427.086304 --> 1412.475464).\n",
            "\t Train_Loss: 625.1844 Val_Loss: 1412.4755  BEST VAL Loss: 1412.4755\n",
            "\n",
            "Epoch 261: Validation loss decreased (1412.475464 --> 1399.713379).\n",
            "\t Train_Loss: 616.5912 Val_Loss: 1399.7134  BEST VAL Loss: 1399.7134\n",
            "\n",
            "Epoch 262: Validation loss decreased (1399.713379 --> 1387.044189).\n",
            "\t Train_Loss: 608.8970 Val_Loss: 1387.0442  BEST VAL Loss: 1387.0442\n",
            "\n",
            "Epoch 263: Validation loss decreased (1387.044189 --> 1374.465698).\n",
            "\t Train_Loss: 601.2769 Val_Loss: 1374.4657  BEST VAL Loss: 1374.4657\n",
            "\n",
            "Epoch 264: Validation loss decreased (1374.465698 --> 1361.986938).\n",
            "\t Train_Loss: 593.7364 Val_Loss: 1361.9869  BEST VAL Loss: 1361.9869\n",
            "\n",
            "Epoch 265: Validation loss decreased (1361.986938 --> 1349.618896).\n",
            "\t Train_Loss: 586.2800 Val_Loss: 1349.6189  BEST VAL Loss: 1349.6189\n",
            "\n",
            "Epoch 266: Validation loss decreased (1349.618896 --> 1337.366455).\n",
            "\t Train_Loss: 578.9141 Val_Loss: 1337.3665  BEST VAL Loss: 1337.3665\n",
            "\n",
            "Epoch 267: Validation loss decreased (1337.366455 --> 1325.235962).\n",
            "\t Train_Loss: 571.6420 Val_Loss: 1325.2360  BEST VAL Loss: 1325.2360\n",
            "\n",
            "Epoch 268: Validation loss decreased (1325.235962 --> 1313.231689).\n",
            "\t Train_Loss: 564.4666 Val_Loss: 1313.2317  BEST VAL Loss: 1313.2317\n",
            "\n",
            "Epoch 269: Validation loss decreased (1313.231689 --> 1301.356812).\n",
            "\t Train_Loss: 557.3901 Val_Loss: 1301.3568  BEST VAL Loss: 1301.3568\n",
            "\n",
            "Epoch 270: Validation loss decreased (1301.356812 --> 1289.613159).\n",
            "\t Train_Loss: 550.4141 Val_Loss: 1289.6132  BEST VAL Loss: 1289.6132\n",
            "\n",
            "Epoch 271: Validation loss decreased (1289.613159 --> 1278.003174).\n",
            "\t Train_Loss: 543.5392 Val_Loss: 1278.0032  BEST VAL Loss: 1278.0032\n",
            "\n",
            "Epoch 272: Validation loss decreased (1278.003174 --> 1266.526855).\n",
            "\t Train_Loss: 536.7659 Val_Loss: 1266.5269  BEST VAL Loss: 1266.5269\n",
            "\n",
            "Epoch 273: Validation loss decreased (1266.526855 --> 1255.185425).\n",
            "\t Train_Loss: 530.0947 Val_Loss: 1255.1854  BEST VAL Loss: 1255.1854\n",
            "\n",
            "Epoch 274: Validation loss decreased (1255.185425 --> 1243.979004).\n",
            "\t Train_Loss: 523.5247 Val_Loss: 1243.9790  BEST VAL Loss: 1243.9790\n",
            "\n",
            "Epoch 275: Validation loss decreased (1243.979004 --> 1232.907349).\n",
            "\t Train_Loss: 517.0565 Val_Loss: 1232.9073  BEST VAL Loss: 1232.9073\n",
            "\n",
            "Epoch 276: Validation loss decreased (1232.907349 --> 1221.970337).\n",
            "\t Train_Loss: 510.6885 Val_Loss: 1221.9703  BEST VAL Loss: 1221.9703\n",
            "\n",
            "Epoch 277: Validation loss decreased (1221.970337 --> 1211.165283).\n",
            "\t Train_Loss: 504.4206 Val_Loss: 1211.1653  BEST VAL Loss: 1211.1653\n",
            "\n",
            "Epoch 278: Validation loss decreased (1211.165283 --> 1200.494019).\n",
            "\t Train_Loss: 498.2516 Val_Loss: 1200.4940  BEST VAL Loss: 1200.4940\n",
            "\n",
            "Epoch 279: Validation loss decreased (1200.494019 --> 1189.954346).\n",
            "\t Train_Loss: 492.1804 Val_Loss: 1189.9543  BEST VAL Loss: 1189.9543\n",
            "\n",
            "Epoch 280: Validation loss decreased (1189.954346 --> 1179.544556).\n",
            "\t Train_Loss: 486.2064 Val_Loss: 1179.5446  BEST VAL Loss: 1179.5446\n",
            "\n",
            "Epoch 281: Validation loss decreased (1179.544556 --> 1169.263184).\n",
            "\t Train_Loss: 480.3276 Val_Loss: 1169.2632  BEST VAL Loss: 1169.2632\n",
            "\n",
            "Epoch 282: Validation loss decreased (1169.263184 --> 1159.109863).\n",
            "\t Train_Loss: 474.5434 Val_Loss: 1159.1099  BEST VAL Loss: 1159.1099\n",
            "\n",
            "Epoch 283: Validation loss decreased (1159.109863 --> 1149.083252).\n",
            "\t Train_Loss: 468.8524 Val_Loss: 1149.0833  BEST VAL Loss: 1149.0833\n",
            "\n",
            "Epoch 284: Validation loss decreased (1149.083252 --> 1139.182007).\n",
            "\t Train_Loss: 463.2536 Val_Loss: 1139.1820  BEST VAL Loss: 1139.1820\n",
            "\n",
            "Epoch 285: Validation loss decreased (1139.182007 --> 1129.401611).\n",
            "\t Train_Loss: 457.7446 Val_Loss: 1129.4016  BEST VAL Loss: 1129.4016\n",
            "\n",
            "Epoch 286: Validation loss decreased (1129.401611 --> 1119.744507).\n",
            "\t Train_Loss: 452.3250 Val_Loss: 1119.7445  BEST VAL Loss: 1119.7445\n",
            "\n",
            "Epoch 287: Validation loss decreased (1119.744507 --> 1110.207764).\n",
            "\t Train_Loss: 446.9936 Val_Loss: 1110.2078  BEST VAL Loss: 1110.2078\n",
            "\n",
            "Epoch 288: Validation loss decreased (1110.207764 --> 1100.788696).\n",
            "\t Train_Loss: 441.7487 Val_Loss: 1100.7887  BEST VAL Loss: 1100.7887\n",
            "\n",
            "Epoch 289: Validation loss decreased (1100.788696 --> 1091.487183).\n",
            "\t Train_Loss: 436.5888 Val_Loss: 1091.4872  BEST VAL Loss: 1091.4872\n",
            "\n",
            "Epoch 290: Validation loss decreased (1091.487183 --> 1082.300415).\n",
            "\t Train_Loss: 431.5127 Val_Loss: 1082.3004  BEST VAL Loss: 1082.3004\n",
            "\n",
            "Epoch 291: Validation loss decreased (1082.300415 --> 1073.227173).\n",
            "\t Train_Loss: 426.5190 Val_Loss: 1073.2272  BEST VAL Loss: 1073.2272\n",
            "\n",
            "Epoch 292: Validation loss decreased (1073.227173 --> 1064.266968).\n",
            "\t Train_Loss: 421.6067 Val_Loss: 1064.2670  BEST VAL Loss: 1064.2670\n",
            "\n",
            "Epoch 293: Validation loss decreased (1064.266968 --> 1055.416382).\n",
            "\t Train_Loss: 416.7740 Val_Loss: 1055.4164  BEST VAL Loss: 1055.4164\n",
            "\n",
            "Epoch 294: Validation loss decreased (1055.416382 --> 1046.675171).\n",
            "\t Train_Loss: 412.0199 Val_Loss: 1046.6752  BEST VAL Loss: 1046.6752\n",
            "\n",
            "Epoch 295: Validation loss decreased (1046.675171 --> 1038.041626).\n",
            "\t Train_Loss: 407.3431 Val_Loss: 1038.0416  BEST VAL Loss: 1038.0416\n",
            "\n",
            "Epoch 296: Validation loss decreased (1038.041626 --> 1029.514526).\n",
            "\t Train_Loss: 402.7425 Val_Loss: 1029.5145  BEST VAL Loss: 1029.5145\n",
            "\n",
            "Epoch 297: Validation loss decreased (1029.514526 --> 1021.091248).\n",
            "\t Train_Loss: 398.2167 Val_Loss: 1021.0912  BEST VAL Loss: 1021.0912\n",
            "\n",
            "Epoch 298: Validation loss decreased (1021.091248 --> 1012.770813).\n",
            "\t Train_Loss: 393.7646 Val_Loss: 1012.7708  BEST VAL Loss: 1012.7708\n",
            "\n",
            "Epoch 299: Validation loss decreased (1012.770813 --> 1004.551941).\n",
            "\t Train_Loss: 389.3846 Val_Loss: 1004.5519  BEST VAL Loss: 1004.5519\n",
            "\n",
            "Epoch 300: Validation loss decreased (1004.551941 --> 996.432922).\n",
            "\t Train_Loss: 385.0757 Val_Loss: 996.4329  BEST VAL Loss: 996.4329\n",
            "\n",
            "Epoch 301: Validation loss decreased (996.432922 --> 988.413269).\n",
            "\t Train_Loss: 380.8369 Val_Loss: 988.4133  BEST VAL Loss: 988.4133\n",
            "\n",
            "Epoch 302: Validation loss decreased (988.413269 --> 980.490356).\n",
            "\t Train_Loss: 376.6670 Val_Loss: 980.4904  BEST VAL Loss: 980.4904\n",
            "\n",
            "Epoch 303: Validation loss decreased (980.490356 --> 972.663696).\n",
            "\t Train_Loss: 372.5652 Val_Loss: 972.6637  BEST VAL Loss: 972.6637\n",
            "\n",
            "Epoch 304: Validation loss decreased (972.663696 --> 964.931824).\n",
            "\t Train_Loss: 368.5295 Val_Loss: 964.9318  BEST VAL Loss: 964.9318\n",
            "\n",
            "Epoch 305: Validation loss decreased (964.931824 --> 957.292786).\n",
            "\t Train_Loss: 364.5597 Val_Loss: 957.2928  BEST VAL Loss: 957.2928\n",
            "\n",
            "Epoch 306: Validation loss decreased (957.292786 --> 949.746277).\n",
            "\t Train_Loss: 360.6542 Val_Loss: 949.7463  BEST VAL Loss: 949.7463\n",
            "\n",
            "Epoch 307: Validation loss decreased (949.746277 --> 942.289673).\n",
            "\t Train_Loss: 356.8123 Val_Loss: 942.2897  BEST VAL Loss: 942.2897\n",
            "\n",
            "Epoch 308: Validation loss decreased (942.289673 --> 934.923035).\n",
            "\t Train_Loss: 353.0327 Val_Loss: 934.9230  BEST VAL Loss: 934.9230\n",
            "\n",
            "Epoch 309: Validation loss decreased (934.923035 --> 927.645020).\n",
            "\t Train_Loss: 349.3147 Val_Loss: 927.6450  BEST VAL Loss: 927.6450\n",
            "\n",
            "Epoch 310: Validation loss decreased (927.645020 --> 920.453125).\n",
            "\t Train_Loss: 345.6571 Val_Loss: 920.4531  BEST VAL Loss: 920.4531\n",
            "\n",
            "Epoch 311: Validation loss decreased (920.453125 --> 913.347839).\n",
            "\t Train_Loss: 342.0587 Val_Loss: 913.3478  BEST VAL Loss: 913.3478\n",
            "\n",
            "Epoch 312: Validation loss decreased (913.347839 --> 906.325867).\n",
            "\t Train_Loss: 338.5192 Val_Loss: 906.3259  BEST VAL Loss: 906.3259\n",
            "\n",
            "Epoch 313: Validation loss decreased (906.325867 --> 899.388672).\n",
            "\t Train_Loss: 335.0366 Val_Loss: 899.3887  BEST VAL Loss: 899.3887\n",
            "\n",
            "Epoch 314: Validation loss decreased (899.388672 --> 892.533203).\n",
            "\t Train_Loss: 331.6113 Val_Loss: 892.5332  BEST VAL Loss: 892.5332\n",
            "\n",
            "Epoch 315: Validation loss decreased (892.533203 --> 885.759766).\n",
            "\t Train_Loss: 328.2416 Val_Loss: 885.7598  BEST VAL Loss: 885.7598\n",
            "\n",
            "Epoch 316: Validation loss decreased (885.759766 --> 879.066528).\n",
            "\t Train_Loss: 324.9267 Val_Loss: 879.0665  BEST VAL Loss: 879.0665\n",
            "\n",
            "Epoch 317: Validation loss decreased (879.066528 --> 872.451111).\n",
            "\t Train_Loss: 321.6658 Val_Loss: 872.4511  BEST VAL Loss: 872.4511\n",
            "\n",
            "Epoch 318: Validation loss decreased (872.451111 --> 865.915039).\n",
            "\t Train_Loss: 318.4581 Val_Loss: 865.9150  BEST VAL Loss: 865.9150\n",
            "\n",
            "Epoch 319: Validation loss decreased (865.915039 --> 859.455872).\n",
            "\t Train_Loss: 315.3025 Val_Loss: 859.4559  BEST VAL Loss: 859.4559\n",
            "\n",
            "Epoch 320: Validation loss decreased (859.455872 --> 853.072693).\n",
            "\t Train_Loss: 312.1986 Val_Loss: 853.0727  BEST VAL Loss: 853.0727\n",
            "\n",
            "Epoch 321: Validation loss decreased (853.072693 --> 846.764099).\n",
            "\t Train_Loss: 309.1452 Val_Loss: 846.7641  BEST VAL Loss: 846.7641\n",
            "\n",
            "Epoch 322: Validation loss decreased (846.764099 --> 840.530090).\n",
            "\t Train_Loss: 306.1419 Val_Loss: 840.5301  BEST VAL Loss: 840.5301\n",
            "\n",
            "Epoch 323: Validation loss decreased (840.530090 --> 834.369324).\n",
            "\t Train_Loss: 303.1876 Val_Loss: 834.3693  BEST VAL Loss: 834.3693\n",
            "\n",
            "Epoch 324: Validation loss decreased (834.369324 --> 828.280457).\n",
            "\t Train_Loss: 300.2816 Val_Loss: 828.2805  BEST VAL Loss: 828.2805\n",
            "\n",
            "Epoch 325: Validation loss decreased (828.280457 --> 822.262878).\n",
            "\t Train_Loss: 297.4229 Val_Loss: 822.2629  BEST VAL Loss: 822.2629\n",
            "\n",
            "Epoch 326: Validation loss decreased (822.262878 --> 816.316101).\n",
            "\t Train_Loss: 294.6112 Val_Loss: 816.3161  BEST VAL Loss: 816.3161\n",
            "\n",
            "Epoch 327: Validation loss decreased (816.316101 --> 810.438660).\n",
            "\t Train_Loss: 291.8456 Val_Loss: 810.4387  BEST VAL Loss: 810.4387\n",
            "\n",
            "Epoch 328: Validation loss decreased (810.438660 --> 804.629089).\n",
            "\t Train_Loss: 289.1255 Val_Loss: 804.6291  BEST VAL Loss: 804.6291\n",
            "\n",
            "Epoch 329: Validation loss decreased (804.629089 --> 798.887756).\n",
            "\t Train_Loss: 286.4497 Val_Loss: 798.8878  BEST VAL Loss: 798.8878\n",
            "\n",
            "Epoch 330: Validation loss decreased (798.887756 --> 793.213318).\n",
            "\t Train_Loss: 283.8183 Val_Loss: 793.2133  BEST VAL Loss: 793.2133\n",
            "\n",
            "Epoch 331: Validation loss decreased (793.213318 --> 787.604431).\n",
            "\t Train_Loss: 281.2298 Val_Loss: 787.6044  BEST VAL Loss: 787.6044\n",
            "\n",
            "Epoch 332: Validation loss decreased (787.604431 --> 782.061462).\n",
            "\t Train_Loss: 278.6841 Val_Loss: 782.0615  BEST VAL Loss: 782.0615\n",
            "\n",
            "Epoch 333: Validation loss decreased (782.061462 --> 776.582458).\n",
            "\t Train_Loss: 276.1803 Val_Loss: 776.5825  BEST VAL Loss: 776.5825\n",
            "\n",
            "Epoch 334: Validation loss decreased (776.582458 --> 771.166748).\n",
            "\t Train_Loss: 273.7180 Val_Loss: 771.1667  BEST VAL Loss: 771.1667\n",
            "\n",
            "Epoch 335: Validation loss decreased (771.166748 --> 765.814880).\n",
            "\t Train_Loss: 271.2961 Val_Loss: 765.8149  BEST VAL Loss: 765.8149\n",
            "\n",
            "Epoch 336: Validation loss decreased (765.814880 --> 760.523682).\n",
            "\t Train_Loss: 268.9146 Val_Loss: 760.5237  BEST VAL Loss: 760.5237\n",
            "\n",
            "Epoch 337: Validation loss decreased (760.523682 --> 755.294617).\n",
            "\t Train_Loss: 266.5718 Val_Loss: 755.2946  BEST VAL Loss: 755.2946\n",
            "\n",
            "Epoch 338: Validation loss decreased (755.294617 --> 750.125305).\n",
            "\t Train_Loss: 264.2686 Val_Loss: 750.1253  BEST VAL Loss: 750.1253\n",
            "\n",
            "Epoch 339: Validation loss decreased (750.125305 --> 745.016663).\n",
            "\t Train_Loss: 262.0031 Val_Loss: 745.0167  BEST VAL Loss: 745.0167\n",
            "\n",
            "Epoch 340: Validation loss decreased (745.016663 --> 739.966858).\n",
            "\t Train_Loss: 259.7754 Val_Loss: 739.9669  BEST VAL Loss: 739.9669\n",
            "\n",
            "Epoch 341: Validation loss decreased (739.966858 --> 734.974915).\n",
            "\t Train_Loss: 257.5848 Val_Loss: 734.9749  BEST VAL Loss: 734.9749\n",
            "\n",
            "Epoch 342: Validation loss decreased (734.974915 --> 730.041199).\n",
            "\t Train_Loss: 255.4307 Val_Loss: 730.0412  BEST VAL Loss: 730.0412\n",
            "\n",
            "Epoch 343: Validation loss decreased (730.041199 --> 725.163757).\n",
            "\t Train_Loss: 253.3122 Val_Loss: 725.1638  BEST VAL Loss: 725.1638\n",
            "\n",
            "Epoch 344: Validation loss decreased (725.163757 --> 720.342957).\n",
            "\t Train_Loss: 251.2291 Val_Loss: 720.3430  BEST VAL Loss: 720.3430\n",
            "\n",
            "Epoch 345: Validation loss decreased (720.342957 --> 715.576660).\n",
            "\t Train_Loss: 249.1810 Val_Loss: 715.5767  BEST VAL Loss: 715.5767\n",
            "\n",
            "Epoch 346: Validation loss decreased (715.576660 --> 710.866150).\n",
            "\t Train_Loss: 247.1669 Val_Loss: 710.8661  BEST VAL Loss: 710.8661\n",
            "\n",
            "Epoch 347: Validation loss decreased (710.866150 --> 706.209412).\n",
            "\t Train_Loss: 245.1869 Val_Loss: 706.2094  BEST VAL Loss: 706.2094\n",
            "\n",
            "Epoch 348: Validation loss decreased (706.209412 --> 701.607117).\n",
            "\t Train_Loss: 243.2397 Val_Loss: 701.6071  BEST VAL Loss: 701.6071\n",
            "\n",
            "Epoch 349: Validation loss decreased (701.607117 --> 697.057068).\n",
            "\t Train_Loss: 241.3255 Val_Loss: 697.0571  BEST VAL Loss: 697.0571\n",
            "\n",
            "Epoch 350: Validation loss decreased (697.057068 --> 692.559204).\n",
            "\t Train_Loss: 239.4434 Val_Loss: 692.5592  BEST VAL Loss: 692.5592\n",
            "\n",
            "Epoch 351: Validation loss decreased (692.559204 --> 688.113159).\n",
            "\t Train_Loss: 237.5927 Val_Loss: 688.1132  BEST VAL Loss: 688.1132\n",
            "\n",
            "Epoch 352: Validation loss decreased (688.113159 --> 683.718445).\n",
            "\t Train_Loss: 235.7738 Val_Loss: 683.7184  BEST VAL Loss: 683.7184\n",
            "\n",
            "Epoch 353: Validation loss decreased (683.718445 --> 679.373718).\n",
            "\t Train_Loss: 233.9851 Val_Loss: 679.3737  BEST VAL Loss: 679.3737\n",
            "\n",
            "Epoch 354: Validation loss decreased (679.373718 --> 675.078491).\n",
            "\t Train_Loss: 232.2269 Val_Loss: 675.0785  BEST VAL Loss: 675.0785\n",
            "\n",
            "Epoch 355: Validation loss decreased (675.078491 --> 670.833435).\n",
            "\t Train_Loss: 230.4982 Val_Loss: 670.8334  BEST VAL Loss: 670.8334\n",
            "\n",
            "Epoch 356: Validation loss decreased (670.833435 --> 666.636169).\n",
            "\t Train_Loss: 228.7991 Val_Loss: 666.6362  BEST VAL Loss: 666.6362\n",
            "\n",
            "Epoch 357: Validation loss decreased (666.636169 --> 662.487549).\n",
            "\t Train_Loss: 227.1289 Val_Loss: 662.4875  BEST VAL Loss: 662.4875\n",
            "\n",
            "Epoch 358: Validation loss decreased (662.487549 --> 658.386658).\n",
            "\t Train_Loss: 225.4869 Val_Loss: 658.3867  BEST VAL Loss: 658.3867\n",
            "\n",
            "Epoch 359: Validation loss decreased (658.386658 --> 654.331726).\n",
            "\t Train_Loss: 223.8728 Val_Loss: 654.3317  BEST VAL Loss: 654.3317\n",
            "\n",
            "Epoch 360: Validation loss decreased (654.331726 --> 650.324158).\n",
            "\t Train_Loss: 222.2863 Val_Loss: 650.3242  BEST VAL Loss: 650.3242\n",
            "\n",
            "Epoch 361: Validation loss decreased (650.324158 --> 646.361633).\n",
            "\t Train_Loss: 220.7270 Val_Loss: 646.3616  BEST VAL Loss: 646.3616\n",
            "\n",
            "Epoch 362: Validation loss decreased (646.361633 --> 642.445190).\n",
            "\t Train_Loss: 219.1942 Val_Loss: 642.4452  BEST VAL Loss: 642.4452\n",
            "\n",
            "Epoch 363: Validation loss decreased (642.445190 --> 638.572876).\n",
            "\t Train_Loss: 217.6879 Val_Loss: 638.5729  BEST VAL Loss: 638.5729\n",
            "\n",
            "Epoch 364: Validation loss decreased (638.572876 --> 634.745117).\n",
            "\t Train_Loss: 216.2073 Val_Loss: 634.7451  BEST VAL Loss: 634.7451\n",
            "\n",
            "Epoch 365: Validation loss decreased (634.745117 --> 630.960938).\n",
            "\t Train_Loss: 214.7521 Val_Loss: 630.9609  BEST VAL Loss: 630.9609\n",
            "\n",
            "Epoch 366: Validation loss decreased (630.960938 --> 627.220642).\n",
            "\t Train_Loss: 213.3219 Val_Loss: 627.2206  BEST VAL Loss: 627.2206\n",
            "\n",
            "Epoch 367: Validation loss decreased (627.220642 --> 623.522888).\n",
            "\t Train_Loss: 211.9167 Val_Loss: 623.5229  BEST VAL Loss: 623.5229\n",
            "\n",
            "Epoch 368: Validation loss decreased (623.522888 --> 619.866272).\n",
            "\t Train_Loss: 210.5355 Val_Loss: 619.8663  BEST VAL Loss: 619.8663\n",
            "\n",
            "Epoch 369: Validation loss decreased (619.866272 --> 616.252625).\n",
            "\t Train_Loss: 209.1780 Val_Loss: 616.2526  BEST VAL Loss: 616.2526\n",
            "\n",
            "Epoch 370: Validation loss decreased (616.252625 --> 612.679626).\n",
            "\t Train_Loss: 207.8444 Val_Loss: 612.6796  BEST VAL Loss: 612.6796\n",
            "\n",
            "Epoch 371: Validation loss decreased (612.679626 --> 609.147827).\n",
            "\t Train_Loss: 206.5336 Val_Loss: 609.1478  BEST VAL Loss: 609.1478\n",
            "\n",
            "Epoch 372: Validation loss decreased (609.147827 --> 605.656067).\n",
            "\t Train_Loss: 205.2459 Val_Loss: 605.6561  BEST VAL Loss: 605.6561\n",
            "\n",
            "Epoch 373: Validation loss decreased (605.656067 --> 602.204041).\n",
            "\t Train_Loss: 203.9804 Val_Loss: 602.2040  BEST VAL Loss: 602.2040\n",
            "\n",
            "Epoch 374: Validation loss decreased (602.204041 --> 598.791443).\n",
            "\t Train_Loss: 202.7368 Val_Loss: 598.7914  BEST VAL Loss: 598.7914\n",
            "\n",
            "Epoch 375: Validation loss decreased (598.791443 --> 595.418152).\n",
            "\t Train_Loss: 201.5153 Val_Loss: 595.4182  BEST VAL Loss: 595.4182\n",
            "\n",
            "Epoch 376: Validation loss decreased (595.418152 --> 592.082581).\n",
            "\t Train_Loss: 200.3150 Val_Loss: 592.0826  BEST VAL Loss: 592.0826\n",
            "\n",
            "Epoch 377: Validation loss decreased (592.082581 --> 588.786133).\n",
            "\t Train_Loss: 199.1356 Val_Loss: 588.7861  BEST VAL Loss: 588.7861\n",
            "\n",
            "Epoch 378: Validation loss decreased (588.786133 --> 585.526001).\n",
            "\t Train_Loss: 197.9769 Val_Loss: 585.5260  BEST VAL Loss: 585.5260\n",
            "\n",
            "Epoch 379: Validation loss decreased (585.526001 --> 582.303589).\n",
            "\t Train_Loss: 196.8386 Val_Loss: 582.3036  BEST VAL Loss: 582.3036\n",
            "\n",
            "Epoch 380: Validation loss decreased (582.303589 --> 579.118225).\n",
            "\t Train_Loss: 195.7205 Val_Loss: 579.1182  BEST VAL Loss: 579.1182\n",
            "\n",
            "Epoch 381: Validation loss decreased (579.118225 --> 575.968384).\n",
            "\t Train_Loss: 194.6218 Val_Loss: 575.9684  BEST VAL Loss: 575.9684\n",
            "\n",
            "Epoch 382: Validation loss decreased (575.968384 --> 572.854797).\n",
            "\t Train_Loss: 193.5426 Val_Loss: 572.8548  BEST VAL Loss: 572.8548\n",
            "\n",
            "Epoch 383: Validation loss decreased (572.854797 --> 569.776855).\n",
            "\t Train_Loss: 192.4826 Val_Loss: 569.7769  BEST VAL Loss: 569.7769\n",
            "\n",
            "Epoch 384: Validation loss decreased (569.776855 --> 566.733704).\n",
            "\t Train_Loss: 191.4413 Val_Loss: 566.7337  BEST VAL Loss: 566.7337\n",
            "\n",
            "Epoch 385: Validation loss decreased (566.733704 --> 563.725708).\n",
            "\t Train_Loss: 190.4185 Val_Loss: 563.7257  BEST VAL Loss: 563.7257\n",
            "\n",
            "Epoch 386: Validation loss decreased (563.725708 --> 560.751648).\n",
            "\t Train_Loss: 189.4138 Val_Loss: 560.7516  BEST VAL Loss: 560.7516\n",
            "\n",
            "Epoch 387: Validation loss decreased (560.751648 --> 557.811279).\n",
            "\t Train_Loss: 188.4272 Val_Loss: 557.8113  BEST VAL Loss: 557.8113\n",
            "\n",
            "Epoch 388: Validation loss decreased (557.811279 --> 554.905029).\n",
            "\t Train_Loss: 187.4580 Val_Loss: 554.9050  BEST VAL Loss: 554.9050\n",
            "\n",
            "Epoch 389: Validation loss decreased (554.905029 --> 552.030823).\n",
            "\t Train_Loss: 186.5061 Val_Loss: 552.0308  BEST VAL Loss: 552.0308\n",
            "\n",
            "Epoch 390: Validation loss decreased (552.030823 --> 549.189270).\n",
            "\t Train_Loss: 185.5715 Val_Loss: 549.1893  BEST VAL Loss: 549.1893\n",
            "\n",
            "Epoch 391: Validation loss decreased (549.189270 --> 546.381042).\n",
            "\t Train_Loss: 184.6532 Val_Loss: 546.3810  BEST VAL Loss: 546.3810\n",
            "\n",
            "Epoch 392: Validation loss decreased (546.381042 --> 543.604919).\n",
            "\t Train_Loss: 183.7517 Val_Loss: 543.6049  BEST VAL Loss: 543.6049\n",
            "\n",
            "Epoch 393: Validation loss decreased (543.604919 --> 540.859070).\n",
            "\t Train_Loss: 182.8664 Val_Loss: 540.8591  BEST VAL Loss: 540.8591\n",
            "\n",
            "Epoch 394: Validation loss decreased (540.859070 --> 538.145447).\n",
            "\t Train_Loss: 181.9968 Val_Loss: 538.1454  BEST VAL Loss: 538.1454\n",
            "\n",
            "Epoch 395: Validation loss decreased (538.145447 --> 535.462402).\n",
            "\t Train_Loss: 181.1431 Val_Loss: 535.4624  BEST VAL Loss: 535.4624\n",
            "\n",
            "Epoch 396: Validation loss decreased (535.462402 --> 532.810120).\n",
            "\t Train_Loss: 180.3050 Val_Loss: 532.8101  BEST VAL Loss: 532.8101\n",
            "\n",
            "Epoch 397: Validation loss decreased (532.810120 --> 530.187805).\n",
            "\t Train_Loss: 179.4818 Val_Loss: 530.1878  BEST VAL Loss: 530.1878\n",
            "\n",
            "Epoch 398: Validation loss decreased (530.187805 --> 527.596130).\n",
            "\t Train_Loss: 178.6736 Val_Loss: 527.5961  BEST VAL Loss: 527.5961\n",
            "\n",
            "Epoch 399: Validation loss decreased (527.596130 --> 525.032715).\n",
            "\t Train_Loss: 177.8801 Val_Loss: 525.0327  BEST VAL Loss: 525.0327\n",
            "\n",
            "Epoch 400: Validation loss decreased (525.032715 --> 522.498840).\n",
            "\t Train_Loss: 177.1010 Val_Loss: 522.4988  BEST VAL Loss: 522.4988\n",
            "\n",
            "Epoch 401: Validation loss decreased (522.498840 --> 519.993591).\n",
            "\t Train_Loss: 176.3360 Val_Loss: 519.9936  BEST VAL Loss: 519.9936\n",
            "\n",
            "Epoch 402: Validation loss decreased (519.993591 --> 517.517517).\n",
            "\t Train_Loss: 175.5852 Val_Loss: 517.5175  BEST VAL Loss: 517.5175\n",
            "\n",
            "Epoch 403: Validation loss decreased (517.517517 --> 515.069031).\n",
            "\t Train_Loss: 174.8480 Val_Loss: 515.0690  BEST VAL Loss: 515.0690\n",
            "\n",
            "Epoch 404: Validation loss decreased (515.069031 --> 512.648743).\n",
            "\t Train_Loss: 174.1244 Val_Loss: 512.6487  BEST VAL Loss: 512.6487\n",
            "\n",
            "Epoch 405: Validation loss decreased (512.648743 --> 510.255585).\n",
            "\t Train_Loss: 173.4140 Val_Loss: 510.2556  BEST VAL Loss: 510.2556\n",
            "\n",
            "Epoch 406: Validation loss decreased (510.255585 --> 507.889984).\n",
            "\t Train_Loss: 172.7166 Val_Loss: 507.8900  BEST VAL Loss: 507.8900\n",
            "\n",
            "Epoch 407: Validation loss decreased (507.889984 --> 505.551514).\n",
            "\t Train_Loss: 172.0321 Val_Loss: 505.5515  BEST VAL Loss: 505.5515\n",
            "\n",
            "Epoch 408: Validation loss decreased (505.551514 --> 503.239075).\n",
            "\t Train_Loss: 171.3603 Val_Loss: 503.2391  BEST VAL Loss: 503.2391\n",
            "\n",
            "Epoch 409: Validation loss decreased (503.239075 --> 500.953033).\n",
            "\t Train_Loss: 170.7009 Val_Loss: 500.9530  BEST VAL Loss: 500.9530\n",
            "\n",
            "Epoch 410: Validation loss decreased (500.953033 --> 498.692963).\n",
            "\t Train_Loss: 170.0536 Val_Loss: 498.6930  BEST VAL Loss: 498.6930\n",
            "\n",
            "Epoch 411: Validation loss decreased (498.692963 --> 496.459045).\n",
            "\t Train_Loss: 169.4184 Val_Loss: 496.4590  BEST VAL Loss: 496.4590\n",
            "\n",
            "Epoch 412: Validation loss decreased (496.459045 --> 494.249481).\n",
            "\t Train_Loss: 168.7948 Val_Loss: 494.2495  BEST VAL Loss: 494.2495\n",
            "\n",
            "Epoch 413: Validation loss decreased (494.249481 --> 492.066071).\n",
            "\t Train_Loss: 168.1830 Val_Loss: 492.0661  BEST VAL Loss: 492.0661\n",
            "\n",
            "Epoch 414: Validation loss decreased (492.066071 --> 489.907318).\n",
            "\t Train_Loss: 167.5825 Val_Loss: 489.9073  BEST VAL Loss: 489.9073\n",
            "\n",
            "Epoch 415: Validation loss decreased (489.907318 --> 487.773163).\n",
            "\t Train_Loss: 166.9932 Val_Loss: 487.7732  BEST VAL Loss: 487.7732\n",
            "\n",
            "Epoch 416: Validation loss decreased (487.773163 --> 485.663086).\n",
            "\t Train_Loss: 166.4150 Val_Loss: 485.6631  BEST VAL Loss: 485.6631\n",
            "\n",
            "Epoch 417: Validation loss decreased (485.663086 --> 483.576965).\n",
            "\t Train_Loss: 165.8475 Val_Loss: 483.5770  BEST VAL Loss: 483.5770\n",
            "\n",
            "Epoch 418: Validation loss decreased (483.576965 --> 481.514954).\n",
            "\t Train_Loss: 165.2907 Val_Loss: 481.5150  BEST VAL Loss: 481.5150\n",
            "\n",
            "Epoch 419: Validation loss decreased (481.514954 --> 479.475739).\n",
            "\t Train_Loss: 164.7445 Val_Loss: 479.4757  BEST VAL Loss: 479.4757\n",
            "\n",
            "Epoch 420: Validation loss decreased (479.475739 --> 477.460358).\n",
            "\t Train_Loss: 164.2085 Val_Loss: 477.4604  BEST VAL Loss: 477.4604\n",
            "\n",
            "Epoch 421: Validation loss decreased (477.460358 --> 475.467682).\n",
            "\t Train_Loss: 163.6827 Val_Loss: 475.4677  BEST VAL Loss: 475.4677\n",
            "\n",
            "Epoch 422: Validation loss decreased (475.467682 --> 473.497955).\n",
            "\t Train_Loss: 163.1665 Val_Loss: 473.4980  BEST VAL Loss: 473.4980\n",
            "\n",
            "Epoch 423: Validation loss decreased (473.497955 --> 471.549896).\n",
            "\t Train_Loss: 162.6604 Val_Loss: 471.5499  BEST VAL Loss: 471.5499\n",
            "\n",
            "Epoch 424: Validation loss decreased (471.549896 --> 469.624756).\n",
            "\t Train_Loss: 162.1637 Val_Loss: 469.6248  BEST VAL Loss: 469.6248\n",
            "\n",
            "Epoch 425: Validation loss decreased (469.624756 --> 467.721344).\n",
            "\t Train_Loss: 161.6765 Val_Loss: 467.7213  BEST VAL Loss: 467.7213\n",
            "\n",
            "Epoch 426: Validation loss decreased (467.721344 --> 465.839172).\n",
            "\t Train_Loss: 161.1985 Val_Loss: 465.8392  BEST VAL Loss: 465.8392\n",
            "\n",
            "Epoch 427: Validation loss decreased (465.839172 --> 463.978760).\n",
            "\t Train_Loss: 160.7297 Val_Loss: 463.9788  BEST VAL Loss: 463.9788\n",
            "\n",
            "Epoch 428: Validation loss decreased (463.978760 --> 462.139557).\n",
            "\t Train_Loss: 160.2698 Val_Loss: 462.1396  BEST VAL Loss: 462.1396\n",
            "\n",
            "Epoch 429: Validation loss decreased (462.139557 --> 460.321259).\n",
            "\t Train_Loss: 159.8186 Val_Loss: 460.3213  BEST VAL Loss: 460.3213\n",
            "\n",
            "Epoch 430: Validation loss decreased (460.321259 --> 458.524078).\n",
            "\t Train_Loss: 159.3760 Val_Loss: 458.5241  BEST VAL Loss: 458.5241\n",
            "\n",
            "Epoch 431: Validation loss decreased (458.524078 --> 456.746796).\n",
            "\t Train_Loss: 158.9418 Val_Loss: 456.7468  BEST VAL Loss: 456.7468\n",
            "\n",
            "Epoch 432: Validation loss decreased (456.746796 --> 454.989807).\n",
            "\t Train_Loss: 158.5159 Val_Loss: 454.9898  BEST VAL Loss: 454.9898\n",
            "\n",
            "Epoch 433: Validation loss decreased (454.989807 --> 453.252930).\n",
            "\t Train_Loss: 158.0978 Val_Loss: 453.2529  BEST VAL Loss: 453.2529\n",
            "\n",
            "Epoch 434: Validation loss decreased (453.252930 --> 451.536285).\n",
            "\t Train_Loss: 157.6873 Val_Loss: 451.5363  BEST VAL Loss: 451.5363\n",
            "\n",
            "Epoch 435: Validation loss decreased (451.536285 --> 449.840088).\n",
            "\t Train_Loss: 157.2833 Val_Loss: 449.8401  BEST VAL Loss: 449.8401\n",
            "\n",
            "Epoch 436: Validation loss decreased (449.840088 --> 448.169525).\n",
            "\t Train_Loss: 156.8824 Val_Loss: 448.1695  BEST VAL Loss: 448.1695\n",
            "\n",
            "Epoch 437: Validation loss decreased (448.169525 --> 446.610168).\n",
            "\t Train_Loss: 156.4642 Val_Loss: 446.6102  BEST VAL Loss: 446.6102\n",
            "\n",
            "Epoch 438: Validation loss decreased (446.610168 --> 446.138763).\n",
            "\t Train_Loss: 155.8362 Val_Loss: 446.1388  BEST VAL Loss: 446.1388\n",
            "\n",
            "Epoch 439: Validation loss did not decrease\n",
            "\t Train_Loss: 154.3141 Val_Loss: 458.2968  BEST VAL Loss: 446.1388\n",
            "\n",
            "Epoch 440: Validation loss decreased (446.138763 --> 445.212738).\n",
            "\t Train_Loss: 156.0521 Val_Loss: 445.2127  BEST VAL Loss: 445.2127\n",
            "\n",
            "Epoch 441: Validation loss decreased (445.212738 --> 440.877838).\n",
            "\t Train_Loss: 152.9928 Val_Loss: 440.8778  BEST VAL Loss: 440.8778\n",
            "\n",
            "Epoch 442: Validation loss decreased (440.877838 --> 438.727264).\n",
            "\t Train_Loss: 153.3383 Val_Loss: 438.7273  BEST VAL Loss: 438.7273\n",
            "\n",
            "Epoch 443: Validation loss decreased (438.727264 --> 436.981293).\n",
            "\t Train_Loss: 153.4857 Val_Loss: 436.9813  BEST VAL Loss: 436.9813\n",
            "\n",
            "Epoch 444: Validation loss decreased (436.981293 --> 435.361908).\n",
            "\t Train_Loss: 153.3422 Val_Loss: 435.3619  BEST VAL Loss: 435.3619\n",
            "\n",
            "Epoch 445: Validation loss decreased (435.361908 --> 433.812988).\n",
            "\t Train_Loss: 153.0583 Val_Loss: 433.8130  BEST VAL Loss: 433.8130\n",
            "\n",
            "Epoch 446: Validation loss decreased (433.812988 --> 432.355682).\n",
            "\t Train_Loss: 152.6734 Val_Loss: 432.3557  BEST VAL Loss: 432.3557\n",
            "\n",
            "Epoch 447: Validation loss decreased (432.355682 --> 431.390137).\n",
            "\t Train_Loss: 152.1027 Val_Loss: 431.3901  BEST VAL Loss: 431.3901\n",
            "\n",
            "Epoch 448: Validation loss did not decrease\n",
            "\t Train_Loss: 150.7202 Val_Loss: 435.0855  BEST VAL Loss: 431.3901\n",
            "\n",
            "Epoch 449: Validation loss did not decrease\n",
            "\t Train_Loss: 148.6698 Val_Loss: 435.1879  BEST VAL Loss: 431.3901\n",
            "\n",
            "Epoch 450: Validation loss did not decrease\n",
            "\t Train_Loss: 147.4498 Val_Loss: 433.3442  BEST VAL Loss: 431.3901\n",
            "\n",
            "Epoch 451: Validation loss did not decrease\n",
            "\t Train_Loss: 146.9731 Val_Loss: 435.0426  BEST VAL Loss: 431.3901\n",
            "\n",
            "Epoch 452: Validation loss decreased (431.390137 --> 430.980286).\n",
            "\t Train_Loss: 147.4263 Val_Loss: 430.9803  BEST VAL Loss: 430.9803\n",
            "\n",
            "Epoch 453: Validation loss decreased (430.980286 --> 429.553223).\n",
            "\t Train_Loss: 145.5802 Val_Loss: 429.5532  BEST VAL Loss: 429.5532\n",
            "\n",
            "Epoch 454: Validation loss decreased (429.553223 --> 425.496002).\n",
            "\t Train_Loss: 144.8405 Val_Loss: 425.4960  BEST VAL Loss: 425.4960\n",
            "\n",
            "Epoch 455: Validation loss decreased (425.496002 --> 421.675690).\n",
            "\t Train_Loss: 144.4131 Val_Loss: 421.6757  BEST VAL Loss: 421.6757\n",
            "\n",
            "Epoch 456: Validation loss decreased (421.675690 --> 420.236237).\n",
            "\t Train_Loss: 144.0900 Val_Loss: 420.2362  BEST VAL Loss: 420.2362\n",
            "\n",
            "Epoch 457: Validation loss did not decrease\n",
            "\t Train_Loss: 143.3013 Val_Loss: 420.7286  BEST VAL Loss: 420.2362\n",
            "\n",
            "Epoch 458: Validation loss did not decrease\n",
            "\t Train_Loss: 142.1222 Val_Loss: 421.7720  BEST VAL Loss: 420.2362\n",
            "\n",
            "Epoch 459: Validation loss did not decrease\n",
            "\t Train_Loss: 141.4480 Val_Loss: 420.8455  BEST VAL Loss: 420.2362\n",
            "\n",
            "Epoch 460: Validation loss decreased (420.236237 --> 418.186859).\n",
            "\t Train_Loss: 141.0254 Val_Loss: 418.1869  BEST VAL Loss: 418.1869\n",
            "\n",
            "Epoch 461: Validation loss decreased (418.186859 --> 414.788971).\n",
            "\t Train_Loss: 140.5013 Val_Loss: 414.7890  BEST VAL Loss: 414.7890\n",
            "\n",
            "Epoch 462: Validation loss decreased (414.788971 --> 412.701385).\n",
            "\t Train_Loss: 139.5495 Val_Loss: 412.7014  BEST VAL Loss: 412.7014\n",
            "\n",
            "Epoch 463: Validation loss decreased (412.701385 --> 411.626373).\n",
            "\t Train_Loss: 138.8589 Val_Loss: 411.6264  BEST VAL Loss: 411.6264\n",
            "\n",
            "Epoch 464: Validation loss decreased (411.626373 --> 409.467773).\n",
            "\t Train_Loss: 138.5796 Val_Loss: 409.4678  BEST VAL Loss: 409.4678\n",
            "\n",
            "Epoch 465: Validation loss decreased (409.467773 --> 407.353851).\n",
            "\t Train_Loss: 137.9335 Val_Loss: 407.3539  BEST VAL Loss: 407.3539\n",
            "\n",
            "Epoch 466: Validation loss decreased (407.353851 --> 406.428192).\n",
            "\t Train_Loss: 137.2004 Val_Loss: 406.4282  BEST VAL Loss: 406.4282\n",
            "\n",
            "Epoch 467: Validation loss decreased (406.428192 --> 406.256805).\n",
            "\t Train_Loss: 136.6029 Val_Loss: 406.2568  BEST VAL Loss: 406.2568\n",
            "\n",
            "Epoch 468: Validation loss decreased (406.256805 --> 405.517090).\n",
            "\t Train_Loss: 136.1358 Val_Loss: 405.5171  BEST VAL Loss: 405.5171\n",
            "\n",
            "Epoch 469: Validation loss decreased (405.517090 --> 403.318268).\n",
            "\t Train_Loss: 135.6878 Val_Loss: 403.3183  BEST VAL Loss: 403.3183\n",
            "\n",
            "Epoch 470: Validation loss decreased (403.318268 --> 400.475464).\n",
            "\t Train_Loss: 135.0474 Val_Loss: 400.4755  BEST VAL Loss: 400.4755\n",
            "\n",
            "Epoch 471: Validation loss decreased (400.475464 --> 398.125610).\n",
            "\t Train_Loss: 134.4645 Val_Loss: 398.1256  BEST VAL Loss: 398.1256\n",
            "\n",
            "Epoch 472: Validation loss decreased (398.125610 --> 396.534271).\n",
            "\t Train_Loss: 134.0847 Val_Loss: 396.5343  BEST VAL Loss: 396.5343\n",
            "\n",
            "Epoch 473: Validation loss decreased (396.534271 --> 395.555847).\n",
            "\t Train_Loss: 133.6396 Val_Loss: 395.5558  BEST VAL Loss: 395.5558\n",
            "\n",
            "Epoch 474: Validation loss decreased (395.555847 --> 394.847382).\n",
            "\t Train_Loss: 133.0809 Val_Loss: 394.8474  BEST VAL Loss: 394.8474\n",
            "\n",
            "Epoch 475: Validation loss decreased (394.847382 --> 393.886078).\n",
            "\t Train_Loss: 132.5824 Val_Loss: 393.8861  BEST VAL Loss: 393.8861\n",
            "\n",
            "Epoch 476: Validation loss decreased (393.886078 --> 392.387665).\n",
            "\t Train_Loss: 132.1606 Val_Loss: 392.3877  BEST VAL Loss: 392.3877\n",
            "\n",
            "Epoch 477: Validation loss decreased (392.387665 --> 390.578827).\n",
            "\t Train_Loss: 131.7195 Val_Loss: 390.5788  BEST VAL Loss: 390.5788\n",
            "\n",
            "Epoch 478: Validation loss decreased (390.578827 --> 389.972260).\n",
            "\t Train_Loss: 131.1032 Val_Loss: 389.9723  BEST VAL Loss: 389.9723\n",
            "\n",
            "Epoch 479: Validation loss did not decrease\n",
            "\t Train_Loss: 129.7247 Val_Loss: 405.5107  BEST VAL Loss: 389.9723\n",
            "\n",
            "Epoch 480: Validation loss decreased (389.972260 --> 388.859100).\n",
            "\t Train_Loss: 133.0290 Val_Loss: 388.8591  BEST VAL Loss: 388.8591\n",
            "\n",
            "Epoch 481: Validation loss decreased (388.859100 --> 384.463837).\n",
            "\t Train_Loss: 128.6450 Val_Loss: 384.4638  BEST VAL Loss: 384.4638\n",
            "\n",
            "Epoch 482: Validation loss decreased (384.463837 --> 382.911835).\n",
            "\t Train_Loss: 128.8277 Val_Loss: 382.9118  BEST VAL Loss: 382.9118\n",
            "\n",
            "Epoch 483: Validation loss did not decrease\n",
            "\t Train_Loss: 128.5335 Val_Loss: 383.4325  BEST VAL Loss: 382.9118\n",
            "\n",
            "Epoch 484: Validation loss did not decrease\n",
            "\t Train_Loss: 127.0524 Val_Loss: 403.7270  BEST VAL Loss: 382.9118\n",
            "\n",
            "Epoch 485: Validation loss did not decrease\n",
            "\t Train_Loss: 132.2314 Val_Loss: 384.0676  BEST VAL Loss: 382.9118\n",
            "\n",
            "Epoch 486: Validation loss decreased (382.911835 --> 377.696594).\n",
            "\t Train_Loss: 126.3758 Val_Loss: 377.6966  BEST VAL Loss: 377.6966\n",
            "\n",
            "Epoch 487: Validation loss decreased (377.696594 --> 376.089569).\n",
            "\t Train_Loss: 126.3820 Val_Loss: 376.0896  BEST VAL Loss: 376.0896\n",
            "\n",
            "Epoch 488: Validation loss did not decrease\n",
            "\t Train_Loss: 126.1574 Val_Loss: 380.8306  BEST VAL Loss: 376.0896\n",
            "\n",
            "Epoch 489: Validation loss decreased (376.089569 --> 376.064453).\n",
            "\t Train_Loss: 125.5422 Val_Loss: 376.0645  BEST VAL Loss: 376.0645\n",
            "\n",
            "Epoch 490: Validation loss decreased (376.064453 --> 373.714935).\n",
            "\t Train_Loss: 124.9293 Val_Loss: 373.7149  BEST VAL Loss: 373.7149\n",
            "\n",
            "Epoch 491: Validation loss decreased (373.714935 --> 372.983795).\n",
            "\t Train_Loss: 124.6194 Val_Loss: 372.9838  BEST VAL Loss: 372.9838\n",
            "\n",
            "Epoch 492: Validation loss did not decrease\n",
            "\t Train_Loss: 123.9206 Val_Loss: 374.0355  BEST VAL Loss: 372.9838\n",
            "\n",
            "Epoch 493: Validation loss decreased (372.983795 --> 370.994110).\n",
            "\t Train_Loss: 123.5692 Val_Loss: 370.9941  BEST VAL Loss: 370.9941\n",
            "\n",
            "Epoch 494: Validation loss decreased (370.994110 --> 368.403534).\n",
            "\t Train_Loss: 122.8895 Val_Loss: 368.4035  BEST VAL Loss: 368.4035\n",
            "\n",
            "Epoch 495: Validation loss decreased (368.403534 --> 367.445251).\n",
            "\t Train_Loss: 122.4242 Val_Loss: 367.4453  BEST VAL Loss: 367.4453\n",
            "\n",
            "Epoch 496: Validation loss did not decrease\n",
            "\t Train_Loss: 121.8244 Val_Loss: 367.4758  BEST VAL Loss: 367.4453\n",
            "\n",
            "Epoch 497: Validation loss decreased (367.445251 --> 366.189484).\n",
            "\t Train_Loss: 121.3060 Val_Loss: 366.1895  BEST VAL Loss: 366.1895\n",
            "\n",
            "Epoch 498: Validation loss decreased (366.189484 --> 363.414886).\n",
            "\t Train_Loss: 120.8216 Val_Loss: 363.4149  BEST VAL Loss: 363.4149\n",
            "\n",
            "Epoch 499: Validation loss decreased (363.414886 --> 361.463684).\n",
            "\t Train_Loss: 120.2494 Val_Loss: 361.4637  BEST VAL Loss: 361.4637\n",
            "\n",
            "Epoch 500: Validation loss decreased (361.463684 --> 360.540924).\n",
            "\t Train_Loss: 119.8227 Val_Loss: 360.5409  BEST VAL Loss: 360.5409\n",
            "\n",
            "Epoch 501: Validation loss decreased (360.540924 --> 360.067841).\n",
            "\t Train_Loss: 119.2967 Val_Loss: 360.0678  BEST VAL Loss: 360.0678\n",
            "\n",
            "Epoch 502: Validation loss decreased (360.067841 --> 358.749176).\n",
            "\t Train_Loss: 118.8655 Val_Loss: 358.7492  BEST VAL Loss: 358.7492\n",
            "\n",
            "Epoch 503: Validation loss decreased (358.749176 --> 356.913696).\n",
            "\t Train_Loss: 118.4065 Val_Loss: 356.9137  BEST VAL Loss: 356.9137\n",
            "\n",
            "Epoch 504: Validation loss decreased (356.913696 --> 355.387390).\n",
            "\t Train_Loss: 117.9664 Val_Loss: 355.3874  BEST VAL Loss: 355.3874\n",
            "\n",
            "Epoch 505: Validation loss decreased (355.387390 --> 354.222748).\n",
            "\t Train_Loss: 117.5869 Val_Loss: 354.2227  BEST VAL Loss: 354.2227\n",
            "\n",
            "Epoch 506: Validation loss decreased (354.222748 --> 353.229736).\n",
            "\t Train_Loss: 117.1531 Val_Loss: 353.2297  BEST VAL Loss: 353.2297\n",
            "\n",
            "Epoch 507: Validation loss decreased (353.229736 --> 351.970184).\n",
            "\t Train_Loss: 116.7655 Val_Loss: 351.9702  BEST VAL Loss: 351.9702\n",
            "\n",
            "Epoch 508: Validation loss decreased (351.970184 --> 350.378113).\n",
            "\t Train_Loss: 116.4092 Val_Loss: 350.3781  BEST VAL Loss: 350.3781\n",
            "\n",
            "Epoch 509: Validation loss decreased (350.378113 --> 348.922729).\n",
            "\t Train_Loss: 116.0290 Val_Loss: 348.9227  BEST VAL Loss: 348.9227\n",
            "\n",
            "Epoch 510: Validation loss decreased (348.922729 --> 347.811005).\n",
            "\t Train_Loss: 115.6817 Val_Loss: 347.8110  BEST VAL Loss: 347.8110\n",
            "\n",
            "Epoch 511: Validation loss decreased (347.811005 --> 346.921143).\n",
            "\t Train_Loss: 115.3360 Val_Loss: 346.9211  BEST VAL Loss: 346.9211\n",
            "\n",
            "Epoch 512: Validation loss decreased (346.921143 --> 345.975677).\n",
            "\t Train_Loss: 114.9944 Val_Loss: 345.9757  BEST VAL Loss: 345.9757\n",
            "\n",
            "Epoch 513: Validation loss decreased (345.975677 --> 344.747559).\n",
            "\t Train_Loss: 114.6787 Val_Loss: 344.7476  BEST VAL Loss: 344.7476\n",
            "\n",
            "Epoch 514: Validation loss decreased (344.747559 --> 343.328156).\n",
            "\t Train_Loss: 114.3566 Val_Loss: 343.3282  BEST VAL Loss: 343.3282\n",
            "\n",
            "Epoch 515: Validation loss decreased (343.328156 --> 341.978088).\n",
            "\t Train_Loss: 114.0386 Val_Loss: 341.9781  BEST VAL Loss: 341.9781\n",
            "\n",
            "Epoch 516: Validation loss decreased (341.978088 --> 340.836609).\n",
            "\t Train_Loss: 113.7438 Val_Loss: 340.8366  BEST VAL Loss: 340.8366\n",
            "\n",
            "Epoch 517: Validation loss decreased (340.836609 --> 339.875458).\n",
            "\t Train_Loss: 113.4458 Val_Loss: 339.8755  BEST VAL Loss: 339.8755\n",
            "\n",
            "Epoch 518: Validation loss decreased (339.875458 --> 338.948975).\n",
            "\t Train_Loss: 113.1469 Val_Loss: 338.9490  BEST VAL Loss: 338.9490\n",
            "\n",
            "Epoch 519: Validation loss decreased (338.948975 --> 337.896881).\n",
            "\t Train_Loss: 112.8655 Val_Loss: 337.8969  BEST VAL Loss: 337.8969\n",
            "\n",
            "Epoch 520: Validation loss decreased (337.896881 --> 336.707886).\n",
            "\t Train_Loss: 112.5869 Val_Loss: 336.7079  BEST VAL Loss: 336.7079\n",
            "\n",
            "Epoch 521: Validation loss decreased (336.707886 --> 335.504974).\n",
            "\t Train_Loss: 112.3045 Val_Loss: 335.5050  BEST VAL Loss: 335.5050\n",
            "\n",
            "Epoch 522: Validation loss decreased (335.504974 --> 334.401520).\n",
            "\t Train_Loss: 112.0294 Val_Loss: 334.4015  BEST VAL Loss: 334.4015\n",
            "\n",
            "Epoch 523: Validation loss decreased (334.401520 --> 333.425385).\n",
            "\t Train_Loss: 111.7542 Val_Loss: 333.4254  BEST VAL Loss: 333.4254\n",
            "\n",
            "Epoch 524: Validation loss decreased (333.425385 --> 332.526367).\n",
            "\t Train_Loss: 111.4659 Val_Loss: 332.5264  BEST VAL Loss: 332.5264\n",
            "\n",
            "Epoch 525: Validation loss decreased (332.526367 --> 331.642578).\n",
            "\t Train_Loss: 111.1510 Val_Loss: 331.6426  BEST VAL Loss: 331.6426\n",
            "\n",
            "Epoch 526: Validation loss decreased (331.642578 --> 330.972961).\n",
            "\t Train_Loss: 110.7410 Val_Loss: 330.9730  BEST VAL Loss: 330.9730\n",
            "\n",
            "Epoch 527: Validation loss did not decrease\n",
            "\t Train_Loss: 109.9539 Val_Loss: 333.6960  BEST VAL Loss: 330.9730\n",
            "\n",
            "Epoch 528: Validation loss did not decrease\n",
            "\t Train_Loss: 108.7535 Val_Loss: 338.6473  BEST VAL Loss: 330.9730\n",
            "\n",
            "Epoch 529: Validation loss decreased (330.972961 --> 328.814880).\n",
            "\t Train_Loss: 109.5366 Val_Loss: 328.8149  BEST VAL Loss: 328.8149\n",
            "\n",
            "Epoch 530: Validation loss decreased (328.814880 --> 326.800415).\n",
            "\t Train_Loss: 107.9075 Val_Loss: 326.8004  BEST VAL Loss: 326.8004\n",
            "\n",
            "Epoch 531: Validation loss did not decrease\n",
            "\t Train_Loss: 107.5165 Val_Loss: 331.7681  BEST VAL Loss: 326.8004\n",
            "\n",
            "Epoch 532: Validation loss did not decrease\n",
            "\t Train_Loss: 106.7510 Val_Loss: 328.2129  BEST VAL Loss: 326.8004\n",
            "\n",
            "Epoch 533: Validation loss decreased (326.800415 --> 325.485107).\n",
            "\t Train_Loss: 105.9839 Val_Loss: 325.4851  BEST VAL Loss: 325.4851\n",
            "\n",
            "Epoch 534: Validation loss decreased (325.485107 --> 325.256592).\n",
            "\t Train_Loss: 105.4045 Val_Loss: 325.2566  BEST VAL Loss: 325.2566\n",
            "\n",
            "Epoch 535: Validation loss did not decrease\n",
            "\t Train_Loss: 103.9569 Val_Loss: 329.1773  BEST VAL Loss: 325.2566\n",
            "\n",
            "Epoch 536: Validation loss did not decrease\n",
            "\t Train_Loss: 102.2960 Val_Loss: 346.6487  BEST VAL Loss: 325.2566\n",
            "\n",
            "Epoch 537: Validation loss did not decrease\n",
            "\t Train_Loss: 107.4051 Val_Loss: 325.9213  BEST VAL Loss: 325.2566\n",
            "\n",
            "Epoch 538: Validation loss decreased (325.256592 --> 319.734985).\n",
            "\t Train_Loss: 101.2896 Val_Loss: 319.7350  BEST VAL Loss: 319.7350\n",
            "\n",
            "Epoch 539: Validation loss decreased (319.734985 --> 319.134552).\n",
            "\t Train_Loss: 101.5756 Val_Loss: 319.1346  BEST VAL Loss: 319.1346\n",
            "\n",
            "Epoch 540: Validation loss did not decrease\n",
            "\t Train_Loss: 101.0339 Val_Loss: 322.2702  BEST VAL Loss: 319.1346\n",
            "\n",
            "Epoch 541: Validation loss did not decrease\n",
            "\t Train_Loss: 99.7206 Val_Loss: 337.5057  BEST VAL Loss: 319.1346\n",
            "\n",
            "Epoch 542: Validation loss did not decrease\n",
            "\t Train_Loss: 103.2478 Val_Loss: 322.9057  BEST VAL Loss: 319.1346\n",
            "\n",
            "Epoch 543: Validation loss decreased (319.134552 --> 318.046509).\n",
            "\t Train_Loss: 98.2623 Val_Loss: 318.0465  BEST VAL Loss: 318.0465\n",
            "\n",
            "Epoch 544: Validation loss decreased (318.046509 --> 316.386200).\n",
            "\t Train_Loss: 98.1834 Val_Loss: 316.3862  BEST VAL Loss: 316.3862\n",
            "\n",
            "Epoch 545: Validation loss decreased (316.386200 --> 316.324005).\n",
            "\t Train_Loss: 98.1962 Val_Loss: 316.3240  BEST VAL Loss: 316.3240\n",
            "\n",
            "Epoch 546: Validation loss did not decrease\n",
            "\t Train_Loss: 97.1899 Val_Loss: 319.4366  BEST VAL Loss: 316.3240\n",
            "\n",
            "Epoch 547: Validation loss decreased (316.324005 --> 315.448242).\n",
            "\t Train_Loss: 96.5411 Val_Loss: 315.4482  BEST VAL Loss: 315.4482\n",
            "\n",
            "Epoch 548: Validation loss decreased (315.448242 --> 313.854584).\n",
            "\t Train_Loss: 95.4444 Val_Loss: 313.8546  BEST VAL Loss: 313.8546\n",
            "\n",
            "Epoch 549: Validation loss decreased (313.854584 --> 312.105469).\n",
            "\t Train_Loss: 95.2999 Val_Loss: 312.1055  BEST VAL Loss: 312.1055\n",
            "\n",
            "Epoch 550: Validation loss decreased (312.105469 --> 310.240570).\n",
            "\t Train_Loss: 94.8879 Val_Loss: 310.2406  BEST VAL Loss: 310.2406\n",
            "\n",
            "Epoch 551: Validation loss decreased (310.240570 --> 309.374084).\n",
            "\t Train_Loss: 94.1706 Val_Loss: 309.3741  BEST VAL Loss: 309.3741\n",
            "\n",
            "Epoch 552: Validation loss decreased (309.374084 --> 309.077881).\n",
            "\t Train_Loss: 93.4585 Val_Loss: 309.0779  BEST VAL Loss: 309.0779\n",
            "\n",
            "Epoch 553: Validation loss decreased (309.077881 --> 308.185516).\n",
            "\t Train_Loss: 92.7792 Val_Loss: 308.1855  BEST VAL Loss: 308.1855\n",
            "\n",
            "Epoch 554: Validation loss decreased (308.185516 --> 307.675507).\n",
            "\t Train_Loss: 91.9523 Val_Loss: 307.6755  BEST VAL Loss: 307.6755\n",
            "\n",
            "Epoch 555: Validation loss decreased (307.675507 --> 305.299347).\n",
            "\t Train_Loss: 91.5636 Val_Loss: 305.2993  BEST VAL Loss: 305.2993\n",
            "\n",
            "Epoch 556: Validation loss decreased (305.299347 --> 303.707123).\n",
            "\t Train_Loss: 90.8008 Val_Loss: 303.7071  BEST VAL Loss: 303.7071\n",
            "\n",
            "Epoch 557: Validation loss decreased (303.707123 --> 302.919250).\n",
            "\t Train_Loss: 90.2456 Val_Loss: 302.9193  BEST VAL Loss: 302.9193\n",
            "\n",
            "Epoch 558: Validation loss did not decrease\n",
            "\t Train_Loss: 89.5491 Val_Loss: 303.6100  BEST VAL Loss: 302.9193\n",
            "\n",
            "Epoch 559: Validation loss decreased (302.919250 --> 301.440491).\n",
            "\t Train_Loss: 88.8046 Val_Loss: 301.4405  BEST VAL Loss: 301.4405\n",
            "\n",
            "Epoch 560: Validation loss decreased (301.440491 --> 300.053192).\n",
            "\t Train_Loss: 88.1454 Val_Loss: 300.0532  BEST VAL Loss: 300.0532\n",
            "\n",
            "Epoch 561: Validation loss decreased (300.053192 --> 299.243256).\n",
            "\t Train_Loss: 87.6070 Val_Loss: 299.2433  BEST VAL Loss: 299.2433\n",
            "\n",
            "Epoch 562: Validation loss decreased (299.243256 --> 298.944733).\n",
            "\t Train_Loss: 87.0491 Val_Loss: 298.9447  BEST VAL Loss: 298.9447\n",
            "\n",
            "Epoch 563: Validation loss decreased (298.944733 --> 297.098145).\n",
            "\t Train_Loss: 86.5410 Val_Loss: 297.0981  BEST VAL Loss: 297.0981\n",
            "\n",
            "Epoch 564: Validation loss decreased (297.098145 --> 295.266510).\n",
            "\t Train_Loss: 85.8843 Val_Loss: 295.2665  BEST VAL Loss: 295.2665\n",
            "\n",
            "Epoch 565: Validation loss decreased (295.266510 --> 294.570038).\n",
            "\t Train_Loss: 85.2579 Val_Loss: 294.5700  BEST VAL Loss: 294.5700\n",
            "\n",
            "Epoch 566: Validation loss decreased (294.570038 --> 294.250397).\n",
            "\t Train_Loss: 84.6436 Val_Loss: 294.2504  BEST VAL Loss: 294.2504\n",
            "\n",
            "Epoch 567: Validation loss decreased (294.250397 --> 292.610687).\n",
            "\t Train_Loss: 84.1163 Val_Loss: 292.6107  BEST VAL Loss: 292.6107\n",
            "\n",
            "Epoch 568: Validation loss decreased (292.610687 --> 291.260559).\n",
            "\t Train_Loss: 83.5413 Val_Loss: 291.2606  BEST VAL Loss: 291.2606\n",
            "\n",
            "Epoch 569: Validation loss decreased (291.260559 --> 290.421570).\n",
            "\t Train_Loss: 82.9729 Val_Loss: 290.4216  BEST VAL Loss: 290.4216\n",
            "\n",
            "Epoch 570: Validation loss decreased (290.421570 --> 289.207367).\n",
            "\t Train_Loss: 82.4030 Val_Loss: 289.2074  BEST VAL Loss: 289.2074\n",
            "\n",
            "Epoch 571: Validation loss decreased (289.207367 --> 287.359772).\n",
            "\t Train_Loss: 81.8984 Val_Loss: 287.3598  BEST VAL Loss: 287.3598\n",
            "\n",
            "Epoch 572: Validation loss decreased (287.359772 --> 286.417969).\n",
            "\t Train_Loss: 81.3747 Val_Loss: 286.4180  BEST VAL Loss: 286.4180\n",
            "\n",
            "Epoch 573: Validation loss decreased (286.417969 --> 286.098907).\n",
            "\t Train_Loss: 80.7984 Val_Loss: 286.0989  BEST VAL Loss: 286.0989\n",
            "\n",
            "Epoch 574: Validation loss decreased (286.098907 --> 284.977753).\n",
            "\t Train_Loss: 80.2720 Val_Loss: 284.9778  BEST VAL Loss: 284.9778\n",
            "\n",
            "Epoch 575: Validation loss decreased (284.977753 --> 283.945282).\n",
            "\t Train_Loss: 79.7619 Val_Loss: 283.9453  BEST VAL Loss: 283.9453\n",
            "\n",
            "Epoch 576: Validation loss decreased (283.945282 --> 283.113007).\n",
            "\t Train_Loss: 79.2959 Val_Loss: 283.1130  BEST VAL Loss: 283.1130\n",
            "\n",
            "Epoch 577: Validation loss decreased (283.113007 --> 282.067474).\n",
            "\t Train_Loss: 78.7811 Val_Loss: 282.0675  BEST VAL Loss: 282.0675\n",
            "\n",
            "Epoch 578: Validation loss decreased (282.067474 --> 280.440155).\n",
            "\t Train_Loss: 78.2919 Val_Loss: 280.4402  BEST VAL Loss: 280.4402\n",
            "\n",
            "Epoch 579: Validation loss decreased (280.440155 --> 279.142700).\n",
            "\t Train_Loss: 77.8182 Val_Loss: 279.1427  BEST VAL Loss: 279.1427\n",
            "\n",
            "Epoch 580: Validation loss decreased (279.142700 --> 278.387665).\n",
            "\t Train_Loss: 77.3822 Val_Loss: 278.3877  BEST VAL Loss: 278.3877\n",
            "\n",
            "Epoch 581: Validation loss decreased (278.387665 --> 277.610199).\n",
            "\t Train_Loss: 76.9227 Val_Loss: 277.6102  BEST VAL Loss: 277.6102\n",
            "\n",
            "Epoch 582: Validation loss decreased (277.610199 --> 276.518890).\n",
            "\t Train_Loss: 76.4703 Val_Loss: 276.5189  BEST VAL Loss: 276.5189\n",
            "\n",
            "Epoch 583: Validation loss decreased (276.518890 --> 275.579773).\n",
            "\t Train_Loss: 76.0243 Val_Loss: 275.5798  BEST VAL Loss: 275.5798\n",
            "\n",
            "Epoch 584: Validation loss decreased (275.579773 --> 274.789886).\n",
            "\t Train_Loss: 75.6047 Val_Loss: 274.7899  BEST VAL Loss: 274.7899\n",
            "\n",
            "Epoch 585: Validation loss decreased (274.789886 --> 273.787018).\n",
            "\t Train_Loss: 75.1837 Val_Loss: 273.7870  BEST VAL Loss: 273.7870\n",
            "\n",
            "Epoch 586: Validation loss decreased (273.787018 --> 272.436676).\n",
            "\t Train_Loss: 74.7741 Val_Loss: 272.4367  BEST VAL Loss: 272.4367\n",
            "\n",
            "Epoch 587: Validation loss decreased (272.436676 --> 271.218597).\n",
            "\t Train_Loss: 74.3668 Val_Loss: 271.2186  BEST VAL Loss: 271.2186\n",
            "\n",
            "Epoch 588: Validation loss decreased (271.218597 --> 270.329956).\n",
            "\t Train_Loss: 73.9763 Val_Loss: 270.3300  BEST VAL Loss: 270.3300\n",
            "\n",
            "Epoch 589: Validation loss decreased (270.329956 --> 269.509247).\n",
            "\t Train_Loss: 73.5836 Val_Loss: 269.5092  BEST VAL Loss: 269.5092\n",
            "\n",
            "Epoch 590: Validation loss decreased (269.509247 --> 268.488342).\n",
            "\t Train_Loss: 73.2003 Val_Loss: 268.4883  BEST VAL Loss: 268.4883\n",
            "\n",
            "Epoch 591: Validation loss decreased (268.488342 --> 267.456085).\n",
            "\t Train_Loss: 72.8188 Val_Loss: 267.4561  BEST VAL Loss: 267.4561\n",
            "\n",
            "Epoch 592: Validation loss decreased (267.456085 --> 266.557861).\n",
            "\t Train_Loss: 72.4482 Val_Loss: 266.5579  BEST VAL Loss: 266.5579\n",
            "\n",
            "Epoch 593: Validation loss decreased (266.557861 --> 265.691223).\n",
            "\t Train_Loss: 72.0783 Val_Loss: 265.6912  BEST VAL Loss: 265.6912\n",
            "\n",
            "Epoch 594: Validation loss decreased (265.691223 --> 264.669647).\n",
            "\t Train_Loss: 71.7173 Val_Loss: 264.6696  BEST VAL Loss: 264.6696\n",
            "\n",
            "Epoch 595: Validation loss decreased (264.669647 --> 263.568848).\n",
            "\t Train_Loss: 71.3625 Val_Loss: 263.5688  BEST VAL Loss: 263.5688\n",
            "\n",
            "Epoch 596: Validation loss decreased (263.568848 --> 262.582306).\n",
            "\t Train_Loss: 71.0139 Val_Loss: 262.5823  BEST VAL Loss: 262.5823\n",
            "\n",
            "Epoch 597: Validation loss decreased (262.582306 --> 261.710052).\n",
            "\t Train_Loss: 70.6686 Val_Loss: 261.7101  BEST VAL Loss: 261.7101\n",
            "\n",
            "Epoch 598: Validation loss decreased (261.710052 --> 260.790680).\n",
            "\t Train_Loss: 70.3248 Val_Loss: 260.7907  BEST VAL Loss: 260.7907\n",
            "\n",
            "Epoch 599: Validation loss decreased (260.790680 --> 259.743500).\n",
            "\t Train_Loss: 69.9872 Val_Loss: 259.7435  BEST VAL Loss: 259.7435\n",
            "\n",
            "Epoch 600: Validation loss decreased (259.743500 --> 258.688293).\n",
            "\t Train_Loss: 69.6531 Val_Loss: 258.6883  BEST VAL Loss: 258.6883\n",
            "\n",
            "Epoch 601: Validation loss decreased (258.688293 --> 257.731781).\n",
            "\t Train_Loss: 69.3248 Val_Loss: 257.7318  BEST VAL Loss: 257.7318\n",
            "\n",
            "Epoch 602: Validation loss decreased (257.731781 --> 256.839844).\n",
            "\t Train_Loss: 68.9984 Val_Loss: 256.8398  BEST VAL Loss: 256.8398\n",
            "\n",
            "Epoch 603: Validation loss decreased (256.839844 --> 255.900665).\n",
            "\t Train_Loss: 68.6762 Val_Loss: 255.9007  BEST VAL Loss: 255.9007\n",
            "\n",
            "Epoch 604: Validation loss decreased (255.900665 --> 254.894516).\n",
            "\t Train_Loss: 68.3592 Val_Loss: 254.8945  BEST VAL Loss: 254.8945\n",
            "\n",
            "Epoch 605: Validation loss decreased (254.894516 --> 253.912643).\n",
            "\t Train_Loss: 68.0459 Val_Loss: 253.9126  BEST VAL Loss: 253.9126\n",
            "\n",
            "Epoch 606: Validation loss decreased (253.912643 --> 253.009750).\n",
            "\t Train_Loss: 67.7360 Val_Loss: 253.0098  BEST VAL Loss: 253.0098\n",
            "\n",
            "Epoch 607: Validation loss decreased (253.009750 --> 252.141113).\n",
            "\t Train_Loss: 67.4271 Val_Loss: 252.1411  BEST VAL Loss: 252.1411\n",
            "\n",
            "Epoch 608: Validation loss decreased (252.141113 --> 251.231110).\n",
            "\t Train_Loss: 67.1218 Val_Loss: 251.2311  BEST VAL Loss: 251.2311\n",
            "\n",
            "Epoch 609: Validation loss decreased (251.231110 --> 250.286331).\n",
            "\t Train_Loss: 66.8205 Val_Loss: 250.2863  BEST VAL Loss: 250.2863\n",
            "\n",
            "Epoch 610: Validation loss decreased (250.286331 --> 249.371582).\n",
            "\t Train_Loss: 66.5231 Val_Loss: 249.3716  BEST VAL Loss: 249.3716\n",
            "\n",
            "Epoch 611: Validation loss decreased (249.371582 --> 248.508453).\n",
            "\t Train_Loss: 66.2289 Val_Loss: 248.5085  BEST VAL Loss: 248.5085\n",
            "\n",
            "Epoch 612: Validation loss decreased (248.508453 --> 247.650757).\n",
            "\t Train_Loss: 65.9364 Val_Loss: 247.6508  BEST VAL Loss: 247.6508\n",
            "\n",
            "Epoch 613: Validation loss decreased (247.650757 --> 246.745071).\n",
            "\t Train_Loss: 65.6469 Val_Loss: 246.7451  BEST VAL Loss: 246.7451\n",
            "\n",
            "Epoch 614: Validation loss decreased (246.745071 --> 245.804733).\n",
            "\t Train_Loss: 65.3601 Val_Loss: 245.8047  BEST VAL Loss: 245.8047\n",
            "\n",
            "Epoch 615: Validation loss decreased (245.804733 --> 244.891312).\n",
            "\t Train_Loss: 65.0763 Val_Loss: 244.8913  BEST VAL Loss: 244.8913\n",
            "\n",
            "Epoch 616: Validation loss decreased (244.891312 --> 244.033646).\n",
            "\t Train_Loss: 64.7954 Val_Loss: 244.0336  BEST VAL Loss: 244.0336\n",
            "\n",
            "Epoch 617: Validation loss decreased (244.033646 --> 243.199463).\n",
            "\t Train_Loss: 64.5167 Val_Loss: 243.1995  BEST VAL Loss: 243.1995\n",
            "\n",
            "Epoch 618: Validation loss decreased (243.199463 --> 242.343185).\n",
            "\t Train_Loss: 64.2407 Val_Loss: 242.3432  BEST VAL Loss: 242.3432\n",
            "\n",
            "Epoch 619: Validation loss decreased (242.343185 --> 241.465652).\n",
            "\t Train_Loss: 63.9675 Val_Loss: 241.4657  BEST VAL Loss: 241.4657\n",
            "\n",
            "Epoch 620: Validation loss decreased (241.465652 --> 240.606796).\n",
            "\t Train_Loss: 63.6968 Val_Loss: 240.6068  BEST VAL Loss: 240.6068\n",
            "\n",
            "Epoch 621: Validation loss decreased (240.606796 --> 239.788605).\n",
            "\t Train_Loss: 63.4288 Val_Loss: 239.7886  BEST VAL Loss: 239.7886\n",
            "\n",
            "Epoch 622: Validation loss decreased (239.788605 --> 238.989899).\n",
            "\t Train_Loss: 63.1629 Val_Loss: 238.9899  BEST VAL Loss: 238.9899\n",
            "\n",
            "Epoch 623: Validation loss decreased (238.989899 --> 238.174805).\n",
            "\t Train_Loss: 62.8995 Val_Loss: 238.1748  BEST VAL Loss: 238.1748\n",
            "\n",
            "Epoch 624: Validation loss decreased (238.174805 --> 237.337479).\n",
            "\t Train_Loss: 62.6385 Val_Loss: 237.3375  BEST VAL Loss: 237.3375\n",
            "\n",
            "Epoch 625: Validation loss decreased (237.337479 --> 236.505249).\n",
            "\t Train_Loss: 62.3801 Val_Loss: 236.5052  BEST VAL Loss: 236.5052\n",
            "\n",
            "Epoch 626: Validation loss decreased (236.505249 --> 235.701691).\n",
            "\t Train_Loss: 62.1242 Val_Loss: 235.7017  BEST VAL Loss: 235.7017\n",
            "\n",
            "Epoch 627: Validation loss decreased (235.701691 --> 234.919556).\n",
            "\t Train_Loss: 61.8704 Val_Loss: 234.9196  BEST VAL Loss: 234.9196\n",
            "\n",
            "Epoch 628: Validation loss decreased (234.919556 --> 234.135986).\n",
            "\t Train_Loss: 61.6189 Val_Loss: 234.1360  BEST VAL Loss: 234.1360\n",
            "\n",
            "Epoch 629: Validation loss decreased (234.135986 --> 233.343246).\n",
            "\t Train_Loss: 61.3697 Val_Loss: 233.3432  BEST VAL Loss: 233.3432\n",
            "\n",
            "Epoch 630: Validation loss decreased (233.343246 --> 232.557770).\n",
            "\t Train_Loss: 61.1227 Val_Loss: 232.5578  BEST VAL Loss: 232.5578\n",
            "\n",
            "Epoch 631: Validation loss decreased (232.557770 --> 231.794388).\n",
            "\t Train_Loss: 60.8782 Val_Loss: 231.7944  BEST VAL Loss: 231.7944\n",
            "\n",
            "Epoch 632: Validation loss decreased (231.794388 --> 231.045731).\n",
            "\t Train_Loss: 60.6358 Val_Loss: 231.0457  BEST VAL Loss: 231.0457\n",
            "\n",
            "Epoch 633: Validation loss decreased (231.045731 --> 230.292770).\n",
            "\t Train_Loss: 60.3955 Val_Loss: 230.2928  BEST VAL Loss: 230.2928\n",
            "\n",
            "Epoch 634: Validation loss decreased (230.292770 --> 229.528473).\n",
            "\t Train_Loss: 60.1575 Val_Loss: 229.5285  BEST VAL Loss: 229.5285\n",
            "\n",
            "Epoch 635: Validation loss decreased (229.528473 --> 228.768463).\n",
            "\t Train_Loss: 59.9215 Val_Loss: 228.7685  BEST VAL Loss: 228.7685\n",
            "\n",
            "Epoch 636: Validation loss decreased (228.768463 --> 228.031601).\n",
            "\t Train_Loss: 59.6877 Val_Loss: 228.0316  BEST VAL Loss: 228.0316\n",
            "\n",
            "Epoch 637: Validation loss decreased (228.031601 --> 227.317627).\n",
            "\t Train_Loss: 59.4560 Val_Loss: 227.3176  BEST VAL Loss: 227.3176\n",
            "\n",
            "Epoch 638: Validation loss decreased (227.317627 --> 226.609253).\n",
            "\t Train_Loss: 59.2262 Val_Loss: 226.6093  BEST VAL Loss: 226.6093\n",
            "\n",
            "Epoch 639: Validation loss decreased (226.609253 --> 225.892288).\n",
            "\t Train_Loss: 58.9986 Val_Loss: 225.8923  BEST VAL Loss: 225.8923\n",
            "\n",
            "Epoch 640: Validation loss decreased (225.892288 --> 225.170761).\n",
            "\t Train_Loss: 58.7730 Val_Loss: 225.1708  BEST VAL Loss: 225.1708\n",
            "\n",
            "Epoch 641: Validation loss decreased (225.170761 --> 224.457321).\n",
            "\t Train_Loss: 58.5494 Val_Loss: 224.4573  BEST VAL Loss: 224.4573\n",
            "\n",
            "Epoch 642: Validation loss decreased (224.457321 --> 223.756027).\n",
            "\t Train_Loss: 58.3277 Val_Loss: 223.7560  BEST VAL Loss: 223.7560\n",
            "\n",
            "Epoch 643: Validation loss decreased (223.756027 --> 223.059128).\n",
            "\t Train_Loss: 58.1080 Val_Loss: 223.0591  BEST VAL Loss: 223.0591\n",
            "\n",
            "Epoch 644: Validation loss decreased (223.059128 --> 222.361526).\n",
            "\t Train_Loss: 57.8901 Val_Loss: 222.3615  BEST VAL Loss: 222.3615\n",
            "\n",
            "Epoch 645: Validation loss decreased (222.361526 --> 221.669113).\n",
            "\t Train_Loss: 57.6742 Val_Loss: 221.6691  BEST VAL Loss: 221.6691\n",
            "\n",
            "Epoch 646: Validation loss decreased (221.669113 --> 220.990845).\n",
            "\t Train_Loss: 57.4602 Val_Loss: 220.9908  BEST VAL Loss: 220.9908\n",
            "\n",
            "Epoch 647: Validation loss decreased (220.990845 --> 220.326523).\n",
            "\t Train_Loss: 57.2481 Val_Loss: 220.3265  BEST VAL Loss: 220.3265\n",
            "\n",
            "Epoch 648: Validation loss decreased (220.326523 --> 219.665970).\n",
            "\t Train_Loss: 57.0377 Val_Loss: 219.6660  BEST VAL Loss: 219.6660\n",
            "\n",
            "Epoch 649: Validation loss decreased (219.665970 --> 219.002197).\n",
            "\t Train_Loss: 56.8292 Val_Loss: 219.0022  BEST VAL Loss: 219.0022\n",
            "\n",
            "Epoch 650: Validation loss decreased (219.002197 --> 218.338242).\n",
            "\t Train_Loss: 56.6224 Val_Loss: 218.3382  BEST VAL Loss: 218.3382\n",
            "\n",
            "Epoch 651: Validation loss decreased (218.338242 --> 217.680817).\n",
            "\t Train_Loss: 56.4175 Val_Loss: 217.6808  BEST VAL Loss: 217.6808\n",
            "\n",
            "Epoch 652: Validation loss decreased (217.680817 --> 217.028610).\n",
            "\t Train_Loss: 56.2142 Val_Loss: 217.0286  BEST VAL Loss: 217.0286\n",
            "\n",
            "Epoch 653: Validation loss decreased (217.028610 --> 216.376221).\n",
            "\t Train_Loss: 56.0127 Val_Loss: 216.3762  BEST VAL Loss: 216.3762\n",
            "\n",
            "Epoch 654: Validation loss decreased (216.376221 --> 215.722824).\n",
            "\t Train_Loss: 55.8129 Val_Loss: 215.7228  BEST VAL Loss: 215.7228\n",
            "\n",
            "Epoch 655: Validation loss decreased (215.722824 --> 215.076691).\n",
            "\t Train_Loss: 55.6148 Val_Loss: 215.0767  BEST VAL Loss: 215.0767\n",
            "\n",
            "Epoch 656: Validation loss decreased (215.076691 --> 214.443192).\n",
            "\t Train_Loss: 55.4184 Val_Loss: 214.4432  BEST VAL Loss: 214.4432\n",
            "\n",
            "Epoch 657: Validation loss decreased (214.443192 --> 213.818832).\n",
            "\t Train_Loss: 55.2236 Val_Loss: 213.8188  BEST VAL Loss: 213.8188\n",
            "\n",
            "Epoch 658: Validation loss decreased (213.818832 --> 213.195541).\n",
            "\t Train_Loss: 55.0304 Val_Loss: 213.1955  BEST VAL Loss: 213.1955\n",
            "\n",
            "Epoch 659: Validation loss decreased (213.195541 --> 212.571152).\n",
            "\t Train_Loss: 54.8390 Val_Loss: 212.5712  BEST VAL Loss: 212.5712\n",
            "\n",
            "Epoch 660: Validation loss decreased (212.571152 --> 211.949585).\n",
            "\t Train_Loss: 54.6490 Val_Loss: 211.9496  BEST VAL Loss: 211.9496\n",
            "\n",
            "Epoch 661: Validation loss decreased (211.949585 --> 211.331909).\n",
            "\t Train_Loss: 54.4606 Val_Loss: 211.3319  BEST VAL Loss: 211.3319\n",
            "\n",
            "Epoch 662: Validation loss decreased (211.331909 --> 210.715378).\n",
            "\t Train_Loss: 54.2738 Val_Loss: 210.7154  BEST VAL Loss: 210.7154\n",
            "\n",
            "Epoch 663: Validation loss decreased (210.715378 --> 210.099274).\n",
            "\t Train_Loss: 54.0886 Val_Loss: 210.0993  BEST VAL Loss: 210.0993\n",
            "\n",
            "Epoch 664: Validation loss decreased (210.099274 --> 209.489120).\n",
            "\t Train_Loss: 53.9048 Val_Loss: 209.4891  BEST VAL Loss: 209.4891\n",
            "\n",
            "Epoch 665: Validation loss decreased (209.489120 --> 208.887360).\n",
            "\t Train_Loss: 53.7226 Val_Loss: 208.8874  BEST VAL Loss: 208.8874\n",
            "\n",
            "Epoch 666: Validation loss decreased (208.887360 --> 208.290573).\n",
            "\t Train_Loss: 53.5418 Val_Loss: 208.2906  BEST VAL Loss: 208.2906\n",
            "\n",
            "Epoch 667: Validation loss decreased (208.290573 --> 207.694336).\n",
            "\t Train_Loss: 53.3625 Val_Loss: 207.6943  BEST VAL Loss: 207.6943\n",
            "\n",
            "Epoch 668: Validation loss decreased (207.694336 --> 207.099030).\n",
            "\t Train_Loss: 53.1846 Val_Loss: 207.0990  BEST VAL Loss: 207.0990\n",
            "\n",
            "Epoch 669: Validation loss decreased (207.099030 --> 206.508041).\n",
            "\t Train_Loss: 53.0082 Val_Loss: 206.5080  BEST VAL Loss: 206.5080\n",
            "\n",
            "Epoch 670: Validation loss decreased (206.508041 --> 205.920853).\n",
            "\t Train_Loss: 52.8332 Val_Loss: 205.9209  BEST VAL Loss: 205.9209\n",
            "\n",
            "Epoch 671: Validation loss decreased (205.920853 --> 205.335556).\n",
            "\t Train_Loss: 52.6595 Val_Loss: 205.3356  BEST VAL Loss: 205.3356\n",
            "\n",
            "Epoch 672: Validation loss decreased (205.335556 --> 204.752563).\n",
            "\t Train_Loss: 52.4873 Val_Loss: 204.7526  BEST VAL Loss: 204.7526\n",
            "\n",
            "Epoch 673: Validation loss decreased (204.752563 --> 204.174850).\n",
            "\t Train_Loss: 52.3164 Val_Loss: 204.1749  BEST VAL Loss: 204.1749\n",
            "\n",
            "Epoch 674: Validation loss decreased (204.174850 --> 203.602081).\n",
            "\t Train_Loss: 52.1468 Val_Loss: 203.6021  BEST VAL Loss: 203.6021\n",
            "\n",
            "Epoch 675: Validation loss decreased (203.602081 --> 203.031601).\n",
            "\t Train_Loss: 51.9786 Val_Loss: 203.0316  BEST VAL Loss: 203.0316\n",
            "\n",
            "Epoch 676: Validation loss decreased (203.031601 --> 202.462723).\n",
            "\t Train_Loss: 51.8116 Val_Loss: 202.4627  BEST VAL Loss: 202.4627\n",
            "\n",
            "Epoch 677: Validation loss decreased (202.462723 --> 201.896896).\n",
            "\t Train_Loss: 51.6460 Val_Loss: 201.8969  BEST VAL Loss: 201.8969\n",
            "\n",
            "Epoch 678: Validation loss decreased (201.896896 --> 201.335358).\n",
            "\t Train_Loss: 51.4816 Val_Loss: 201.3354  BEST VAL Loss: 201.3354\n",
            "\n",
            "Epoch 679: Validation loss decreased (201.335358 --> 200.776474).\n",
            "\t Train_Loss: 51.3185 Val_Loss: 200.7765  BEST VAL Loss: 200.7765\n",
            "\n",
            "Epoch 680: Validation loss decreased (200.776474 --> 200.219772).\n",
            "\t Train_Loss: 51.1566 Val_Loss: 200.2198  BEST VAL Loss: 200.2198\n",
            "\n",
            "Epoch 681: Validation loss decreased (200.219772 --> 199.666992).\n",
            "\t Train_Loss: 50.9959 Val_Loss: 199.6670  BEST VAL Loss: 199.6670\n",
            "\n",
            "Epoch 682: Validation loss decreased (199.666992 --> 199.119141).\n",
            "\t Train_Loss: 50.8365 Val_Loss: 199.1191  BEST VAL Loss: 199.1191\n",
            "\n",
            "Epoch 683: Validation loss decreased (199.119141 --> 198.574738).\n",
            "\t Train_Loss: 50.6782 Val_Loss: 198.5747  BEST VAL Loss: 198.5747\n",
            "\n",
            "Epoch 684: Validation loss decreased (198.574738 --> 198.032135).\n",
            "\t Train_Loss: 50.5212 Val_Loss: 198.0321  BEST VAL Loss: 198.0321\n",
            "\n",
            "Epoch 685: Validation loss decreased (198.032135 --> 197.492661).\n",
            "\t Train_Loss: 50.3652 Val_Loss: 197.4927  BEST VAL Loss: 197.4927\n",
            "\n",
            "Epoch 686: Validation loss decreased (197.492661 --> 196.956100).\n",
            "\t Train_Loss: 50.2105 Val_Loss: 196.9561  BEST VAL Loss: 196.9561\n",
            "\n",
            "Epoch 687: Validation loss decreased (196.956100 --> 196.422165).\n",
            "\t Train_Loss: 50.0568 Val_Loss: 196.4222  BEST VAL Loss: 196.4222\n",
            "\n",
            "Epoch 688: Validation loss decreased (196.422165 --> 195.890427).\n",
            "\t Train_Loss: 49.9043 Val_Loss: 195.8904  BEST VAL Loss: 195.8904\n",
            "\n",
            "Epoch 689: Validation loss decreased (195.890427 --> 195.361832).\n",
            "\t Train_Loss: 49.7529 Val_Loss: 195.3618  BEST VAL Loss: 195.3618\n",
            "\n",
            "Epoch 690: Validation loss decreased (195.361832 --> 194.837814).\n",
            "\t Train_Loss: 49.6026 Val_Loss: 194.8378  BEST VAL Loss: 194.8378\n",
            "\n",
            "Epoch 691: Validation loss decreased (194.837814 --> 194.317627).\n",
            "\t Train_Loss: 49.4534 Val_Loss: 194.3176  BEST VAL Loss: 194.3176\n",
            "\n",
            "Epoch 692: Validation loss decreased (194.317627 --> 193.800491).\n",
            "\t Train_Loss: 49.3052 Val_Loss: 193.8005  BEST VAL Loss: 193.8005\n",
            "\n",
            "Epoch 693: Validation loss decreased (193.800491 --> 193.286438).\n",
            "\t Train_Loss: 49.1581 Val_Loss: 193.2864  BEST VAL Loss: 193.2864\n",
            "\n",
            "Epoch 694: Validation loss decreased (193.286438 --> 192.775238).\n",
            "\t Train_Loss: 49.0121 Val_Loss: 192.7752  BEST VAL Loss: 192.7752\n",
            "\n",
            "Epoch 695: Validation loss decreased (192.775238 --> 192.266296).\n",
            "\t Train_Loss: 48.8670 Val_Loss: 192.2663  BEST VAL Loss: 192.2663\n",
            "\n",
            "Epoch 696: Validation loss decreased (192.266296 --> 191.759399).\n",
            "\t Train_Loss: 48.7230 Val_Loss: 191.7594  BEST VAL Loss: 191.7594\n",
            "\n",
            "Epoch 697: Validation loss decreased (191.759399 --> 191.255280).\n",
            "\t Train_Loss: 48.5800 Val_Loss: 191.2553  BEST VAL Loss: 191.2553\n",
            "\n",
            "Epoch 698: Validation loss decreased (191.255280 --> 190.754883).\n",
            "\t Train_Loss: 48.4380 Val_Loss: 190.7549  BEST VAL Loss: 190.7549\n",
            "\n",
            "Epoch 699: Validation loss decreased (190.754883 --> 190.257858).\n",
            "\t Train_Loss: 48.2970 Val_Loss: 190.2579  BEST VAL Loss: 190.2579\n",
            "\n",
            "Epoch 700: Validation loss decreased (190.257858 --> 189.763901).\n",
            "\t Train_Loss: 48.1570 Val_Loss: 189.7639  BEST VAL Loss: 189.7639\n",
            "\n",
            "Epoch 701: Validation loss decreased (189.763901 --> 189.273056).\n",
            "\t Train_Loss: 48.0179 Val_Loss: 189.2731  BEST VAL Loss: 189.2731\n",
            "\n",
            "Epoch 702: Validation loss decreased (189.273056 --> 188.785187).\n",
            "\t Train_Loss: 47.8798 Val_Loss: 188.7852  BEST VAL Loss: 188.7852\n",
            "\n",
            "Epoch 703: Validation loss decreased (188.785187 --> 188.299911).\n",
            "\t Train_Loss: 47.7426 Val_Loss: 188.2999  BEST VAL Loss: 188.2999\n",
            "\n",
            "Epoch 704: Validation loss decreased (188.299911 --> 187.816940).\n",
            "\t Train_Loss: 47.6064 Val_Loss: 187.8169  BEST VAL Loss: 187.8169\n",
            "\n",
            "Epoch 705: Validation loss decreased (187.816940 --> 187.336624).\n",
            "\t Train_Loss: 47.4711 Val_Loss: 187.3366  BEST VAL Loss: 187.3366\n",
            "\n",
            "Epoch 706: Validation loss decreased (187.336624 --> 186.859711).\n",
            "\t Train_Loss: 47.3367 Val_Loss: 186.8597  BEST VAL Loss: 186.8597\n",
            "\n",
            "Epoch 707: Validation loss decreased (186.859711 --> 186.385666).\n",
            "\t Train_Loss: 47.2032 Val_Loss: 186.3857  BEST VAL Loss: 186.3857\n",
            "\n",
            "Epoch 708: Validation loss decreased (186.385666 --> 185.914139).\n",
            "\t Train_Loss: 47.0706 Val_Loss: 185.9141  BEST VAL Loss: 185.9141\n",
            "\n",
            "Epoch 709: Validation loss decreased (185.914139 --> 185.445358).\n",
            "\t Train_Loss: 46.9389 Val_Loss: 185.4454  BEST VAL Loss: 185.4454\n",
            "\n",
            "Epoch 710: Validation loss decreased (185.445358 --> 184.979416).\n",
            "\t Train_Loss: 46.8080 Val_Loss: 184.9794  BEST VAL Loss: 184.9794\n",
            "\n",
            "Epoch 711: Validation loss decreased (184.979416 --> 184.515930).\n",
            "\t Train_Loss: 46.6780 Val_Loss: 184.5159  BEST VAL Loss: 184.5159\n",
            "\n",
            "Epoch 712: Validation loss decreased (184.515930 --> 184.054779).\n",
            "\t Train_Loss: 46.5489 Val_Loss: 184.0548  BEST VAL Loss: 184.0548\n",
            "\n",
            "Epoch 713: Validation loss decreased (184.054779 --> 183.596375).\n",
            "\t Train_Loss: 46.4207 Val_Loss: 183.5964  BEST VAL Loss: 183.5964\n",
            "\n",
            "Epoch 714: Validation loss decreased (183.596375 --> 183.140793).\n",
            "\t Train_Loss: 46.2932 Val_Loss: 183.1408  BEST VAL Loss: 183.1408\n",
            "\n",
            "Epoch 715: Validation loss decreased (183.140793 --> 182.687790).\n",
            "\t Train_Loss: 46.1666 Val_Loss: 182.6878  BEST VAL Loss: 182.6878\n",
            "\n",
            "Epoch 716: Validation loss decreased (182.687790 --> 182.237595).\n",
            "\t Train_Loss: 46.0408 Val_Loss: 182.2376  BEST VAL Loss: 182.2376\n",
            "\n",
            "Epoch 717: Validation loss decreased (182.237595 --> 181.789948).\n",
            "\t Train_Loss: 45.9158 Val_Loss: 181.7899  BEST VAL Loss: 181.7899\n",
            "\n",
            "Epoch 718: Validation loss decreased (181.789948 --> 181.344803).\n",
            "\t Train_Loss: 45.7916 Val_Loss: 181.3448  BEST VAL Loss: 181.3448\n",
            "\n",
            "Epoch 719: Validation loss decreased (181.344803 --> 180.902100).\n",
            "\t Train_Loss: 45.6682 Val_Loss: 180.9021  BEST VAL Loss: 180.9021\n",
            "\n",
            "Epoch 720: Validation loss decreased (180.902100 --> 180.461594).\n",
            "\t Train_Loss: 45.5456 Val_Loss: 180.4616  BEST VAL Loss: 180.4616\n",
            "\n",
            "Epoch 721: Validation loss decreased (180.461594 --> 180.023346).\n",
            "\t Train_Loss: 45.4237 Val_Loss: 180.0233  BEST VAL Loss: 180.0233\n",
            "\n",
            "Epoch 722: Validation loss decreased (180.023346 --> 179.587677).\n",
            "\t Train_Loss: 45.3026 Val_Loss: 179.5877  BEST VAL Loss: 179.5877\n",
            "\n",
            "Epoch 723: Validation loss decreased (179.587677 --> 179.154373).\n",
            "\t Train_Loss: 45.1823 Val_Loss: 179.1544  BEST VAL Loss: 179.1544\n",
            "\n",
            "Epoch 724: Validation loss decreased (179.154373 --> 178.723557).\n",
            "\t Train_Loss: 45.0627 Val_Loss: 178.7236  BEST VAL Loss: 178.7236\n",
            "\n",
            "Epoch 725: Validation loss decreased (178.723557 --> 178.295212).\n",
            "\t Train_Loss: 44.9438 Val_Loss: 178.2952  BEST VAL Loss: 178.2952\n",
            "\n",
            "Epoch 726: Validation loss decreased (178.295212 --> 177.869339).\n",
            "\t Train_Loss: 44.8257 Val_Loss: 177.8693  BEST VAL Loss: 177.8693\n",
            "\n",
            "Epoch 727: Validation loss decreased (177.869339 --> 177.445587).\n",
            "\t Train_Loss: 44.7082 Val_Loss: 177.4456  BEST VAL Loss: 177.4456\n",
            "\n",
            "Epoch 728: Validation loss decreased (177.445587 --> 177.024002).\n",
            "\t Train_Loss: 44.5915 Val_Loss: 177.0240  BEST VAL Loss: 177.0240\n",
            "\n",
            "Epoch 729: Validation loss decreased (177.024002 --> 176.604721).\n",
            "\t Train_Loss: 44.4754 Val_Loss: 176.6047  BEST VAL Loss: 176.6047\n",
            "\n",
            "Epoch 730: Validation loss decreased (176.604721 --> 176.188232).\n",
            "\t Train_Loss: 44.3599 Val_Loss: 176.1882  BEST VAL Loss: 176.1882\n",
            "\n",
            "Epoch 731: Validation loss decreased (176.188232 --> 175.778290).\n",
            "\t Train_Loss: 44.2448 Val_Loss: 175.7783  BEST VAL Loss: 175.7783\n",
            "\n",
            "Epoch 732: Validation loss decreased (175.778290 --> 175.432465).\n",
            "\t Train_Loss: 44.1294 Val_Loss: 175.4325  BEST VAL Loss: 175.4325\n",
            "\n",
            "Epoch 733: Validation loss decreased (175.432465 --> 174.899277).\n",
            "\t Train_Loss: 44.0150 Val_Loss: 174.8993  BEST VAL Loss: 174.8993\n",
            "\n",
            "Epoch 734: Validation loss decreased (174.899277 --> 174.455200).\n",
            "\t Train_Loss: 43.8996 Val_Loss: 174.4552  BEST VAL Loss: 174.4552\n",
            "\n",
            "Epoch 735: Validation loss decreased (174.455200 --> 174.165771).\n",
            "\t Train_Loss: 43.7851 Val_Loss: 174.1658  BEST VAL Loss: 174.1658\n",
            "\n",
            "Epoch 736: Validation loss decreased (174.165771 --> 173.549789).\n",
            "\t Train_Loss: 43.6718 Val_Loss: 173.5498  BEST VAL Loss: 173.5498\n",
            "\n",
            "Epoch 737: Validation loss decreased (173.549789 --> 173.169724).\n",
            "\t Train_Loss: 43.5597 Val_Loss: 173.1697  BEST VAL Loss: 173.1697\n",
            "\n",
            "Epoch 738: Validation loss decreased (173.169724 --> 172.960785).\n",
            "\t Train_Loss: 43.4460 Val_Loss: 172.9608  BEST VAL Loss: 172.9608\n",
            "\n",
            "Epoch 739: Validation loss decreased (172.960785 --> 172.519287).\n",
            "\t Train_Loss: 43.3296 Val_Loss: 172.5193  BEST VAL Loss: 172.5193\n",
            "\n",
            "Epoch 740: Validation loss decreased (172.519287 --> 171.884399).\n",
            "\t Train_Loss: 43.2156 Val_Loss: 171.8844  BEST VAL Loss: 171.8844\n",
            "\n",
            "Epoch 741: Validation loss decreased (171.884399 --> 171.517975).\n",
            "\t Train_Loss: 43.1060 Val_Loss: 171.5180  BEST VAL Loss: 171.5180\n",
            "\n",
            "Epoch 742: Validation loss decreased (171.517975 --> 171.353516).\n",
            "\t Train_Loss: 42.9884 Val_Loss: 171.3535  BEST VAL Loss: 171.3535\n",
            "\n",
            "Epoch 743: Validation loss decreased (171.353516 --> 170.375870).\n",
            "\t Train_Loss: 42.8953 Val_Loss: 170.3759  BEST VAL Loss: 170.3759\n",
            "\n",
            "Epoch 744: Validation loss decreased (170.375870 --> 170.002121).\n",
            "\t Train_Loss: 42.7907 Val_Loss: 170.0021  BEST VAL Loss: 170.0021\n",
            "\n",
            "Epoch 745: Validation loss decreased (170.002121 --> 169.941574).\n",
            "\t Train_Loss: 42.6946 Val_Loss: 169.9416  BEST VAL Loss: 169.9416\n",
            "\n",
            "Epoch 746: Validation loss decreased (169.941574 --> 169.932495).\n",
            "\t Train_Loss: 42.5719 Val_Loss: 169.9325  BEST VAL Loss: 169.9325\n",
            "\n",
            "Epoch 747: Validation loss decreased (169.932495 --> 169.715790).\n",
            "\t Train_Loss: 42.4636 Val_Loss: 169.7158  BEST VAL Loss: 169.7158\n",
            "\n",
            "Epoch 748: Validation loss decreased (169.715790 --> 168.974152).\n",
            "\t Train_Loss: 42.3694 Val_Loss: 168.9742  BEST VAL Loss: 168.9742\n",
            "\n",
            "Epoch 749: Validation loss decreased (168.974152 --> 168.112350).\n",
            "\t Train_Loss: 42.2368 Val_Loss: 168.1124  BEST VAL Loss: 168.1124\n",
            "\n",
            "Epoch 750: Validation loss decreased (168.112350 --> 167.574310).\n",
            "\t Train_Loss: 42.1333 Val_Loss: 167.5743  BEST VAL Loss: 167.5743\n",
            "\n",
            "Epoch 751: Validation loss did not decrease\n",
            "\t Train_Loss: 42.0263 Val_Loss: 167.5930  BEST VAL Loss: 167.5743\n",
            "\n",
            "Epoch 752: Validation loss decreased (167.574310 --> 166.717102).\n",
            "\t Train_Loss: 41.9469 Val_Loss: 166.7171  BEST VAL Loss: 166.7171\n",
            "\n",
            "Epoch 753: Validation loss decreased (166.717102 --> 166.455566).\n",
            "\t Train_Loss: 41.8268 Val_Loss: 166.4556  BEST VAL Loss: 166.4556\n",
            "\n",
            "Epoch 754: Validation loss did not decrease\n",
            "\t Train_Loss: 41.7211 Val_Loss: 166.5288  BEST VAL Loss: 166.4556\n",
            "\n",
            "Epoch 755: Validation loss did not decrease\n",
            "\t Train_Loss: 41.6022 Val_Loss: 166.5127  BEST VAL Loss: 166.4556\n",
            "\n",
            "Epoch 756: Validation loss decreased (166.455566 --> 165.913345).\n",
            "\t Train_Loss: 41.5202 Val_Loss: 165.9133  BEST VAL Loss: 165.9133\n",
            "\n",
            "Epoch 757: Validation loss decreased (165.913345 --> 165.184845).\n",
            "\t Train_Loss: 41.4014 Val_Loss: 165.1848  BEST VAL Loss: 165.1848\n",
            "\n",
            "Epoch 758: Validation loss decreased (165.184845 --> 164.665558).\n",
            "\t Train_Loss: 41.3033 Val_Loss: 164.6656  BEST VAL Loss: 164.6656\n",
            "\n",
            "Epoch 759: Validation loss decreased (164.665558 --> 164.470825).\n",
            "\t Train_Loss: 41.2027 Val_Loss: 164.4708  BEST VAL Loss: 164.4708\n",
            "\n",
            "Epoch 760: Validation loss decreased (164.470825 --> 164.374619).\n",
            "\t Train_Loss: 41.0852 Val_Loss: 164.3746  BEST VAL Loss: 164.3746\n",
            "\n",
            "Epoch 761: Validation loss decreased (164.374619 --> 163.689453).\n",
            "\t Train_Loss: 41.0004 Val_Loss: 163.6895  BEST VAL Loss: 163.6895\n",
            "\n",
            "Epoch 762: Validation loss decreased (163.689453 --> 163.157394).\n",
            "\t Train_Loss: 40.8783 Val_Loss: 163.1574  BEST VAL Loss: 163.1574\n",
            "\n",
            "Epoch 763: Validation loss decreased (163.157394 --> 163.014145).\n",
            "\t Train_Loss: 40.7872 Val_Loss: 163.0141  BEST VAL Loss: 163.0141\n",
            "\n",
            "Epoch 764: Validation loss did not decrease\n",
            "\t Train_Loss: 40.6670 Val_Loss: 163.0593  BEST VAL Loss: 163.0141\n",
            "\n",
            "Epoch 765: Validation loss decreased (163.014145 --> 162.615601).\n",
            "\t Train_Loss: 40.5660 Val_Loss: 162.6156  BEST VAL Loss: 162.6156\n",
            "\n",
            "Epoch 766: Validation loss decreased (162.615601 --> 161.985992).\n",
            "\t Train_Loss: 40.4552 Val_Loss: 161.9860  BEST VAL Loss: 161.9860\n",
            "\n",
            "Epoch 767: Validation loss decreased (161.985992 --> 161.567368).\n",
            "\t Train_Loss: 40.3539 Val_Loss: 161.5674  BEST VAL Loss: 161.5674\n",
            "\n",
            "Epoch 768: Validation loss decreased (161.567368 --> 161.388947).\n",
            "\t Train_Loss: 40.2544 Val_Loss: 161.3889  BEST VAL Loss: 161.3889\n",
            "\n",
            "Epoch 769: Validation loss decreased (161.388947 --> 161.376129).\n",
            "\t Train_Loss: 40.1327 Val_Loss: 161.3761  BEST VAL Loss: 161.3761\n",
            "\n",
            "Epoch 770: Validation loss decreased (161.376129 --> 161.071686).\n",
            "\t Train_Loss: 40.0368 Val_Loss: 161.0717  BEST VAL Loss: 161.0717\n",
            "\n",
            "Epoch 771: Validation loss decreased (161.071686 --> 160.352692).\n",
            "\t Train_Loss: 39.9348 Val_Loss: 160.3527  BEST VAL Loss: 160.3527\n",
            "\n",
            "Epoch 772: Validation loss decreased (160.352692 --> 159.873245).\n",
            "\t Train_Loss: 39.8277 Val_Loss: 159.8732  BEST VAL Loss: 159.8732\n",
            "\n",
            "Epoch 773: Validation loss decreased (159.873245 --> 159.737625).\n",
            "\t Train_Loss: 39.7254 Val_Loss: 159.7376  BEST VAL Loss: 159.7376\n",
            "\n",
            "Epoch 774: Validation loss decreased (159.737625 --> 159.536240).\n",
            "\t Train_Loss: 39.6077 Val_Loss: 159.5362  BEST VAL Loss: 159.5362\n",
            "\n",
            "Epoch 775: Validation loss decreased (159.536240 --> 158.911819).\n",
            "\t Train_Loss: 39.5150 Val_Loss: 158.9118  BEST VAL Loss: 158.9118\n",
            "\n",
            "Epoch 776: Validation loss decreased (158.911819 --> 158.493469).\n",
            "\t Train_Loss: 39.4052 Val_Loss: 158.4935  BEST VAL Loss: 158.4935\n",
            "\n",
            "Epoch 777: Validation loss decreased (158.493469 --> 158.362320).\n",
            "\t Train_Loss: 39.3077 Val_Loss: 158.3623  BEST VAL Loss: 158.3623\n",
            "\n",
            "Epoch 778: Validation loss decreased (158.362320 --> 158.306320).\n",
            "\t Train_Loss: 39.1980 Val_Loss: 158.3063  BEST VAL Loss: 158.3063\n",
            "\n",
            "Epoch 779: Validation loss decreased (158.306320 --> 157.964310).\n",
            "\t Train_Loss: 39.0991 Val_Loss: 157.9643  BEST VAL Loss: 157.9643\n",
            "\n",
            "Epoch 780: Validation loss decreased (157.964310 --> 157.442902).\n",
            "\t Train_Loss: 38.9921 Val_Loss: 157.4429  BEST VAL Loss: 157.4429\n",
            "\n",
            "Epoch 781: Validation loss decreased (157.442902 --> 157.056671).\n",
            "\t Train_Loss: 38.8917 Val_Loss: 157.0567  BEST VAL Loss: 157.0567\n",
            "\n",
            "Epoch 782: Validation loss decreased (157.056671 --> 156.871857).\n",
            "\t Train_Loss: 38.7899 Val_Loss: 156.8719  BEST VAL Loss: 156.8719\n",
            "\n",
            "Epoch 783: Validation loss decreased (156.871857 --> 156.668625).\n",
            "\t Train_Loss: 38.6860 Val_Loss: 156.6686  BEST VAL Loss: 156.6686\n",
            "\n",
            "Epoch 784: Validation loss decreased (156.668625 --> 156.223801).\n",
            "\t Train_Loss: 38.5878 Val_Loss: 156.2238  BEST VAL Loss: 156.2238\n",
            "\n",
            "Epoch 785: Validation loss decreased (156.223801 --> 155.842804).\n",
            "\t Train_Loss: 38.4813 Val_Loss: 155.8428  BEST VAL Loss: 155.8428\n",
            "\n",
            "Epoch 786: Validation loss decreased (155.842804 --> 155.653290).\n",
            "\t Train_Loss: 38.3838 Val_Loss: 155.6533  BEST VAL Loss: 155.6533\n",
            "\n",
            "Epoch 787: Validation loss decreased (155.653290 --> 155.477325).\n",
            "\t Train_Loss: 38.2784 Val_Loss: 155.4773  BEST VAL Loss: 155.4773\n",
            "\n",
            "Epoch 788: Validation loss decreased (155.477325 --> 155.020187).\n",
            "\t Train_Loss: 38.1831 Val_Loss: 155.0202  BEST VAL Loss: 155.0202\n",
            "\n",
            "Epoch 789: Validation loss decreased (155.020187 --> 154.550674).\n",
            "\t Train_Loss: 38.0777 Val_Loss: 154.5507  BEST VAL Loss: 154.5507\n",
            "\n",
            "Epoch 790: Validation loss decreased (154.550674 --> 154.287155).\n",
            "\t Train_Loss: 37.9820 Val_Loss: 154.2872  BEST VAL Loss: 154.2872\n",
            "\n",
            "Epoch 791: Validation loss decreased (154.287155 --> 154.124985).\n",
            "\t Train_Loss: 37.8786 Val_Loss: 154.1250  BEST VAL Loss: 154.1250\n",
            "\n",
            "Epoch 792: Validation loss decreased (154.124985 --> 153.818741).\n",
            "\t Train_Loss: 37.7817 Val_Loss: 153.8187  BEST VAL Loss: 153.8187\n",
            "\n",
            "Epoch 793: Validation loss decreased (153.818741 --> 153.476456).\n",
            "\t Train_Loss: 37.6802 Val_Loss: 153.4765  BEST VAL Loss: 153.4765\n",
            "\n",
            "Epoch 794: Validation loss decreased (153.476456 --> 153.250076).\n",
            "\t Train_Loss: 37.5828 Val_Loss: 153.2501  BEST VAL Loss: 153.2501\n",
            "\n",
            "Epoch 795: Validation loss decreased (153.250076 --> 153.067978).\n",
            "\t Train_Loss: 37.4833 Val_Loss: 153.0680  BEST VAL Loss: 153.0680\n",
            "\n",
            "Epoch 796: Validation loss decreased (153.067978 --> 152.739944).\n",
            "\t Train_Loss: 37.3861 Val_Loss: 152.7399  BEST VAL Loss: 152.7399\n",
            "\n",
            "Epoch 797: Validation loss decreased (152.739944 --> 152.322372).\n",
            "\t Train_Loss: 37.2869 Val_Loss: 152.3224  BEST VAL Loss: 152.3224\n",
            "\n",
            "Epoch 798: Validation loss decreased (152.322372 --> 152.002762).\n",
            "\t Train_Loss: 37.1897 Val_Loss: 152.0028  BEST VAL Loss: 152.0028\n",
            "\n",
            "Epoch 799: Validation loss decreased (152.002762 --> 151.779266).\n",
            "\t Train_Loss: 37.0924 Val_Loss: 151.7793  BEST VAL Loss: 151.7793\n",
            "\n",
            "Epoch 800: Validation loss decreased (151.779266 --> 151.501877).\n",
            "\t Train_Loss: 36.9954 Val_Loss: 151.5019  BEST VAL Loss: 151.5019\n",
            "\n",
            "Epoch 801: Validation loss decreased (151.501877 --> 151.171921).\n",
            "\t Train_Loss: 36.8987 Val_Loss: 151.1719  BEST VAL Loss: 151.1719\n",
            "\n",
            "Epoch 802: Validation loss decreased (151.171921 --> 150.909119).\n",
            "\t Train_Loss: 36.8023 Val_Loss: 150.9091  BEST VAL Loss: 150.9091\n",
            "\n",
            "Epoch 803: Validation loss decreased (150.909119 --> 150.695282).\n",
            "\t Train_Loss: 36.7063 Val_Loss: 150.6953  BEST VAL Loss: 150.6953\n",
            "\n",
            "Epoch 804: Validation loss decreased (150.695282 --> 150.407089).\n",
            "\t Train_Loss: 36.6104 Val_Loss: 150.4071  BEST VAL Loss: 150.4071\n",
            "\n",
            "Epoch 805: Validation loss decreased (150.407089 --> 150.056152).\n",
            "\t Train_Loss: 36.5150 Val_Loss: 150.0562  BEST VAL Loss: 150.0562\n",
            "\n",
            "Epoch 806: Validation loss decreased (150.056152 --> 149.758469).\n",
            "\t Train_Loss: 36.4200 Val_Loss: 149.7585  BEST VAL Loss: 149.7585\n",
            "\n",
            "Epoch 807: Validation loss decreased (149.758469 --> 149.515778).\n",
            "\t Train_Loss: 36.3251 Val_Loss: 149.5158  BEST VAL Loss: 149.5158\n",
            "\n",
            "Epoch 808: Validation loss decreased (149.515778 --> 149.241638).\n",
            "\t Train_Loss: 36.2304 Val_Loss: 149.2416  BEST VAL Loss: 149.2416\n",
            "\n",
            "Epoch 809: Validation loss decreased (149.241638 --> 148.941727).\n",
            "\t Train_Loss: 36.1359 Val_Loss: 148.9417  BEST VAL Loss: 148.9417\n",
            "\n",
            "Epoch 810: Validation loss decreased (148.941727 --> 148.683121).\n",
            "\t Train_Loss: 36.0420 Val_Loss: 148.6831  BEST VAL Loss: 148.6831\n",
            "\n",
            "Epoch 811: Validation loss decreased (148.683121 --> 148.444519).\n",
            "\t Train_Loss: 35.9480 Val_Loss: 148.4445  BEST VAL Loss: 148.4445\n",
            "\n",
            "Epoch 812: Validation loss decreased (148.444519 --> 148.155930).\n",
            "\t Train_Loss: 35.8547 Val_Loss: 148.1559  BEST VAL Loss: 148.1559\n",
            "\n",
            "Epoch 813: Validation loss decreased (148.155930 --> 147.848389).\n",
            "\t Train_Loss: 35.7611 Val_Loss: 147.8484  BEST VAL Loss: 147.8484\n",
            "\n",
            "Epoch 814: Validation loss decreased (147.848389 --> 147.575363).\n",
            "\t Train_Loss: 35.6684 Val_Loss: 147.5754  BEST VAL Loss: 147.5754\n",
            "\n",
            "Epoch 815: Validation loss decreased (147.575363 --> 147.322296).\n",
            "\t Train_Loss: 35.5753 Val_Loss: 147.3223  BEST VAL Loss: 147.3223\n",
            "\n",
            "Epoch 816: Validation loss decreased (147.322296 --> 147.037582).\n",
            "\t Train_Loss: 35.4831 Val_Loss: 147.0376  BEST VAL Loss: 147.0376\n",
            "\n",
            "Epoch 817: Validation loss decreased (147.037582 --> 146.759796).\n",
            "\t Train_Loss: 35.3910 Val_Loss: 146.7598  BEST VAL Loss: 146.7598\n",
            "\n",
            "Epoch 818: Validation loss decreased (146.759796 --> 146.505539).\n",
            "\t Train_Loss: 35.2997 Val_Loss: 146.5055  BEST VAL Loss: 146.5055\n",
            "\n",
            "Epoch 819: Validation loss decreased (146.505539 --> 146.253448).\n",
            "\t Train_Loss: 35.2083 Val_Loss: 146.2534  BEST VAL Loss: 146.2534\n",
            "\n",
            "Epoch 820: Validation loss decreased (146.253448 --> 145.959351).\n",
            "\t Train_Loss: 35.1162 Val_Loss: 145.9594  BEST VAL Loss: 145.9594\n",
            "\n",
            "Epoch 821: Validation loss decreased (145.959351 --> 145.677536).\n",
            "\t Train_Loss: 35.0233 Val_Loss: 145.6775  BEST VAL Loss: 145.6775\n",
            "\n",
            "Epoch 822: Validation loss decreased (145.677536 --> 145.414505).\n",
            "\t Train_Loss: 34.9297 Val_Loss: 145.4145  BEST VAL Loss: 145.4145\n",
            "\n",
            "Epoch 823: Validation loss decreased (145.414505 --> 145.142929).\n",
            "\t Train_Loss: 34.8372 Val_Loss: 145.1429  BEST VAL Loss: 145.1429\n",
            "\n",
            "Epoch 824: Validation loss decreased (145.142929 --> 144.860886).\n",
            "\t Train_Loss: 34.7463 Val_Loss: 144.8609  BEST VAL Loss: 144.8609\n",
            "\n",
            "Epoch 825: Validation loss decreased (144.860886 --> 144.590561).\n",
            "\t Train_Loss: 34.6559 Val_Loss: 144.5906  BEST VAL Loss: 144.5906\n",
            "\n",
            "Epoch 826: Validation loss decreased (144.590561 --> 144.344818).\n",
            "\t Train_Loss: 34.5650 Val_Loss: 144.3448  BEST VAL Loss: 144.3448\n",
            "\n",
            "Epoch 827: Validation loss decreased (144.344818 --> 144.078964).\n",
            "\t Train_Loss: 34.4723 Val_Loss: 144.0790  BEST VAL Loss: 144.0790\n",
            "\n",
            "Epoch 828: Validation loss decreased (144.078964 --> 143.802734).\n",
            "\t Train_Loss: 34.3794 Val_Loss: 143.8027  BEST VAL Loss: 143.8027\n",
            "\n",
            "Epoch 829: Validation loss decreased (143.802734 --> 143.538452).\n",
            "\t Train_Loss: 34.2875 Val_Loss: 143.5385  BEST VAL Loss: 143.5385\n",
            "\n",
            "Epoch 830: Validation loss decreased (143.538452 --> 143.274506).\n",
            "\t Train_Loss: 34.1964 Val_Loss: 143.2745  BEST VAL Loss: 143.2745\n",
            "\n",
            "Epoch 831: Validation loss decreased (143.274506 --> 143.000443).\n",
            "\t Train_Loss: 34.1057 Val_Loss: 143.0004  BEST VAL Loss: 143.0004\n",
            "\n",
            "Epoch 832: Validation loss decreased (143.000443 --> 142.724167).\n",
            "\t Train_Loss: 34.0142 Val_Loss: 142.7242  BEST VAL Loss: 142.7242\n",
            "\n",
            "Epoch 833: Validation loss decreased (142.724167 --> 142.464722).\n",
            "\t Train_Loss: 33.9221 Val_Loss: 142.4647  BEST VAL Loss: 142.4647\n",
            "\n",
            "Epoch 834: Validation loss decreased (142.464722 --> 142.201752).\n",
            "\t Train_Loss: 33.8291 Val_Loss: 142.2018  BEST VAL Loss: 142.2018\n",
            "\n",
            "Epoch 835: Validation loss decreased (142.201752 --> 141.929977).\n",
            "\t Train_Loss: 33.7364 Val_Loss: 141.9300  BEST VAL Loss: 141.9300\n",
            "\n",
            "Epoch 836: Validation loss decreased (141.929977 --> 141.666367).\n",
            "\t Train_Loss: 33.6441 Val_Loss: 141.6664  BEST VAL Loss: 141.6664\n",
            "\n",
            "Epoch 837: Validation loss decreased (141.666367 --> 141.409409).\n",
            "\t Train_Loss: 33.5523 Val_Loss: 141.4094  BEST VAL Loss: 141.4094\n",
            "\n",
            "Epoch 838: Validation loss decreased (141.409409 --> 141.140808).\n",
            "\t Train_Loss: 33.4606 Val_Loss: 141.1408  BEST VAL Loss: 141.1408\n",
            "\n",
            "Epoch 839: Validation loss decreased (141.140808 --> 140.868011).\n",
            "\t Train_Loss: 33.3689 Val_Loss: 140.8680  BEST VAL Loss: 140.8680\n",
            "\n",
            "Epoch 840: Validation loss decreased (140.868011 --> 140.603882).\n",
            "\t Train_Loss: 33.2774 Val_Loss: 140.6039  BEST VAL Loss: 140.6039\n",
            "\n",
            "Epoch 841: Validation loss decreased (140.603882 --> 140.347580).\n",
            "\t Train_Loss: 33.1846 Val_Loss: 140.3476  BEST VAL Loss: 140.3476\n",
            "\n",
            "Epoch 842: Validation loss decreased (140.347580 --> 140.075394).\n",
            "\t Train_Loss: 33.0915 Val_Loss: 140.0754  BEST VAL Loss: 140.0754\n",
            "\n",
            "Epoch 843: Validation loss decreased (140.075394 --> 139.814575).\n",
            "\t Train_Loss: 32.9970 Val_Loss: 139.8146  BEST VAL Loss: 139.8146\n",
            "\n",
            "Epoch 844: Validation loss decreased (139.814575 --> 139.559464).\n",
            "\t Train_Loss: 32.9027 Val_Loss: 139.5595  BEST VAL Loss: 139.5595\n",
            "\n",
            "Epoch 845: Validation loss decreased (139.559464 --> 139.298706).\n",
            "\t Train_Loss: 32.8089 Val_Loss: 139.2987  BEST VAL Loss: 139.2987\n",
            "\n",
            "Epoch 846: Validation loss decreased (139.298706 --> 139.031601).\n",
            "\t Train_Loss: 32.7156 Val_Loss: 139.0316  BEST VAL Loss: 139.0316\n",
            "\n",
            "Epoch 847: Validation loss decreased (139.031601 --> 138.762482).\n",
            "\t Train_Loss: 32.6229 Val_Loss: 138.7625  BEST VAL Loss: 138.7625\n",
            "\n",
            "Epoch 848: Validation loss decreased (138.762482 --> 138.507324).\n",
            "\t Train_Loss: 32.5303 Val_Loss: 138.5073  BEST VAL Loss: 138.5073\n",
            "\n",
            "Epoch 849: Validation loss decreased (138.507324 --> 138.229294).\n",
            "\t Train_Loss: 32.4384 Val_Loss: 138.2293  BEST VAL Loss: 138.2293\n",
            "\n",
            "Epoch 850: Validation loss decreased (138.229294 --> 137.984253).\n",
            "\t Train_Loss: 32.3457 Val_Loss: 137.9843  BEST VAL Loss: 137.9843\n",
            "\n",
            "Epoch 851: Validation loss decreased (137.984253 --> 137.706100).\n",
            "\t Train_Loss: 32.2532 Val_Loss: 137.7061  BEST VAL Loss: 137.7061\n",
            "\n",
            "Epoch 852: Validation loss decreased (137.706100 --> 137.467484).\n",
            "\t Train_Loss: 32.1573 Val_Loss: 137.4675  BEST VAL Loss: 137.4675\n",
            "\n",
            "Epoch 853: Validation loss decreased (137.467484 --> 137.192673).\n",
            "\t Train_Loss: 32.0615 Val_Loss: 137.1927  BEST VAL Loss: 137.1927\n",
            "\n",
            "Epoch 854: Validation loss decreased (137.192673 --> 136.941711).\n",
            "\t Train_Loss: 31.9660 Val_Loss: 136.9417  BEST VAL Loss: 136.9417\n",
            "\n",
            "Epoch 855: Validation loss decreased (136.941711 --> 136.688431).\n",
            "\t Train_Loss: 31.8721 Val_Loss: 136.6884  BEST VAL Loss: 136.6884\n",
            "\n",
            "Epoch 856: Validation loss decreased (136.688431 --> 136.399765).\n",
            "\t Train_Loss: 31.7794 Val_Loss: 136.3998  BEST VAL Loss: 136.3998\n",
            "\n",
            "Epoch 857: Validation loss decreased (136.399765 --> 136.182281).\n",
            "\t Train_Loss: 31.6874 Val_Loss: 136.1823  BEST VAL Loss: 136.1823\n",
            "\n",
            "Epoch 858: Validation loss decreased (136.182281 --> 135.864426).\n",
            "\t Train_Loss: 31.5968 Val_Loss: 135.8644  BEST VAL Loss: 135.8644\n",
            "\n",
            "Epoch 859: Validation loss decreased (135.864426 --> 135.682678).\n",
            "\t Train_Loss: 31.5052 Val_Loss: 135.6827  BEST VAL Loss: 135.6827\n",
            "\n",
            "Epoch 860: Validation loss decreased (135.682678 --> 135.325119).\n",
            "\t Train_Loss: 31.4160 Val_Loss: 135.3251  BEST VAL Loss: 135.3251\n",
            "\n",
            "Epoch 861: Validation loss decreased (135.325119 --> 135.183624).\n",
            "\t Train_Loss: 31.3225 Val_Loss: 135.1836  BEST VAL Loss: 135.1836\n",
            "\n",
            "Epoch 862: Validation loss decreased (135.183624 --> 134.815659).\n",
            "\t Train_Loss: 31.2308 Val_Loss: 134.8157  BEST VAL Loss: 134.8157\n",
            "\n",
            "Epoch 863: Validation loss decreased (134.815659 --> 134.643860).\n",
            "\t Train_Loss: 31.1345 Val_Loss: 134.6439  BEST VAL Loss: 134.6439\n",
            "\n",
            "Epoch 864: Validation loss decreased (134.643860 --> 134.324890).\n",
            "\t Train_Loss: 31.0395 Val_Loss: 134.3249  BEST VAL Loss: 134.3249\n",
            "\n",
            "Epoch 865: Validation loss decreased (134.324890 --> 134.102219).\n",
            "\t Train_Loss: 30.9442 Val_Loss: 134.1022  BEST VAL Loss: 134.1022\n",
            "\n",
            "Epoch 866: Validation loss decreased (134.102219 --> 133.823776).\n",
            "\t Train_Loss: 30.8509 Val_Loss: 133.8238  BEST VAL Loss: 133.8238\n",
            "\n",
            "Epoch 867: Validation loss decreased (133.823776 --> 133.585251).\n",
            "\t Train_Loss: 30.7588 Val_Loss: 133.5853  BEST VAL Loss: 133.5853\n",
            "\n",
            "Epoch 868: Validation loss decreased (133.585251 --> 133.324203).\n",
            "\t Train_Loss: 30.6676 Val_Loss: 133.3242  BEST VAL Loss: 133.3242\n",
            "\n",
            "Epoch 869: Validation loss decreased (133.324203 --> 133.057144).\n",
            "\t Train_Loss: 30.5769 Val_Loss: 133.0571  BEST VAL Loss: 133.0571\n",
            "\n",
            "Epoch 870: Validation loss decreased (133.057144 --> 132.836411).\n",
            "\t Train_Loss: 30.4869 Val_Loss: 132.8364  BEST VAL Loss: 132.8364\n",
            "\n",
            "Epoch 871: Validation loss decreased (132.836411 --> 132.521896).\n",
            "\t Train_Loss: 30.3980 Val_Loss: 132.5219  BEST VAL Loss: 132.5219\n",
            "\n",
            "Epoch 872: Validation loss decreased (132.521896 --> 132.393311).\n",
            "\t Train_Loss: 30.3128 Val_Loss: 132.3933  BEST VAL Loss: 132.3933\n",
            "\n",
            "Epoch 873: Validation loss decreased (132.393311 --> 131.941925).\n",
            "\t Train_Loss: 30.2433 Val_Loss: 131.9419  BEST VAL Loss: 131.9419\n",
            "\n",
            "Epoch 874: Validation loss did not decrease\n",
            "\t Train_Loss: 30.1984 Val_Loss: 132.0239  BEST VAL Loss: 131.9419\n",
            "\n",
            "Epoch 875: Validation loss decreased (131.941925 --> 131.501633).\n",
            "\t Train_Loss: 30.1922 Val_Loss: 131.5016  BEST VAL Loss: 131.5016\n",
            "\n",
            "Epoch 876: Validation loss decreased (131.501633 --> 131.174194).\n",
            "\t Train_Loss: 29.9609 Val_Loss: 131.1742  BEST VAL Loss: 131.1742\n",
            "\n",
            "Epoch 877: Validation loss did not decrease\n",
            "\t Train_Loss: 30.0259 Val_Loss: 131.2555  BEST VAL Loss: 131.1742\n",
            "\n",
            "Epoch 878: Validation loss decreased (131.174194 --> 130.878204).\n",
            "\t Train_Loss: 30.0125 Val_Loss: 130.8782  BEST VAL Loss: 130.8782\n",
            "\n",
            "Epoch 879: Validation loss decreased (130.878204 --> 130.401581).\n",
            "\t Train_Loss: 29.8498 Val_Loss: 130.4016  BEST VAL Loss: 130.4016\n",
            "\n",
            "Epoch 880: Validation loss decreased (130.401581 --> 130.315842).\n",
            "\t Train_Loss: 29.7660 Val_Loss: 130.3158  BEST VAL Loss: 130.3158\n",
            "\n",
            "Epoch 881: Validation loss did not decrease\n",
            "\t Train_Loss: 29.6018 Val_Loss: 130.3359  BEST VAL Loss: 130.3158\n",
            "\n",
            "Epoch 882: Validation loss decreased (130.315842 --> 129.972580).\n",
            "\t Train_Loss: 29.6445 Val_Loss: 129.9726  BEST VAL Loss: 129.9726\n",
            "\n",
            "Epoch 883: Validation loss decreased (129.972580 --> 129.435532).\n",
            "\t Train_Loss: 29.4561 Val_Loss: 129.4355  BEST VAL Loss: 129.4355\n",
            "\n",
            "Epoch 884: Validation loss decreased (129.435532 --> 129.166595).\n",
            "\t Train_Loss: 29.4234 Val_Loss: 129.1666  BEST VAL Loss: 129.1666\n",
            "\n",
            "Epoch 885: Validation loss decreased (129.166595 --> 129.157837).\n",
            "\t Train_Loss: 29.2466 Val_Loss: 129.1578  BEST VAL Loss: 129.1578\n",
            "\n",
            "Epoch 886: Validation loss decreased (129.157837 --> 128.993881).\n",
            "\t Train_Loss: 29.2428 Val_Loss: 128.9939  BEST VAL Loss: 128.9939\n",
            "\n",
            "Epoch 887: Validation loss decreased (128.993881 --> 128.592850).\n",
            "\t Train_Loss: 29.1185 Val_Loss: 128.5928  BEST VAL Loss: 128.5928\n",
            "\n",
            "Epoch 888: Validation loss decreased (128.592850 --> 128.192490).\n",
            "\t Train_Loss: 29.0240 Val_Loss: 128.1925  BEST VAL Loss: 128.1925\n",
            "\n",
            "Epoch 889: Validation loss decreased (128.192490 --> 128.079834).\n",
            "\t Train_Loss: 28.9456 Val_Loss: 128.0798  BEST VAL Loss: 128.0798\n",
            "\n",
            "Epoch 890: Validation loss decreased (128.079834 --> 128.032028).\n",
            "\t Train_Loss: 28.8543 Val_Loss: 128.0320  BEST VAL Loss: 128.0320\n",
            "\n",
            "Epoch 891: Validation loss decreased (128.032028 --> 127.749649).\n",
            "\t Train_Loss: 28.7846 Val_Loss: 127.7496  BEST VAL Loss: 127.7496\n",
            "\n",
            "Epoch 892: Validation loss decreased (127.749649 --> 127.257469).\n",
            "\t Train_Loss: 28.6861 Val_Loss: 127.2575  BEST VAL Loss: 127.2575\n",
            "\n",
            "Epoch 893: Validation loss decreased (127.257469 --> 127.015762).\n",
            "\t Train_Loss: 28.6107 Val_Loss: 127.0158  BEST VAL Loss: 127.0158\n",
            "\n",
            "Epoch 894: Validation loss decreased (127.015762 --> 126.992027).\n",
            "\t Train_Loss: 28.5294 Val_Loss: 126.9920  BEST VAL Loss: 126.9920\n",
            "\n",
            "Epoch 895: Validation loss decreased (126.992027 --> 126.860374).\n",
            "\t Train_Loss: 28.4429 Val_Loss: 126.8604  BEST VAL Loss: 126.8604\n",
            "\n",
            "Epoch 896: Validation loss decreased (126.860374 --> 126.419533).\n",
            "\t Train_Loss: 28.3795 Val_Loss: 126.4195  BEST VAL Loss: 126.4195\n",
            "\n",
            "Epoch 897: Validation loss decreased (126.419533 --> 126.085327).\n",
            "\t Train_Loss: 28.2724 Val_Loss: 126.0853  BEST VAL Loss: 126.0853\n",
            "\n",
            "Epoch 898: Validation loss decreased (126.085327 --> 126.009666).\n",
            "\t Train_Loss: 28.2215 Val_Loss: 126.0097  BEST VAL Loss: 126.0097\n",
            "\n",
            "Epoch 899: Validation loss decreased (126.009666 --> 125.940178).\n",
            "\t Train_Loss: 28.1224 Val_Loss: 125.9402  BEST VAL Loss: 125.9402\n",
            "\n",
            "Epoch 900: Validation loss decreased (125.940178 --> 125.573906).\n",
            "\t Train_Loss: 28.0620 Val_Loss: 125.5739  BEST VAL Loss: 125.5739\n",
            "\n",
            "Epoch 901: Validation loss decreased (125.573906 --> 125.184059).\n",
            "\t Train_Loss: 27.9675 Val_Loss: 125.1841  BEST VAL Loss: 125.1841\n",
            "\n",
            "Epoch 902: Validation loss decreased (125.184059 --> 125.019859).\n",
            "\t Train_Loss: 27.9079 Val_Loss: 125.0199  BEST VAL Loss: 125.0199\n",
            "\n",
            "Epoch 903: Validation loss decreased (125.019859 --> 124.953003).\n",
            "\t Train_Loss: 27.8206 Val_Loss: 124.9530  BEST VAL Loss: 124.9530\n",
            "\n",
            "Epoch 904: Validation loss decreased (124.953003 --> 124.671654).\n",
            "\t Train_Loss: 27.7527 Val_Loss: 124.6717  BEST VAL Loss: 124.6717\n",
            "\n",
            "Epoch 905: Validation loss decreased (124.671654 --> 124.327271).\n",
            "\t Train_Loss: 27.6711 Val_Loss: 124.3273  BEST VAL Loss: 124.3273\n",
            "\n",
            "Epoch 906: Validation loss decreased (124.327271 --> 124.140648).\n",
            "\t Train_Loss: 27.6044 Val_Loss: 124.1406  BEST VAL Loss: 124.1406\n",
            "\n",
            "Epoch 907: Validation loss decreased (124.140648 --> 124.063072).\n",
            "\t Train_Loss: 27.5231 Val_Loss: 124.0631  BEST VAL Loss: 124.0631\n",
            "\n",
            "Epoch 908: Validation loss decreased (124.063072 --> 123.819679).\n",
            "\t Train_Loss: 27.4560 Val_Loss: 123.8197  BEST VAL Loss: 123.8197\n",
            "\n",
            "Epoch 909: Validation loss decreased (123.819679 --> 123.495056).\n",
            "\t Train_Loss: 27.3768 Val_Loss: 123.4951  BEST VAL Loss: 123.4951\n",
            "\n",
            "Epoch 910: Validation loss decreased (123.495056 --> 123.262871).\n",
            "\t Train_Loss: 27.3099 Val_Loss: 123.2629  BEST VAL Loss: 123.2629\n",
            "\n",
            "Epoch 911: Validation loss decreased (123.262871 --> 123.118858).\n",
            "\t Train_Loss: 27.2324 Val_Loss: 123.1189  BEST VAL Loss: 123.1189\n",
            "\n",
            "Epoch 912: Validation loss decreased (123.118858 --> 122.903404).\n",
            "\t Train_Loss: 27.1637 Val_Loss: 122.9034  BEST VAL Loss: 122.9034\n",
            "\n",
            "Epoch 913: Validation loss decreased (122.903404 --> 122.637939).\n",
            "\t Train_Loss: 27.0910 Val_Loss: 122.6379  BEST VAL Loss: 122.6379\n",
            "\n",
            "Epoch 914: Validation loss decreased (122.637939 --> 122.411522).\n",
            "\t Train_Loss: 27.0198 Val_Loss: 122.4115  BEST VAL Loss: 122.4115\n",
            "\n",
            "Epoch 915: Validation loss decreased (122.411522 --> 122.216942).\n",
            "\t Train_Loss: 26.9506 Val_Loss: 122.2169  BEST VAL Loss: 122.2169\n",
            "\n",
            "Epoch 916: Validation loss decreased (122.216942 --> 122.027382).\n",
            "\t Train_Loss: 26.8766 Val_Loss: 122.0274  BEST VAL Loss: 122.0274\n",
            "\n",
            "Epoch 917: Validation loss decreased (122.027382 --> 121.808838).\n",
            "\t Train_Loss: 26.8105 Val_Loss: 121.8088  BEST VAL Loss: 121.8088\n",
            "\n",
            "Epoch 918: Validation loss decreased (121.808838 --> 121.562988).\n",
            "\t Train_Loss: 26.7373 Val_Loss: 121.5630  BEST VAL Loss: 121.5630\n",
            "\n",
            "Epoch 919: Validation loss decreased (121.562988 --> 121.327797).\n",
            "\t Train_Loss: 26.6718 Val_Loss: 121.3278  BEST VAL Loss: 121.3278\n",
            "\n",
            "Epoch 920: Validation loss decreased (121.327797 --> 121.157387).\n",
            "\t Train_Loss: 26.6004 Val_Loss: 121.1574  BEST VAL Loss: 121.1574\n",
            "\n",
            "Epoch 921: Validation loss decreased (121.157387 --> 120.975830).\n",
            "\t Train_Loss: 26.5327 Val_Loss: 120.9758  BEST VAL Loss: 120.9758\n",
            "\n",
            "Epoch 922: Validation loss decreased (120.975830 --> 120.717957).\n",
            "\t Train_Loss: 26.4649 Val_Loss: 120.7180  BEST VAL Loss: 120.7180\n",
            "\n",
            "Epoch 923: Validation loss decreased (120.717957 --> 120.486671).\n",
            "\t Train_Loss: 26.3958 Val_Loss: 120.4867  BEST VAL Loss: 120.4867\n",
            "\n",
            "Epoch 924: Validation loss decreased (120.486671 --> 120.329178).\n",
            "\t Train_Loss: 26.3306 Val_Loss: 120.3292  BEST VAL Loss: 120.3292\n",
            "\n",
            "Epoch 925: Validation loss decreased (120.329178 --> 120.144798).\n",
            "\t Train_Loss: 26.2614 Val_Loss: 120.1448  BEST VAL Loss: 120.1448\n",
            "\n",
            "Epoch 926: Validation loss decreased (120.144798 --> 119.884438).\n",
            "\t Train_Loss: 26.1965 Val_Loss: 119.8844  BEST VAL Loss: 119.8844\n",
            "\n",
            "Epoch 927: Validation loss decreased (119.884438 --> 119.659134).\n",
            "\t Train_Loss: 26.1290 Val_Loss: 119.6591  BEST VAL Loss: 119.6591\n",
            "\n",
            "Epoch 928: Validation loss decreased (119.659134 --> 119.483498).\n",
            "\t Train_Loss: 26.0634 Val_Loss: 119.4835  BEST VAL Loss: 119.4835\n",
            "\n",
            "Epoch 929: Validation loss decreased (119.483498 --> 119.275993).\n",
            "\t Train_Loss: 25.9981 Val_Loss: 119.2760  BEST VAL Loss: 119.2760\n",
            "\n",
            "Epoch 930: Validation loss decreased (119.275993 --> 119.047745).\n",
            "\t Train_Loss: 25.9322 Val_Loss: 119.0477  BEST VAL Loss: 119.0477\n",
            "\n",
            "Epoch 931: Validation loss decreased (119.047745 --> 118.843788).\n",
            "\t Train_Loss: 25.8676 Val_Loss: 118.8438  BEST VAL Loss: 118.8438\n",
            "\n",
            "Epoch 932: Validation loss decreased (118.843788 --> 118.649864).\n",
            "\t Train_Loss: 25.8028 Val_Loss: 118.6499  BEST VAL Loss: 118.6499\n",
            "\n",
            "Epoch 933: Validation loss decreased (118.649864 --> 118.450562).\n",
            "\t Train_Loss: 25.7382 Val_Loss: 118.4506  BEST VAL Loss: 118.4506\n",
            "\n",
            "Epoch 934: Validation loss decreased (118.450562 --> 118.250542).\n",
            "\t Train_Loss: 25.6746 Val_Loss: 118.2505  BEST VAL Loss: 118.2505\n",
            "\n",
            "Epoch 935: Validation loss decreased (118.250542 --> 118.040184).\n",
            "\t Train_Loss: 25.6104 Val_Loss: 118.0402  BEST VAL Loss: 118.0402\n",
            "\n",
            "Epoch 936: Validation loss decreased (118.040184 --> 117.833595).\n",
            "\t Train_Loss: 25.5474 Val_Loss: 117.8336  BEST VAL Loss: 117.8336\n",
            "\n",
            "Epoch 937: Validation loss decreased (117.833595 --> 117.644829).\n",
            "\t Train_Loss: 25.4841 Val_Loss: 117.6448  BEST VAL Loss: 117.6448\n",
            "\n",
            "Epoch 938: Validation loss decreased (117.644829 --> 117.444557).\n",
            "\t Train_Loss: 25.4213 Val_Loss: 117.4446  BEST VAL Loss: 117.4446\n",
            "\n",
            "Epoch 939: Validation loss decreased (117.444557 --> 117.228264).\n",
            "\t Train_Loss: 25.3588 Val_Loss: 117.2283  BEST VAL Loss: 117.2283\n",
            "\n",
            "Epoch 940: Validation loss decreased (117.228264 --> 117.036819).\n",
            "\t Train_Loss: 25.2967 Val_Loss: 117.0368  BEST VAL Loss: 117.0368\n",
            "\n",
            "Epoch 941: Validation loss decreased (117.036819 --> 116.852715).\n",
            "\t Train_Loss: 25.2346 Val_Loss: 116.8527  BEST VAL Loss: 116.8527\n",
            "\n",
            "Epoch 942: Validation loss decreased (116.852715 --> 116.641479).\n",
            "\t Train_Loss: 25.1733 Val_Loss: 116.6415  BEST VAL Loss: 116.6415\n",
            "\n",
            "Epoch 943: Validation loss decreased (116.641479 --> 116.439796).\n",
            "\t Train_Loss: 25.1115 Val_Loss: 116.4398  BEST VAL Loss: 116.4398\n",
            "\n",
            "Epoch 944: Validation loss decreased (116.439796 --> 116.260063).\n",
            "\t Train_Loss: 25.0506 Val_Loss: 116.2601  BEST VAL Loss: 116.2601\n",
            "\n",
            "Epoch 945: Validation loss decreased (116.260063 --> 116.063858).\n",
            "\t Train_Loss: 24.9897 Val_Loss: 116.0639  BEST VAL Loss: 116.0639\n",
            "\n",
            "Epoch 946: Validation loss decreased (116.063858 --> 115.860161).\n",
            "\t Train_Loss: 24.9288 Val_Loss: 115.8602  BEST VAL Loss: 115.8602\n",
            "\n",
            "Epoch 947: Validation loss decreased (115.860161 --> 115.673195).\n",
            "\t Train_Loss: 24.8684 Val_Loss: 115.6732  BEST VAL Loss: 115.6732\n",
            "\n",
            "Epoch 948: Validation loss decreased (115.673195 --> 115.487686).\n",
            "\t Train_Loss: 24.8079 Val_Loss: 115.4877  BEST VAL Loss: 115.4877\n",
            "\n",
            "Epoch 949: Validation loss decreased (115.487686 --> 115.300774).\n",
            "\t Train_Loss: 24.7474 Val_Loss: 115.3008  BEST VAL Loss: 115.3008\n",
            "\n",
            "Epoch 950: Validation loss decreased (115.300774 --> 115.121094).\n",
            "\t Train_Loss: 24.6869 Val_Loss: 115.1211  BEST VAL Loss: 115.1211\n",
            "\n",
            "Epoch 951: Validation loss decreased (115.121094 --> 114.943657).\n",
            "\t Train_Loss: 24.6258 Val_Loss: 114.9437  BEST VAL Loss: 114.9437\n",
            "\n",
            "Epoch 952: Validation loss decreased (114.943657 --> 114.789284).\n",
            "\t Train_Loss: 24.5637 Val_Loss: 114.7893  BEST VAL Loss: 114.7893\n",
            "\n",
            "Epoch 953: Validation loss decreased (114.789284 --> 114.708084).\n",
            "\t Train_Loss: 24.4975 Val_Loss: 114.7081  BEST VAL Loss: 114.7081\n",
            "\n",
            "Epoch 954: Validation loss did not decrease\n",
            "\t Train_Loss: 24.4089 Val_Loss: 114.9317  BEST VAL Loss: 114.7081\n",
            "\n",
            "Epoch 955: Validation loss decreased (114.708084 --> 114.272423).\n",
            "\t Train_Loss: 24.3010 Val_Loss: 114.2724  BEST VAL Loss: 114.2724\n",
            "\n",
            "Epoch 956: Validation loss did not decrease\n",
            "\t Train_Loss: 24.2551 Val_Loss: 187.5695  BEST VAL Loss: 114.2724\n",
            "\n",
            "Epoch 957: Validation loss did not decrease\n",
            "\t Train_Loss: 43.3640 Val_Loss: 118.0786  BEST VAL Loss: 114.2724\n",
            "\n",
            "Epoch 958: Validation loss decreased (114.272423 --> 111.284424).\n",
            "\t Train_Loss: 27.3379 Val_Loss: 111.2844  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 959: Validation loss did not decrease\n",
            "\t Train_Loss: 25.9433 Val_Loss: 112.3195  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 960: Validation loss did not decrease\n",
            "\t Train_Loss: 24.8592 Val_Loss: 114.1016  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 961: Validation loss did not decrease\n",
            "\t Train_Loss: 25.6664 Val_Loss: 113.7426  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 962: Validation loss did not decrease\n",
            "\t Train_Loss: 24.8495 Val_Loss: 113.3993  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 963: Validation loss did not decrease\n",
            "\t Train_Loss: 24.6791 Val_Loss: 113.3598  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 964: Validation loss did not decrease\n",
            "\t Train_Loss: 25.4801 Val_Loss: 113.2624  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 965: Validation loss did not decrease\n",
            "\t Train_Loss: 24.5117 Val_Loss: 112.9235  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 966: Validation loss did not decrease\n",
            "\t Train_Loss: 24.3935 Val_Loss: 112.3037  BEST VAL Loss: 111.2844\n",
            "\n",
            "Epoch 967: Validation loss decreased (111.284424 --> 110.867676).\n",
            "\t Train_Loss: 24.7239 Val_Loss: 110.8677  BEST VAL Loss: 110.8677\n",
            "\n",
            "Epoch 968: Validation loss decreased (110.867676 --> 110.309685).\n",
            "\t Train_Loss: 24.0310 Val_Loss: 110.3097  BEST VAL Loss: 110.3097\n",
            "\n",
            "Epoch 969: Validation loss did not decrease\n",
            "\t Train_Loss: 24.3621 Val_Loss: 110.7574  BEST VAL Loss: 110.3097\n",
            "\n",
            "Epoch 970: Validation loss did not decrease\n",
            "\t Train_Loss: 23.9257 Val_Loss: 111.8382  BEST VAL Loss: 110.3097\n",
            "\n",
            "Epoch 971: Validation loss did not decrease\n",
            "\t Train_Loss: 24.1389 Val_Loss: 111.4550  BEST VAL Loss: 110.3097\n",
            "\n",
            "Epoch 972: Validation loss did not decrease\n",
            "\t Train_Loss: 23.8908 Val_Loss: 110.9678  BEST VAL Loss: 110.3097\n",
            "\n",
            "Epoch 973: Validation loss did not decrease\n",
            "\t Train_Loss: 23.7195 Val_Loss: 111.4363  BEST VAL Loss: 110.3097\n",
            "\n",
            "Epoch 974: Validation loss did not decrease\n",
            "\t Train_Loss: 23.7553 Val_Loss: 111.3395  BEST VAL Loss: 110.3097\n",
            "\n",
            "Epoch 975: Validation loss did not decrease\n",
            "\t Train_Loss: 23.5571 Val_Loss: 110.7196  BEST VAL Loss: 110.3097\n",
            "\n",
            "Epoch 976: Validation loss decreased (110.309685 --> 109.886375).\n",
            "\t Train_Loss: 23.6490 Val_Loss: 109.8864  BEST VAL Loss: 109.8864\n",
            "\n",
            "Epoch 977: Validation loss decreased (109.886375 --> 109.645775).\n",
            "\t Train_Loss: 23.3682 Val_Loss: 109.6458  BEST VAL Loss: 109.6458\n",
            "\n",
            "Epoch 978: Validation loss did not decrease\n",
            "\t Train_Loss: 23.3157 Val_Loss: 109.9655  BEST VAL Loss: 109.6458\n",
            "\n",
            "Epoch 979: Validation loss did not decrease\n",
            "\t Train_Loss: 23.2330 Val_Loss: 109.9910  BEST VAL Loss: 109.6458\n",
            "\n",
            "Epoch 980: Validation loss decreased (109.645775 --> 109.285149).\n",
            "\t Train_Loss: 23.1847 Val_Loss: 109.2851  BEST VAL Loss: 109.2851\n",
            "\n",
            "Epoch 981: Validation loss decreased (109.285149 --> 108.692932).\n",
            "\t Train_Loss: 23.1226 Val_Loss: 108.6929  BEST VAL Loss: 108.6929\n",
            "\n",
            "Epoch 982: Validation loss did not decrease\n",
            "\t Train_Loss: 23.0025 Val_Loss: 109.1973  BEST VAL Loss: 108.6929\n",
            "\n",
            "Epoch 983: Validation loss did not decrease\n",
            "\t Train_Loss: 22.9022 Val_Loss: 109.4917  BEST VAL Loss: 108.6929\n",
            "\n",
            "Epoch 984: Validation loss did not decrease\n",
            "\t Train_Loss: 22.8800 Val_Loss: 108.8610  BEST VAL Loss: 108.6929\n",
            "\n",
            "Epoch 985: Validation loss decreased (108.692932 --> 108.383621).\n",
            "\t Train_Loss: 22.7230 Val_Loss: 108.3836  BEST VAL Loss: 108.3836\n",
            "\n",
            "Epoch 986: Validation loss decreased (108.383621 --> 108.376785).\n",
            "\t Train_Loss: 22.7449 Val_Loss: 108.3768  BEST VAL Loss: 108.3768\n",
            "\n",
            "Epoch 987: Validation loss did not decrease\n",
            "\t Train_Loss: 22.5770 Val_Loss: 108.7814  BEST VAL Loss: 108.3768\n",
            "\n",
            "Epoch 988: Validation loss did not decrease\n",
            "\t Train_Loss: 22.5589 Val_Loss: 108.4427  BEST VAL Loss: 108.3768\n",
            "\n",
            "Epoch 989: Validation loss decreased (108.376785 --> 107.555107).\n",
            "\t Train_Loss: 22.4685 Val_Loss: 107.5551  BEST VAL Loss: 107.5551\n",
            "\n",
            "Epoch 990: Validation loss decreased (107.555107 --> 107.346046).\n",
            "\t Train_Loss: 22.3827 Val_Loss: 107.3460  BEST VAL Loss: 107.3460\n",
            "\n",
            "Epoch 991: Validation loss did not decrease\n",
            "\t Train_Loss: 22.3538 Val_Loss: 107.7094  BEST VAL Loss: 107.3460\n",
            "\n",
            "Epoch 992: Validation loss did not decrease\n",
            "\t Train_Loss: 22.2475 Val_Loss: 107.7661  BEST VAL Loss: 107.3460\n",
            "\n",
            "Epoch 993: Validation loss did not decrease\n",
            "\t Train_Loss: 22.1906 Val_Loss: 107.3786  BEST VAL Loss: 107.3460\n",
            "\n",
            "Epoch 994: Validation loss decreased (107.346046 --> 107.124733).\n",
            "\t Train_Loss: 22.1063 Val_Loss: 107.1247  BEST VAL Loss: 107.1247\n",
            "\n",
            "Epoch 995: Validation loss did not decrease\n",
            "\t Train_Loss: 22.0314 Val_Loss: 107.1347  BEST VAL Loss: 107.1247\n",
            "\n",
            "Epoch 996: Validation loss decreased (107.124733 --> 107.044922).\n",
            "\t Train_Loss: 21.9980 Val_Loss: 107.0449  BEST VAL Loss: 107.0449\n",
            "\n",
            "Epoch 997: Validation loss decreased (107.044922 --> 106.788612).\n",
            "\t Train_Loss: 21.8955 Val_Loss: 106.7886  BEST VAL Loss: 106.7886\n",
            "\n",
            "Epoch 998: Validation loss decreased (106.788612 --> 106.419678).\n",
            "\t Train_Loss: 21.8584 Val_Loss: 106.4197  BEST VAL Loss: 106.4197\n",
            "\n",
            "Epoch 999: Validation loss decreased (106.419678 --> 106.208473).\n",
            "\t Train_Loss: 21.7620 Val_Loss: 106.2085  BEST VAL Loss: 106.2085\n",
            "\n",
            "Epoch 1000: Validation loss decreased (106.208473 --> 106.001381).\n",
            "\t Train_Loss: 21.7160 Val_Loss: 106.0014  BEST VAL Loss: 106.0014\n",
            "\n",
            "Epoch 1001: Validation loss decreased (106.001381 --> 105.860313).\n",
            "\t Train_Loss: 21.6472 Val_Loss: 105.8603  BEST VAL Loss: 105.8603\n",
            "\n",
            "Epoch 1002: Validation loss did not decrease\n",
            "\t Train_Loss: 21.5765 Val_Loss: 105.8853  BEST VAL Loss: 105.8603\n",
            "\n",
            "Epoch 1003: Validation loss decreased (105.860313 --> 105.785210).\n",
            "\t Train_Loss: 21.5179 Val_Loss: 105.7852  BEST VAL Loss: 105.7852\n",
            "\n",
            "Epoch 1004: Validation loss decreased (105.785210 --> 105.457573).\n",
            "\t Train_Loss: 21.4409 Val_Loss: 105.4576  BEST VAL Loss: 105.4576\n",
            "\n",
            "Epoch 1005: Validation loss decreased (105.457573 --> 105.179375).\n",
            "\t Train_Loss: 21.3846 Val_Loss: 105.1794  BEST VAL Loss: 105.1794\n",
            "\n",
            "Epoch 1006: Validation loss decreased (105.179375 --> 105.154350).\n",
            "\t Train_Loss: 21.3198 Val_Loss: 105.1544  BEST VAL Loss: 105.1544\n",
            "\n",
            "Epoch 1007: Validation loss decreased (105.154350 --> 105.111961).\n",
            "\t Train_Loss: 21.2524 Val_Loss: 105.1120  BEST VAL Loss: 105.1120\n",
            "\n",
            "Epoch 1008: Validation loss decreased (105.111961 --> 104.753777).\n",
            "\t Train_Loss: 21.1978 Val_Loss: 104.7538  BEST VAL Loss: 104.7538\n",
            "\n",
            "Epoch 1009: Validation loss decreased (104.753777 --> 104.385597).\n",
            "\t Train_Loss: 21.1216 Val_Loss: 104.3856  BEST VAL Loss: 104.3856\n",
            "\n",
            "Epoch 1010: Validation loss decreased (104.385597 --> 104.309105).\n",
            "\t Train_Loss: 21.0701 Val_Loss: 104.3091  BEST VAL Loss: 104.3091\n",
            "\n",
            "Epoch 1011: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9993 Val_Loss: 104.3465  BEST VAL Loss: 104.3091\n",
            "\n",
            "Epoch 1012: Validation loss decreased (104.309105 --> 104.119507).\n",
            "\t Train_Loss: 20.9430 Val_Loss: 104.1195  BEST VAL Loss: 104.1195\n",
            "\n",
            "Epoch 1013: Validation loss decreased (104.119507 --> 103.753838).\n",
            "\t Train_Loss: 20.8791 Val_Loss: 103.7538  BEST VAL Loss: 103.7538\n",
            "\n",
            "Epoch 1014: Validation loss decreased (103.753838 --> 103.563377).\n",
            "\t Train_Loss: 20.8154 Val_Loss: 103.5634  BEST VAL Loss: 103.5634\n",
            "\n",
            "Epoch 1015: Validation loss decreased (103.563377 --> 103.503738).\n",
            "\t Train_Loss: 20.7573 Val_Loss: 103.5037  BEST VAL Loss: 103.5037\n",
            "\n",
            "Epoch 1016: Validation loss decreased (103.503738 --> 103.373070).\n",
            "\t Train_Loss: 20.6953 Val_Loss: 103.3731  BEST VAL Loss: 103.3731\n",
            "\n",
            "Epoch 1017: Validation loss decreased (103.373070 --> 103.159355).\n",
            "\t Train_Loss: 20.6351 Val_Loss: 103.1594  BEST VAL Loss: 103.1594\n",
            "\n",
            "Epoch 1018: Validation loss decreased (103.159355 --> 102.981300).\n",
            "\t Train_Loss: 20.5752 Val_Loss: 102.9813  BEST VAL Loss: 102.9813\n",
            "\n",
            "Epoch 1019: Validation loss decreased (102.981300 --> 102.832474).\n",
            "\t Train_Loss: 20.5113 Val_Loss: 102.8325  BEST VAL Loss: 102.8325\n",
            "\n",
            "Epoch 1020: Validation loss decreased (102.832474 --> 102.650284).\n",
            "\t Train_Loss: 20.4556 Val_Loss: 102.6503  BEST VAL Loss: 102.6503\n",
            "\n",
            "Epoch 1021: Validation loss decreased (102.650284 --> 102.498878).\n",
            "\t Train_Loss: 20.3913 Val_Loss: 102.4989  BEST VAL Loss: 102.4989\n",
            "\n",
            "Epoch 1022: Validation loss decreased (102.498878 --> 102.372002).\n",
            "\t Train_Loss: 20.3333 Val_Loss: 102.3720  BEST VAL Loss: 102.3720\n",
            "\n",
            "Epoch 1023: Validation loss decreased (102.372002 --> 102.178940).\n",
            "\t Train_Loss: 20.2694 Val_Loss: 102.1789  BEST VAL Loss: 102.1789\n",
            "\n",
            "Epoch 1024: Validation loss decreased (102.178940 --> 101.959114).\n",
            "\t Train_Loss: 20.2078 Val_Loss: 101.9591  BEST VAL Loss: 101.9591\n",
            "\n",
            "Epoch 1025: Validation loss decreased (101.959114 --> 101.858482).\n",
            "\t Train_Loss: 20.1451 Val_Loss: 101.8585  BEST VAL Loss: 101.8585\n",
            "\n",
            "Epoch 1026: Validation loss decreased (101.858482 --> 101.835503).\n",
            "\t Train_Loss: 20.0803 Val_Loss: 101.8355  BEST VAL Loss: 101.8355\n",
            "\n",
            "Epoch 1027: Validation loss decreased (101.835503 --> 101.684364).\n",
            "\t Train_Loss: 20.0195 Val_Loss: 101.6844  BEST VAL Loss: 101.6844\n",
            "\n",
            "Epoch 1028: Validation loss decreased (101.684364 --> 101.391838).\n",
            "\t Train_Loss: 19.9611 Val_Loss: 101.3918  BEST VAL Loss: 101.3918\n",
            "\n",
            "Epoch 1029: Validation loss decreased (101.391838 --> 101.184593).\n",
            "\t Train_Loss: 19.9032 Val_Loss: 101.1846  BEST VAL Loss: 101.1846\n",
            "\n",
            "Epoch 1030: Validation loss decreased (101.184593 --> 101.055832).\n",
            "\t Train_Loss: 19.8433 Val_Loss: 101.0558  BEST VAL Loss: 101.0558\n",
            "\n",
            "Epoch 1031: Validation loss decreased (101.055832 --> 100.813324).\n",
            "\t Train_Loss: 19.7834 Val_Loss: 100.8133  BEST VAL Loss: 100.8133\n",
            "\n",
            "Epoch 1032: Validation loss decreased (100.813324 --> 100.496780).\n",
            "\t Train_Loss: 19.7240 Val_Loss: 100.4968  BEST VAL Loss: 100.4968\n",
            "\n",
            "Epoch 1033: Validation loss decreased (100.496780 --> 100.272354).\n",
            "\t Train_Loss: 19.6641 Val_Loss: 100.2724  BEST VAL Loss: 100.2724\n",
            "\n",
            "Epoch 1034: Validation loss decreased (100.272354 --> 100.126526).\n",
            "\t Train_Loss: 19.6060 Val_Loss: 100.1265  BEST VAL Loss: 100.1265\n",
            "\n",
            "Epoch 1035: Validation loss decreased (100.126526 --> 99.966286).\n",
            "\t Train_Loss: 19.5499 Val_Loss: 99.9663  BEST VAL Loss: 99.9663\n",
            "\n",
            "Epoch 1036: Validation loss decreased (99.966286 --> 99.809181).\n",
            "\t Train_Loss: 19.4908 Val_Loss: 99.8092  BEST VAL Loss: 99.8092\n",
            "\n",
            "Epoch 1037: Validation loss decreased (99.809181 --> 99.672256).\n",
            "\t Train_Loss: 19.4319 Val_Loss: 99.6723  BEST VAL Loss: 99.6723\n",
            "\n",
            "Epoch 1038: Validation loss decreased (99.672256 --> 99.488510).\n",
            "\t Train_Loss: 19.3719 Val_Loss: 99.4885  BEST VAL Loss: 99.4885\n",
            "\n",
            "Epoch 1039: Validation loss decreased (99.488510 --> 99.272453).\n",
            "\t Train_Loss: 19.3148 Val_Loss: 99.2725  BEST VAL Loss: 99.2725\n",
            "\n",
            "Epoch 1040: Validation loss decreased (99.272453 --> 99.135429).\n",
            "\t Train_Loss: 19.2580 Val_Loss: 99.1354  BEST VAL Loss: 99.1354\n",
            "\n",
            "Epoch 1041: Validation loss decreased (99.135429 --> 99.039368).\n",
            "\t Train_Loss: 19.2009 Val_Loss: 99.0394  BEST VAL Loss: 99.0394\n",
            "\n",
            "Epoch 1042: Validation loss decreased (99.039368 --> 98.863396).\n",
            "\t Train_Loss: 19.1438 Val_Loss: 98.8634  BEST VAL Loss: 98.8634\n",
            "\n",
            "Epoch 1043: Validation loss decreased (98.863396 --> 98.672295).\n",
            "\t Train_Loss: 19.0856 Val_Loss: 98.6723  BEST VAL Loss: 98.6723\n",
            "\n",
            "Epoch 1044: Validation loss decreased (98.672295 --> 98.547874).\n",
            "\t Train_Loss: 19.0291 Val_Loss: 98.5479  BEST VAL Loss: 98.5479\n",
            "\n",
            "Epoch 1045: Validation loss decreased (98.547874 --> 98.404633).\n",
            "\t Train_Loss: 18.9728 Val_Loss: 98.4046  BEST VAL Loss: 98.4046\n",
            "\n",
            "Epoch 1046: Validation loss decreased (98.404633 --> 98.193535).\n",
            "\t Train_Loss: 18.9166 Val_Loss: 98.1935  BEST VAL Loss: 98.1935\n",
            "\n",
            "Epoch 1047: Validation loss decreased (98.193535 --> 98.012077).\n",
            "\t Train_Loss: 18.8605 Val_Loss: 98.0121  BEST VAL Loss: 98.0121\n",
            "\n",
            "Epoch 1048: Validation loss decreased (98.012077 --> 97.883339).\n",
            "\t Train_Loss: 18.8043 Val_Loss: 97.8833  BEST VAL Loss: 97.8833\n",
            "\n",
            "Epoch 1049: Validation loss decreased (97.883339 --> 97.746849).\n",
            "\t Train_Loss: 18.7487 Val_Loss: 97.7468  BEST VAL Loss: 97.7468\n",
            "\n",
            "Epoch 1050: Validation loss decreased (97.746849 --> 97.632866).\n",
            "\t Train_Loss: 18.6931 Val_Loss: 97.6329  BEST VAL Loss: 97.6329\n",
            "\n",
            "Epoch 1051: Validation loss decreased (97.632866 --> 97.535675).\n",
            "\t Train_Loss: 18.6384 Val_Loss: 97.5357  BEST VAL Loss: 97.5357\n",
            "\n",
            "Epoch 1052: Validation loss decreased (97.535675 --> 97.352463).\n",
            "\t Train_Loss: 18.5840 Val_Loss: 97.3525  BEST VAL Loss: 97.3525\n",
            "\n",
            "Epoch 1053: Validation loss decreased (97.352463 --> 97.130188).\n",
            "\t Train_Loss: 18.5292 Val_Loss: 97.1302  BEST VAL Loss: 97.1302\n",
            "\n",
            "Epoch 1054: Validation loss decreased (97.130188 --> 96.972534).\n",
            "\t Train_Loss: 18.4749 Val_Loss: 96.9725  BEST VAL Loss: 96.9725\n",
            "\n",
            "Epoch 1055: Validation loss decreased (96.972534 --> 96.822227).\n",
            "\t Train_Loss: 18.4208 Val_Loss: 96.8222  BEST VAL Loss: 96.8222\n",
            "\n",
            "Epoch 1056: Validation loss decreased (96.822227 --> 96.651321).\n",
            "\t Train_Loss: 18.3671 Val_Loss: 96.6513  BEST VAL Loss: 96.6513\n",
            "\n",
            "Epoch 1057: Validation loss decreased (96.651321 --> 96.519218).\n",
            "\t Train_Loss: 18.3136 Val_Loss: 96.5192  BEST VAL Loss: 96.5192\n",
            "\n",
            "Epoch 1058: Validation loss decreased (96.519218 --> 96.376976).\n",
            "\t Train_Loss: 18.2602 Val_Loss: 96.3770  BEST VAL Loss: 96.3770\n",
            "\n",
            "Epoch 1059: Validation loss decreased (96.376976 --> 96.212074).\n",
            "\t Train_Loss: 18.2071 Val_Loss: 96.2121  BEST VAL Loss: 96.2121\n",
            "\n",
            "Epoch 1060: Validation loss decreased (96.212074 --> 96.090424).\n",
            "\t Train_Loss: 18.1541 Val_Loss: 96.0904  BEST VAL Loss: 96.0904\n",
            "\n",
            "Epoch 1061: Validation loss decreased (96.090424 --> 95.944206).\n",
            "\t Train_Loss: 18.1013 Val_Loss: 95.9442  BEST VAL Loss: 95.9442\n",
            "\n",
            "Epoch 1062: Validation loss decreased (95.944206 --> 95.761421).\n",
            "\t Train_Loss: 18.0486 Val_Loss: 95.7614  BEST VAL Loss: 95.7614\n",
            "\n",
            "Epoch 1063: Validation loss decreased (95.761421 --> 95.613564).\n",
            "\t Train_Loss: 17.9960 Val_Loss: 95.6136  BEST VAL Loss: 95.6136\n",
            "\n",
            "Epoch 1064: Validation loss decreased (95.613564 --> 95.441444).\n",
            "\t Train_Loss: 17.9436 Val_Loss: 95.4414  BEST VAL Loss: 95.4414\n",
            "\n",
            "Epoch 1065: Validation loss decreased (95.441444 --> 95.271423).\n",
            "\t Train_Loss: 17.8912 Val_Loss: 95.2714  BEST VAL Loss: 95.2714\n",
            "\n",
            "Epoch 1066: Validation loss decreased (95.271423 --> 95.127098).\n",
            "\t Train_Loss: 17.8391 Val_Loss: 95.1271  BEST VAL Loss: 95.1271\n",
            "\n",
            "Epoch 1067: Validation loss decreased (95.127098 --> 94.942810).\n",
            "\t Train_Loss: 17.7869 Val_Loss: 94.9428  BEST VAL Loss: 94.9428\n",
            "\n",
            "Epoch 1068: Validation loss decreased (94.942810 --> 94.790871).\n",
            "\t Train_Loss: 17.7347 Val_Loss: 94.7909  BEST VAL Loss: 94.7909\n",
            "\n",
            "Epoch 1069: Validation loss decreased (94.790871 --> 94.626564).\n",
            "\t Train_Loss: 17.6825 Val_Loss: 94.6266  BEST VAL Loss: 94.6266\n",
            "\n",
            "Epoch 1070: Validation loss decreased (94.626564 --> 94.445976).\n",
            "\t Train_Loss: 17.6304 Val_Loss: 94.4460  BEST VAL Loss: 94.4460\n",
            "\n",
            "Epoch 1071: Validation loss decreased (94.445976 --> 94.288460).\n",
            "\t Train_Loss: 17.5783 Val_Loss: 94.2885  BEST VAL Loss: 94.2885\n",
            "\n",
            "Epoch 1072: Validation loss decreased (94.288460 --> 94.092674).\n",
            "\t Train_Loss: 17.5263 Val_Loss: 94.0927  BEST VAL Loss: 94.0927\n",
            "\n",
            "Epoch 1073: Validation loss decreased (94.092674 --> 93.960876).\n",
            "\t Train_Loss: 17.4742 Val_Loss: 93.9609  BEST VAL Loss: 93.9609\n",
            "\n",
            "Epoch 1074: Validation loss decreased (93.960876 --> 93.762993).\n",
            "\t Train_Loss: 17.4220 Val_Loss: 93.7630  BEST VAL Loss: 93.7630\n",
            "\n",
            "Epoch 1075: Validation loss decreased (93.762993 --> 93.651703).\n",
            "\t Train_Loss: 17.3700 Val_Loss: 93.6517  BEST VAL Loss: 93.6517\n",
            "\n",
            "Epoch 1076: Validation loss decreased (93.651703 --> 93.389343).\n",
            "\t Train_Loss: 17.3181 Val_Loss: 93.3893  BEST VAL Loss: 93.3893\n",
            "\n",
            "Epoch 1077: Validation loss decreased (93.389343 --> 93.374649).\n",
            "\t Train_Loss: 17.2668 Val_Loss: 93.3746  BEST VAL Loss: 93.3746\n",
            "\n",
            "Epoch 1078: Validation loss decreased (93.374649 --> 92.910507).\n",
            "\t Train_Loss: 17.2173 Val_Loss: 92.9105  BEST VAL Loss: 92.9105\n",
            "\n",
            "Epoch 1079: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1778 Val_Loss: 93.3385  BEST VAL Loss: 92.9105\n",
            "\n",
            "Epoch 1080: Validation loss decreased (92.910507 --> 91.911491).\n",
            "\t Train_Loss: 17.1642 Val_Loss: 91.9115  BEST VAL Loss: 91.9115\n",
            "\n",
            "Epoch 1081: Validation loss did not decrease\n",
            "\t Train_Loss: 17.3725 Val_Loss: 93.7022  BEST VAL Loss: 91.9115\n",
            "\n",
            "Epoch 1082: Validation loss decreased (91.911491 --> 91.132553).\n",
            "\t Train_Loss: 17.3961 Val_Loss: 91.1326  BEST VAL Loss: 91.1326\n",
            "\n",
            "Epoch 1083: Validation loss did not decrease\n",
            "\t Train_Loss: 17.8122 Val_Loss: 92.5311  BEST VAL Loss: 91.1326\n",
            "\n",
            "Epoch 1084: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9992 Val_Loss: 94.2614  BEST VAL Loss: 91.1326\n",
            "\n",
            "Epoch 1085: Validation loss decreased (91.132553 --> 90.356285).\n",
            "\t Train_Loss: 17.7194 Val_Loss: 90.3563  BEST VAL Loss: 90.3563\n",
            "\n",
            "Epoch 1086: Validation loss did not decrease\n",
            "\t Train_Loss: 18.4640 Val_Loss: 90.8489  BEST VAL Loss: 90.3563\n",
            "\n",
            "Epoch 1087: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2474 Val_Loss: 95.5597  BEST VAL Loss: 90.3563\n",
            "\n",
            "Epoch 1088: Validation loss did not decrease\n",
            "\t Train_Loss: 18.8621 Val_Loss: 90.5969  BEST VAL Loss: 90.3563\n",
            "\n",
            "Epoch 1089: Validation loss decreased (90.356285 --> 89.869438).\n",
            "\t Train_Loss: 17.2168 Val_Loss: 89.8694  BEST VAL Loss: 89.8694\n",
            "\n",
            "Epoch 1090: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0585 Val_Loss: 91.8214  BEST VAL Loss: 89.8694\n",
            "\n",
            "Epoch 1091: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8947 Val_Loss: 94.5019  BEST VAL Loss: 89.8694\n",
            "\n",
            "Epoch 1092: Validation loss did not decrease\n",
            "\t Train_Loss: 18.1418 Val_Loss: 90.1737  BEST VAL Loss: 89.8694\n",
            "\n",
            "Epoch 1093: Validation loss decreased (89.869438 --> 89.637115).\n",
            "\t Train_Loss: 17.1687 Val_Loss: 89.6371  BEST VAL Loss: 89.6371\n",
            "\n",
            "Epoch 1094: Validation loss did not decrease\n",
            "\t Train_Loss: 17.7796 Val_Loss: 91.8202  BEST VAL Loss: 89.6371\n",
            "\n",
            "Epoch 1095: Validation loss did not decrease\n",
            "\t Train_Loss: 16.7563 Val_Loss: 92.9612  BEST VAL Loss: 89.6371\n",
            "\n",
            "Epoch 1096: Validation loss decreased (89.637115 --> 89.255066).\n",
            "\t Train_Loss: 17.5590 Val_Loss: 89.2551  BEST VAL Loss: 89.2551\n",
            "\n",
            "Epoch 1097: Validation loss decreased (89.255066 --> 88.685265).\n",
            "\t Train_Loss: 16.9347 Val_Loss: 88.6853  BEST VAL Loss: 88.6853\n",
            "\n",
            "Epoch 1098: Validation loss did not decrease\n",
            "\t Train_Loss: 17.3626 Val_Loss: 90.4356  BEST VAL Loss: 88.6853\n",
            "\n",
            "Epoch 1099: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5353 Val_Loss: 91.9849  BEST VAL Loss: 88.6853\n",
            "\n",
            "Epoch 1100: Validation loss did not decrease\n",
            "\t Train_Loss: 17.3597 Val_Loss: 88.7733  BEST VAL Loss: 88.6853\n",
            "\n",
            "Epoch 1101: Validation loss decreased (88.685265 --> 88.100876).\n",
            "\t Train_Loss: 16.6640 Val_Loss: 88.1009  BEST VAL Loss: 88.1009\n",
            "\n",
            "Epoch 1102: Validation loss did not decrease\n",
            "\t Train_Loss: 17.1451 Val_Loss: 89.4816  BEST VAL Loss: 88.1009\n",
            "\n",
            "Epoch 1103: Validation loss did not decrease\n",
            "\t Train_Loss: 16.2955 Val_Loss: 91.3710  BEST VAL Loss: 88.1009\n",
            "\n",
            "Epoch 1104: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2884 Val_Loss: 88.1603  BEST VAL Loss: 88.1009\n",
            "\n",
            "Epoch 1105: Validation loss decreased (88.100876 --> 87.645378).\n",
            "\t Train_Loss: 16.4512 Val_Loss: 87.6454  BEST VAL Loss: 87.6454\n",
            "\n",
            "Epoch 1106: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9243 Val_Loss: 89.1438  BEST VAL Loss: 87.6454\n",
            "\n",
            "Epoch 1107: Validation loss did not decrease\n",
            "\t Train_Loss: 16.1176 Val_Loss: 90.5245  BEST VAL Loss: 87.6454\n",
            "\n",
            "Epoch 1108: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8037 Val_Loss: 87.6774  BEST VAL Loss: 87.6454\n",
            "\n",
            "Epoch 1109: Validation loss decreased (87.645378 --> 87.277344).\n",
            "\t Train_Loss: 16.2777 Val_Loss: 87.2773  BEST VAL Loss: 87.2773\n",
            "\n",
            "Epoch 1110: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5365 Val_Loss: 88.9085  BEST VAL Loss: 87.2773\n",
            "\n",
            "Epoch 1111: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0012 Val_Loss: 89.6408  BEST VAL Loss: 87.2773\n",
            "\n",
            "Epoch 1112: Validation loss decreased (87.277344 --> 87.130409).\n",
            "\t Train_Loss: 16.3397 Val_Loss: 87.1304  BEST VAL Loss: 87.1304\n",
            "\n",
            "Epoch 1113: Validation loss decreased (87.130409 --> 86.811684).\n",
            "\t Train_Loss: 16.1140 Val_Loss: 86.8117  BEST VAL Loss: 86.8117\n",
            "\n",
            "Epoch 1114: Validation loss did not decrease\n",
            "\t Train_Loss: 16.1828 Val_Loss: 88.4279  BEST VAL Loss: 86.8117\n",
            "\n",
            "Epoch 1115: Validation loss did not decrease\n",
            "\t Train_Loss: 15.9188 Val_Loss: 88.1841  BEST VAL Loss: 86.8117\n",
            "\n",
            "Epoch 1116: Validation loss decreased (86.811684 --> 86.475746).\n",
            "\t Train_Loss: 15.8468 Val_Loss: 86.4757  BEST VAL Loss: 86.4757\n",
            "\n",
            "Epoch 1117: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0024 Val_Loss: 86.6376  BEST VAL Loss: 86.4757\n",
            "\n",
            "Epoch 1118: Validation loss did not decrease\n",
            "\t Train_Loss: 15.7673 Val_Loss: 88.2463  BEST VAL Loss: 86.4757\n",
            "\n",
            "Epoch 1119: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0598 Val_Loss: 86.4767  BEST VAL Loss: 86.4757\n",
            "\n",
            "Epoch 1120: Validation loss decreased (86.475746 --> 85.958885).\n",
            "\t Train_Loss: 15.6254 Val_Loss: 85.9589  BEST VAL Loss: 85.9589\n",
            "\n",
            "Epoch 1121: Validation loss did not decrease\n",
            "\t Train_Loss: 15.7346 Val_Loss: 86.9235  BEST VAL Loss: 85.9589\n",
            "\n",
            "Epoch 1122: Validation loss did not decrease\n",
            "\t Train_Loss: 15.6012 Val_Loss: 86.4359  BEST VAL Loss: 85.9589\n",
            "\n",
            "Epoch 1123: Validation loss decreased (85.958885 --> 85.503227).\n",
            "\t Train_Loss: 15.4879 Val_Loss: 85.5032  BEST VAL Loss: 85.5032\n",
            "\n",
            "Epoch 1124: Validation loss did not decrease\n",
            "\t Train_Loss: 15.5994 Val_Loss: 86.0582  BEST VAL Loss: 85.5032\n",
            "\n",
            "Epoch 1125: Validation loss did not decrease\n",
            "\t Train_Loss: 15.4034 Val_Loss: 86.3355  BEST VAL Loss: 85.5032\n",
            "\n",
            "Epoch 1126: Validation loss decreased (85.503227 --> 85.218704).\n",
            "\t Train_Loss: 15.4615 Val_Loss: 85.2187  BEST VAL Loss: 85.2187\n",
            "\n",
            "Epoch 1127: Validation loss did not decrease\n",
            "\t Train_Loss: 15.4489 Val_Loss: 85.4716  BEST VAL Loss: 85.2187\n",
            "\n",
            "Epoch 1128: Validation loss did not decrease\n",
            "\t Train_Loss: 15.2937 Val_Loss: 85.9590  BEST VAL Loss: 85.2187\n",
            "\n",
            "Epoch 1129: Validation loss decreased (85.218704 --> 84.737846).\n",
            "\t Train_Loss: 15.4012 Val_Loss: 84.7378  BEST VAL Loss: 84.7378\n",
            "\n",
            "Epoch 1130: Validation loss did not decrease\n",
            "\t Train_Loss: 15.3445 Val_Loss: 84.9788  BEST VAL Loss: 84.7378\n",
            "\n",
            "Epoch 1131: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1966 Val_Loss: 85.6665  BEST VAL Loss: 84.7378\n",
            "\n",
            "Epoch 1132: Validation loss decreased (84.737846 --> 84.360016).\n",
            "\t Train_Loss: 15.3370 Val_Loss: 84.3600  BEST VAL Loss: 84.3600\n",
            "\n",
            "Epoch 1133: Validation loss did not decrease\n",
            "\t Train_Loss: 15.2312 Val_Loss: 84.4974  BEST VAL Loss: 84.3600\n",
            "\n",
            "Epoch 1134: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1030 Val_Loss: 85.2703  BEST VAL Loss: 84.3600\n",
            "\n",
            "Epoch 1135: Validation loss decreased (84.360016 --> 83.963333).\n",
            "\t Train_Loss: 15.2538 Val_Loss: 83.9633  BEST VAL Loss: 83.9633\n",
            "\n",
            "Epoch 1136: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1237 Val_Loss: 84.1233  BEST VAL Loss: 83.9633\n",
            "\n",
            "Epoch 1137: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0022 Val_Loss: 84.8225  BEST VAL Loss: 83.9633\n",
            "\n",
            "Epoch 1138: Validation loss decreased (83.963333 --> 83.640984).\n",
            "\t Train_Loss: 15.1223 Val_Loss: 83.6410  BEST VAL Loss: 83.6410\n",
            "\n",
            "Epoch 1139: Validation loss did not decrease\n",
            "\t Train_Loss: 14.9981 Val_Loss: 83.7675  BEST VAL Loss: 83.6410\n",
            "\n",
            "Epoch 1140: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8976 Val_Loss: 84.2622  BEST VAL Loss: 83.6410\n",
            "\n",
            "Epoch 1141: Validation loss decreased (83.640984 --> 83.293083).\n",
            "\t Train_Loss: 14.9627 Val_Loss: 83.2931  BEST VAL Loss: 83.2931\n",
            "\n",
            "Epoch 1142: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8831 Val_Loss: 83.4580  BEST VAL Loss: 83.2931\n",
            "\n",
            "Epoch 1143: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7958 Val_Loss: 83.6567  BEST VAL Loss: 83.2931\n",
            "\n",
            "Epoch 1144: Validation loss decreased (83.293083 --> 82.900833).\n",
            "\t Train_Loss: 14.8077 Val_Loss: 82.9008  BEST VAL Loss: 82.9008\n",
            "\n",
            "Epoch 1145: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7727 Val_Loss: 83.1564  BEST VAL Loss: 82.9008\n",
            "\n",
            "Epoch 1146: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7041 Val_Loss: 83.0423  BEST VAL Loss: 82.9008\n",
            "\n",
            "Epoch 1147: Validation loss decreased (82.900833 --> 82.542580).\n",
            "\t Train_Loss: 14.6741 Val_Loss: 82.5426  BEST VAL Loss: 82.5426\n",
            "\n",
            "Epoch 1148: Validation loss did not decrease\n",
            "\t Train_Loss: 14.6674 Val_Loss: 82.9151  BEST VAL Loss: 82.5426\n",
            "\n",
            "Epoch 1149: Validation loss decreased (82.542580 --> 82.465782).\n",
            "\t Train_Loss: 14.6335 Val_Loss: 82.4658  BEST VAL Loss: 82.4658\n",
            "\n",
            "Epoch 1150: Validation loss decreased (82.465782 --> 82.229141).\n",
            "\t Train_Loss: 14.5733 Val_Loss: 82.2291  BEST VAL Loss: 82.2291\n",
            "\n",
            "Epoch 1151: Validation loss did not decrease\n",
            "\t Train_Loss: 14.5529 Val_Loss: 82.5313  BEST VAL Loss: 82.2291\n",
            "\n",
            "Epoch 1152: Validation loss decreased (82.229141 --> 81.933983).\n",
            "\t Train_Loss: 14.5460 Val_Loss: 81.9340  BEST VAL Loss: 81.9340\n",
            "\n",
            "Epoch 1153: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4940 Val_Loss: 81.9744  BEST VAL Loss: 81.9340\n",
            "\n",
            "Epoch 1154: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4465 Val_Loss: 81.9739  BEST VAL Loss: 81.9340\n",
            "\n",
            "Epoch 1155: Validation loss decreased (81.933983 --> 81.510635).\n",
            "\t Train_Loss: 14.4251 Val_Loss: 81.5106  BEST VAL Loss: 81.5106\n",
            "\n",
            "Epoch 1156: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3980 Val_Loss: 81.6624  BEST VAL Loss: 81.5106\n",
            "\n",
            "Epoch 1157: Validation loss decreased (81.510635 --> 81.399696).\n",
            "\t Train_Loss: 14.3562 Val_Loss: 81.3997  BEST VAL Loss: 81.3997\n",
            "\n",
            "Epoch 1158: Validation loss decreased (81.399696 --> 81.107948).\n",
            "\t Train_Loss: 14.3140 Val_Loss: 81.1079  BEST VAL Loss: 81.1079\n",
            "\n",
            "Epoch 1159: Validation loss did not decrease\n",
            "\t Train_Loss: 14.2865 Val_Loss: 81.1818  BEST VAL Loss: 81.1079\n",
            "\n",
            "Epoch 1160: Validation loss decreased (81.107948 --> 80.749260).\n",
            "\t Train_Loss: 14.2642 Val_Loss: 80.7493  BEST VAL Loss: 80.7493\n",
            "\n",
            "Epoch 1161: Validation loss did not decrease\n",
            "\t Train_Loss: 14.2328 Val_Loss: 80.9392  BEST VAL Loss: 80.7493\n",
            "\n",
            "Epoch 1162: Validation loss decreased (80.749260 --> 80.646034).\n",
            "\t Train_Loss: 14.1950 Val_Loss: 80.6460  BEST VAL Loss: 80.6460\n",
            "\n",
            "Epoch 1163: Validation loss decreased (80.646034 --> 80.582069).\n",
            "\t Train_Loss: 14.1515 Val_Loss: 80.5821  BEST VAL Loss: 80.5821\n",
            "\n",
            "Epoch 1164: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1164 Val_Loss: 80.5977  BEST VAL Loss: 80.5821\n",
            "\n",
            "Epoch 1165: Validation loss decreased (80.582069 --> 80.293007).\n",
            "\t Train_Loss: 14.0894 Val_Loss: 80.2930  BEST VAL Loss: 80.2930\n",
            "\n",
            "Epoch 1166: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0688 Val_Loss: 80.5257  BEST VAL Loss: 80.2930\n",
            "\n",
            "Epoch 1167: Validation loss decreased (80.293007 --> 80.014641).\n",
            "\t Train_Loss: 14.0493 Val_Loss: 80.0146  BEST VAL Loss: 80.0146\n",
            "\n",
            "Epoch 1168: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0247 Val_Loss: 80.3303  BEST VAL Loss: 80.0146\n",
            "\n",
            "Epoch 1169: Validation loss decreased (80.014641 --> 79.861664).\n",
            "\t Train_Loss: 13.9924 Val_Loss: 79.8617  BEST VAL Loss: 79.8617\n",
            "\n",
            "Epoch 1170: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9449 Val_Loss: 80.0350  BEST VAL Loss: 79.8617\n",
            "\n",
            "Epoch 1171: Validation loss decreased (79.861664 --> 79.859451).\n",
            "\t Train_Loss: 13.8934 Val_Loss: 79.8595  BEST VAL Loss: 79.8595\n",
            "\n",
            "Epoch 1172: Validation loss decreased (79.859451 --> 79.627640).\n",
            "\t Train_Loss: 13.8536 Val_Loss: 79.6276  BEST VAL Loss: 79.6276\n",
            "\n",
            "Epoch 1173: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8285 Val_Loss: 79.7249  BEST VAL Loss: 79.6276\n",
            "\n",
            "Epoch 1174: Validation loss decreased (79.627640 --> 79.320915).\n",
            "\t Train_Loss: 13.8142 Val_Loss: 79.3209  BEST VAL Loss: 79.3209\n",
            "\n",
            "Epoch 1175: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8155 Val_Loss: 79.7911  BEST VAL Loss: 79.3209\n",
            "\n",
            "Epoch 1176: Validation loss decreased (79.320915 --> 79.043045).\n",
            "\t Train_Loss: 13.8361 Val_Loss: 79.0430  BEST VAL Loss: 79.0430\n",
            "\n",
            "Epoch 1177: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8653 Val_Loss: 79.8389  BEST VAL Loss: 79.0430\n",
            "\n",
            "Epoch 1178: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8277 Val_Loss: 79.1294  BEST VAL Loss: 79.0430\n",
            "\n",
            "Epoch 1179: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6821 Val_Loss: 79.3217  BEST VAL Loss: 79.0430\n",
            "\n",
            "Epoch 1180: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5971 Val_Loss: 79.6368  BEST VAL Loss: 79.0430\n",
            "\n",
            "Epoch 1181: Validation loss decreased (79.043045 --> 78.927811).\n",
            "\t Train_Loss: 13.6325 Val_Loss: 78.9278  BEST VAL Loss: 78.9278\n",
            "\n",
            "Epoch 1182: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6206 Val_Loss: 79.2029  BEST VAL Loss: 78.9278\n",
            "\n",
            "Epoch 1183: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5252 Val_Loss: 78.9678  BEST VAL Loss: 78.9278\n",
            "\n",
            "Epoch 1184: Validation loss decreased (78.927811 --> 78.718002).\n",
            "\t Train_Loss: 13.4511 Val_Loss: 78.7180  BEST VAL Loss: 78.7180\n",
            "\n",
            "Epoch 1185: Validation loss did not decrease\n",
            "\t Train_Loss: 13.4558 Val_Loss: 78.8829  BEST VAL Loss: 78.7180\n",
            "\n",
            "Epoch 1186: Validation loss decreased (78.718002 --> 78.481178).\n",
            "\t Train_Loss: 13.4594 Val_Loss: 78.4812  BEST VAL Loss: 78.4812\n",
            "\n",
            "Epoch 1187: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3993 Val_Loss: 78.5138  BEST VAL Loss: 78.4812\n",
            "\n",
            "Epoch 1188: Validation loss decreased (78.481178 --> 78.433022).\n",
            "\t Train_Loss: 13.3010 Val_Loss: 78.4330  BEST VAL Loss: 78.4330\n",
            "\n",
            "Epoch 1189: Validation loss decreased (78.433022 --> 78.306435).\n",
            "\t Train_Loss: 13.2335 Val_Loss: 78.3064  BEST VAL Loss: 78.3064\n",
            "\n",
            "Epoch 1190: Validation loss decreased (78.306435 --> 78.141907).\n",
            "\t Train_Loss: 13.2099 Val_Loss: 78.1419  BEST VAL Loss: 78.1419\n",
            "\n",
            "Epoch 1191: Validation loss decreased (78.141907 --> 78.010521).\n",
            "\t Train_Loss: 13.2232 Val_Loss: 78.0105  BEST VAL Loss: 78.0105\n",
            "\n",
            "Epoch 1192: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5537 Val_Loss: 78.5624  BEST VAL Loss: 78.0105\n",
            "\n",
            "Epoch 1193: Validation loss decreased (78.010521 --> 77.283257).\n",
            "\t Train_Loss: 13.7597 Val_Loss: 77.2833  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1194: Validation loss did not decrease\n",
            "\t Train_Loss: 14.2372 Val_Loss: 77.7814  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1195: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0044 Val_Loss: 79.9112  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1196: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1957 Val_Loss: 77.6182  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1197: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3695 Val_Loss: 77.4188  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1198: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5899 Val_Loss: 80.3740  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1199: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1521 Val_Loss: 79.6910  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1200: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0649 Val_Loss: 78.6012  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1201: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6934 Val_Loss: 78.1191  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1202: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0036 Val_Loss: 80.2067  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1203: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3782 Val_Loss: 78.9722  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1204: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7655 Val_Loss: 77.6090  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1205: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1275 Val_Loss: 77.9901  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1206: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7581 Val_Loss: 79.3049  BEST VAL Loss: 77.2833\n",
            "\n",
            "Epoch 1207: Validation loss decreased (77.283257 --> 76.699959).\n",
            "\t Train_Loss: 12.9585 Val_Loss: 76.7000  BEST VAL Loss: 76.7000\n",
            "\n",
            "Epoch 1208: Validation loss decreased (76.699959 --> 75.168694).\n",
            "\t Train_Loss: 12.5175 Val_Loss: 75.1687  BEST VAL Loss: 75.1687\n",
            "\n",
            "Epoch 1209: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7511 Val_Loss: 75.9447  BEST VAL Loss: 75.1687\n",
            "\n",
            "Epoch 1210: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3244 Val_Loss: 77.5329  BEST VAL Loss: 75.1687\n",
            "\n",
            "Epoch 1211: Validation loss decreased (75.168694 --> 74.681152).\n",
            "\t Train_Loss: 12.6447 Val_Loss: 74.6812  BEST VAL Loss: 74.6812\n",
            "\n",
            "Epoch 1212: Validation loss decreased (74.681152 --> 73.374748).\n",
            "\t Train_Loss: 12.2056 Val_Loss: 73.3747  BEST VAL Loss: 73.3747\n",
            "\n",
            "Epoch 1213: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4567 Val_Loss: 74.3007  BEST VAL Loss: 73.3747\n",
            "\n",
            "Epoch 1214: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0919 Val_Loss: 75.7911  BEST VAL Loss: 73.3747\n",
            "\n",
            "Epoch 1215: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2676 Val_Loss: 73.9308  BEST VAL Loss: 73.3747\n",
            "\n",
            "Epoch 1216: Validation loss decreased (73.374748 --> 73.041733).\n",
            "\t Train_Loss: 11.9701 Val_Loss: 73.0417  BEST VAL Loss: 73.0417\n",
            "\n",
            "Epoch 1217: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0490 Val_Loss: 74.4425  BEST VAL Loss: 73.0417\n",
            "\n",
            "Epoch 1218: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8837 Val_Loss: 75.5634  BEST VAL Loss: 73.0417\n",
            "\n",
            "Epoch 1219: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8540 Val_Loss: 74.1676  BEST VAL Loss: 73.0417\n",
            "\n",
            "Epoch 1220: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7707 Val_Loss: 73.4388  BEST VAL Loss: 73.0417\n",
            "\n",
            "Epoch 1221: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7100 Val_Loss: 74.3936  BEST VAL Loss: 73.0417\n",
            "\n",
            "Epoch 1222: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6609 Val_Loss: 74.2474  BEST VAL Loss: 73.0417\n",
            "\n",
            "Epoch 1223: Validation loss decreased (73.041733 --> 72.905624).\n",
            "\t Train_Loss: 11.5407 Val_Loss: 72.9056  BEST VAL Loss: 72.9056\n",
            "\n",
            "Epoch 1224: Validation loss decreased (72.905624 --> 72.457985).\n",
            "\t Train_Loss: 11.5286 Val_Loss: 72.4580  BEST VAL Loss: 72.4580\n",
            "\n",
            "Epoch 1225: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4030 Val_Loss: 73.0404  BEST VAL Loss: 72.4580\n",
            "\n",
            "Epoch 1226: Validation loss decreased (72.457985 --> 71.971298).\n",
            "\t Train_Loss: 11.4398 Val_Loss: 71.9713  BEST VAL Loss: 71.9713\n",
            "\n",
            "Epoch 1227: Validation loss decreased (71.971298 --> 71.815269).\n",
            "\t Train_Loss: 11.2677 Val_Loss: 71.8153  BEST VAL Loss: 71.8153\n",
            "\n",
            "Epoch 1228: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2400 Val_Loss: 72.5485  BEST VAL Loss: 71.8153\n",
            "\n",
            "Epoch 1229: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1630 Val_Loss: 72.1254  BEST VAL Loss: 71.8153\n",
            "\n",
            "Epoch 1230: Validation loss decreased (71.815269 --> 71.050369).\n",
            "\t Train_Loss: 11.1027 Val_Loss: 71.0504  BEST VAL Loss: 71.0504\n",
            "\n",
            "Epoch 1231: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0457 Val_Loss: 71.1762  BEST VAL Loss: 71.0504\n",
            "\n",
            "Epoch 1232: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9594 Val_Loss: 71.7072  BEST VAL Loss: 71.0504\n",
            "\n",
            "Epoch 1233: Validation loss decreased (71.050369 --> 70.801056).\n",
            "\t Train_Loss: 10.9315 Val_Loss: 70.8011  BEST VAL Loss: 70.8011\n",
            "\n",
            "Epoch 1234: Validation loss decreased (70.801056 --> 70.158264).\n",
            "\t Train_Loss: 10.8308 Val_Loss: 70.1583  BEST VAL Loss: 70.1583\n",
            "\n",
            "Epoch 1235: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7967 Val_Loss: 70.7149  BEST VAL Loss: 70.1583\n",
            "\n",
            "Epoch 1236: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7007 Val_Loss: 70.9519  BEST VAL Loss: 70.1583\n",
            "\n",
            "Epoch 1237: Validation loss decreased (70.158264 --> 69.797569).\n",
            "\t Train_Loss: 10.6710 Val_Loss: 69.7976  BEST VAL Loss: 69.7976\n",
            "\n",
            "Epoch 1238: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5987 Val_Loss: 69.8171  BEST VAL Loss: 69.7976\n",
            "\n",
            "Epoch 1239: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5354 Val_Loss: 70.3152  BEST VAL Loss: 69.7976\n",
            "\n",
            "Epoch 1240: Validation loss decreased (69.797569 --> 69.151588).\n",
            "\t Train_Loss: 10.4811 Val_Loss: 69.1516  BEST VAL Loss: 69.1516\n",
            "\n",
            "Epoch 1241: Validation loss decreased (69.151588 --> 68.886414).\n",
            "\t Train_Loss: 10.4173 Val_Loss: 68.8864  BEST VAL Loss: 68.8864\n",
            "\n",
            "Epoch 1242: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3534 Val_Loss: 69.5450  BEST VAL Loss: 68.8864\n",
            "\n",
            "Epoch 1243: Validation loss decreased (68.886414 --> 68.549583).\n",
            "\t Train_Loss: 10.3127 Val_Loss: 68.5496  BEST VAL Loss: 68.5496\n",
            "\n",
            "Epoch 1244: Validation loss decreased (68.549583 --> 68.251366).\n",
            "\t Train_Loss: 10.2349 Val_Loss: 68.2514  BEST VAL Loss: 68.2514\n",
            "\n",
            "Epoch 1245: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1855 Val_Loss: 68.8178  BEST VAL Loss: 68.2514\n",
            "\n",
            "Epoch 1246: Validation loss decreased (68.251366 --> 67.728775).\n",
            "\t Train_Loss: 10.1310 Val_Loss: 67.7288  BEST VAL Loss: 67.7288\n",
            "\n",
            "Epoch 1247: Validation loss decreased (67.728775 --> 67.611755).\n",
            "\t Train_Loss: 10.0727 Val_Loss: 67.6118  BEST VAL Loss: 67.6118\n",
            "\n",
            "Epoch 1248: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0087 Val_Loss: 67.8014  BEST VAL Loss: 67.6118\n",
            "\n",
            "Epoch 1249: Validation loss decreased (67.611755 --> 66.679100).\n",
            "\t Train_Loss: 9.9649 Val_Loss: 66.6791  BEST VAL Loss: 66.6791\n",
            "\n",
            "Epoch 1250: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9165 Val_Loss: 67.2984  BEST VAL Loss: 66.6791\n",
            "\n",
            "Epoch 1251: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8506 Val_Loss: 66.8114  BEST VAL Loss: 66.6791\n",
            "\n",
            "Epoch 1252: Validation loss decreased (66.679100 --> 66.091072).\n",
            "\t Train_Loss: 9.7946 Val_Loss: 66.0911  BEST VAL Loss: 66.0911\n",
            "\n",
            "Epoch 1253: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7472 Val_Loss: 66.7814  BEST VAL Loss: 66.0911\n",
            "\n",
            "Epoch 1254: Validation loss decreased (66.091072 --> 65.410698).\n",
            "\t Train_Loss: 9.7003 Val_Loss: 65.4107  BEST VAL Loss: 65.4107\n",
            "\n",
            "Epoch 1255: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6456 Val_Loss: 65.8480  BEST VAL Loss: 65.4107\n",
            "\n",
            "Epoch 1256: Validation loss decreased (65.410698 --> 65.095482).\n",
            "\t Train_Loss: 9.5888 Val_Loss: 65.0955  BEST VAL Loss: 65.0955\n",
            "\n",
            "Epoch 1257: Validation loss decreased (65.095482 --> 65.063683).\n",
            "\t Train_Loss: 9.5301 Val_Loss: 65.0637  BEST VAL Loss: 65.0637\n",
            "\n",
            "Epoch 1258: Validation loss decreased (65.063683 --> 64.813339).\n",
            "\t Train_Loss: 9.4790 Val_Loss: 64.8133  BEST VAL Loss: 64.8133\n",
            "\n",
            "Epoch 1259: Validation loss decreased (64.813339 --> 64.111382).\n",
            "\t Train_Loss: 9.4268 Val_Loss: 64.1114  BEST VAL Loss: 64.1114\n",
            "\n",
            "Epoch 1260: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3798 Val_Loss: 64.5646  BEST VAL Loss: 64.1114\n",
            "\n",
            "Epoch 1261: Validation loss decreased (64.111382 --> 63.250801).\n",
            "\t Train_Loss: 9.3333 Val_Loss: 63.2508  BEST VAL Loss: 63.2508\n",
            "\n",
            "Epoch 1262: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2941 Val_Loss: 64.3378  BEST VAL Loss: 63.2508\n",
            "\n",
            "Epoch 1263: Validation loss decreased (63.250801 --> 61.967659).\n",
            "\t Train_Loss: 9.2620 Val_Loss: 61.9677  BEST VAL Loss: 61.9677\n",
            "\n",
            "Epoch 1264: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2620 Val_Loss: 64.6727  BEST VAL Loss: 61.9677\n",
            "\n",
            "Epoch 1265: Validation loss decreased (61.967659 --> 60.703121).\n",
            "\t Train_Loss: 9.2594 Val_Loss: 60.7031  BEST VAL Loss: 60.7031\n",
            "\n",
            "Epoch 1266: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3065 Val_Loss: 64.0942  BEST VAL Loss: 60.7031\n",
            "\n",
            "Epoch 1267: Validation loss decreased (60.703121 --> 60.690411).\n",
            "\t Train_Loss: 9.1862 Val_Loss: 60.6904  BEST VAL Loss: 60.6904\n",
            "\n",
            "Epoch 1268: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0990 Val_Loss: 62.8355  BEST VAL Loss: 60.6904\n",
            "\n",
            "Epoch 1269: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9921 Val_Loss: 60.9482  BEST VAL Loss: 60.6904\n",
            "\n",
            "Epoch 1270: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9134 Val_Loss: 61.5457  BEST VAL Loss: 60.6904\n",
            "\n",
            "Epoch 1271: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8541 Val_Loss: 61.0670  BEST VAL Loss: 60.6904\n",
            "\n",
            "Epoch 1272: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8074 Val_Loss: 60.7606  BEST VAL Loss: 60.6904\n",
            "\n",
            "Epoch 1273: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7616 Val_Loss: 60.6947  BEST VAL Loss: 60.6904\n",
            "\n",
            "Epoch 1274: Validation loss decreased (60.690411 --> 59.896801).\n",
            "\t Train_Loss: 8.7187 Val_Loss: 59.8968  BEST VAL Loss: 59.8968\n",
            "\n",
            "Epoch 1275: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6803 Val_Loss: 61.0676  BEST VAL Loss: 59.8968\n",
            "\n",
            "Epoch 1276: Validation loss decreased (59.896801 --> 58.374645).\n",
            "\t Train_Loss: 8.6696 Val_Loss: 58.3746  BEST VAL Loss: 58.3746\n",
            "\n",
            "Epoch 1277: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7260 Val_Loss: 62.6337  BEST VAL Loss: 58.3746\n",
            "\n",
            "Epoch 1278: Validation loss decreased (58.374645 --> 56.526600).\n",
            "\t Train_Loss: 8.9395 Val_Loss: 56.5266  BEST VAL Loss: 56.5266\n",
            "\n",
            "Epoch 1279: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3449 Val_Loss: 62.1089  BEST VAL Loss: 56.5266\n",
            "\n",
            "Epoch 1280: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8116 Val_Loss: 58.3060  BEST VAL Loss: 56.5266\n",
            "\n",
            "Epoch 1281: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4638 Val_Loss: 56.8561  BEST VAL Loss: 56.5266\n",
            "\n",
            "Epoch 1282: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5582 Val_Loss: 62.1682  BEST VAL Loss: 56.5266\n",
            "\n",
            "Epoch 1283: Validation loss decreased (56.526600 --> 55.602924).\n",
            "\t Train_Loss: 8.9123 Val_Loss: 55.6029  BEST VAL Loss: 55.6029\n",
            "\n",
            "Epoch 1284: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9546 Val_Loss: 59.0184  BEST VAL Loss: 55.6029\n",
            "\n",
            "Epoch 1285: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3571 Val_Loss: 59.7072  BEST VAL Loss: 55.6029\n",
            "\n",
            "Epoch 1286: Validation loss decreased (55.602924 --> 54.944801).\n",
            "\t Train_Loss: 8.4377 Val_Loss: 54.9448  BEST VAL Loss: 54.9448\n",
            "\n",
            "Epoch 1287: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7377 Val_Loss: 59.1839  BEST VAL Loss: 54.9448\n",
            "\n",
            "Epoch 1288: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3366 Val_Loss: 57.7515  BEST VAL Loss: 54.9448\n",
            "\n",
            "Epoch 1289: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1908 Val_Loss: 55.0312  BEST VAL Loss: 54.9448\n",
            "\n",
            "Epoch 1290: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4476 Val_Loss: 60.4072  BEST VAL Loss: 54.9448\n",
            "\n",
            "Epoch 1291: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4291 Val_Loss: 55.8091  BEST VAL Loss: 54.9448\n",
            "\n",
            "Epoch 1292: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1488 Val_Loss: 56.4922  BEST VAL Loss: 54.9448\n",
            "\n",
            "Epoch 1293: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9950 Val_Loss: 57.7879  BEST VAL Loss: 54.9448\n",
            "\n",
            "Epoch 1294: Validation loss decreased (54.944801 --> 54.449127).\n",
            "\t Train_Loss: 8.0750 Val_Loss: 54.4491  BEST VAL Loss: 54.4491\n",
            "\n",
            "Epoch 1295: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1817 Val_Loss: 59.0349  BEST VAL Loss: 54.4491\n",
            "\n",
            "Epoch 1296: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1680 Val_Loss: 55.0356  BEST VAL Loss: 54.4491\n",
            "\n",
            "Epoch 1297: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9696 Val_Loss: 56.2220  BEST VAL Loss: 54.4491\n",
            "\n",
            "Epoch 1298: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8273 Val_Loss: 57.5806  BEST VAL Loss: 54.4491\n",
            "\n",
            "Epoch 1299: Validation loss decreased (54.449127 --> 53.893200).\n",
            "\t Train_Loss: 7.8914 Val_Loss: 53.8932  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1300: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9260 Val_Loss: 57.0059  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1301: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8705 Val_Loss: 55.1131  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1302: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7368 Val_Loss: 55.5406  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1303: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6808 Val_Loss: 56.8241  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1304: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7214 Val_Loss: 54.0489  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1305: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6931 Val_Loss: 56.4695  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1306: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6287 Val_Loss: 54.7862  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1307: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5443 Val_Loss: 54.2852  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1308: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5353 Val_Loss: 56.5161  BEST VAL Loss: 53.8932\n",
            "\n",
            "Epoch 1309: Validation loss decreased (53.893200 --> 53.484657).\n",
            "\t Train_Loss: 7.5594 Val_Loss: 53.4847  BEST VAL Loss: 53.4847\n",
            "\n",
            "Epoch 1310: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5219 Val_Loss: 54.9696  BEST VAL Loss: 53.4847\n",
            "\n",
            "Epoch 1311: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4571 Val_Loss: 54.2177  BEST VAL Loss: 53.4847\n",
            "\n",
            "Epoch 1312: Validation loss decreased (53.484657 --> 53.450562).\n",
            "\t Train_Loss: 7.4028 Val_Loss: 53.4506  BEST VAL Loss: 53.4506\n",
            "\n",
            "Epoch 1313: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3933 Val_Loss: 54.8315  BEST VAL Loss: 53.4506\n",
            "\n",
            "Epoch 1314: Validation loss decreased (53.450562 --> 53.293976).\n",
            "\t Train_Loss: 7.3918 Val_Loss: 53.2940  BEST VAL Loss: 53.2940\n",
            "\n",
            "Epoch 1315: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3459 Val_Loss: 54.1368  BEST VAL Loss: 53.2940\n",
            "\n",
            "Epoch 1316: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2966 Val_Loss: 53.9437  BEST VAL Loss: 53.2940\n",
            "\n",
            "Epoch 1317: Validation loss decreased (53.293976 --> 52.756634).\n",
            "\t Train_Loss: 7.2770 Val_Loss: 52.7566  BEST VAL Loss: 52.7566\n",
            "\n",
            "Epoch 1318: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2670 Val_Loss: 54.4001  BEST VAL Loss: 52.7566\n",
            "\n",
            "Epoch 1319: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2393 Val_Loss: 53.2145  BEST VAL Loss: 52.7566\n",
            "\n",
            "Epoch 1320: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1941 Val_Loss: 53.3337  BEST VAL Loss: 52.7566\n",
            "\n",
            "Epoch 1321: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1659 Val_Loss: 54.0590  BEST VAL Loss: 52.7566\n",
            "\n",
            "Epoch 1322: Validation loss decreased (52.756634 --> 52.467175).\n",
            "\t Train_Loss: 7.1536 Val_Loss: 52.4672  BEST VAL Loss: 52.4672\n",
            "\n",
            "Epoch 1323: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1345 Val_Loss: 53.3967  BEST VAL Loss: 52.4672\n",
            "\n",
            "Epoch 1324: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1020 Val_Loss: 52.7702  BEST VAL Loss: 52.4672\n",
            "\n",
            "Epoch 1325: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0666 Val_Loss: 52.6036  BEST VAL Loss: 52.4672\n",
            "\n",
            "Epoch 1326: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0427 Val_Loss: 53.0942  BEST VAL Loss: 52.4672\n",
            "\n",
            "Epoch 1327: Validation loss decreased (52.467175 --> 51.998440).\n",
            "\t Train_Loss: 7.0281 Val_Loss: 51.9984  BEST VAL Loss: 51.9984\n",
            "\n",
            "Epoch 1328: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0056 Val_Loss: 52.6228  BEST VAL Loss: 51.9984\n",
            "\n",
            "Epoch 1329: Validation loss decreased (51.998440 --> 51.943996).\n",
            "\t Train_Loss: 6.9752 Val_Loss: 51.9440  BEST VAL Loss: 51.9440\n",
            "\n",
            "Epoch 1330: Validation loss decreased (51.943996 --> 51.738613).\n",
            "\t Train_Loss: 6.9472 Val_Loss: 51.7386  BEST VAL Loss: 51.7386\n",
            "\n",
            "Epoch 1331: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9255 Val_Loss: 52.3719  BEST VAL Loss: 51.7386\n",
            "\n",
            "Epoch 1332: Validation loss decreased (51.738613 --> 51.320850).\n",
            "\t Train_Loss: 6.9085 Val_Loss: 51.3209  BEST VAL Loss: 51.3209\n",
            "\n",
            "Epoch 1333: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8867 Val_Loss: 51.9277  BEST VAL Loss: 51.3209\n",
            "\n",
            "Epoch 1334: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8584 Val_Loss: 51.4495  BEST VAL Loss: 51.3209\n",
            "\n",
            "Epoch 1335: Validation loss decreased (51.320850 --> 51.195354).\n",
            "\t Train_Loss: 6.8328 Val_Loss: 51.1954  BEST VAL Loss: 51.1954\n",
            "\n",
            "Epoch 1336: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8102 Val_Loss: 51.5343  BEST VAL Loss: 51.1954\n",
            "\n",
            "Epoch 1337: Validation loss decreased (51.195354 --> 50.951454).\n",
            "\t Train_Loss: 6.7906 Val_Loss: 50.9515  BEST VAL Loss: 50.9515\n",
            "\n",
            "Epoch 1338: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7713 Val_Loss: 51.3212  BEST VAL Loss: 50.9515\n",
            "\n",
            "Epoch 1339: Validation loss decreased (50.951454 --> 50.711040).\n",
            "\t Train_Loss: 6.7473 Val_Loss: 50.7110  BEST VAL Loss: 50.7110\n",
            "\n",
            "Epoch 1340: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7224 Val_Loss: 50.8144  BEST VAL Loss: 50.7110\n",
            "\n",
            "Epoch 1341: Validation loss decreased (50.711040 --> 50.674110).\n",
            "\t Train_Loss: 6.6993 Val_Loss: 50.6741  BEST VAL Loss: 50.6741\n",
            "\n",
            "Epoch 1342: Validation loss decreased (50.674110 --> 50.296650).\n",
            "\t Train_Loss: 6.6780 Val_Loss: 50.2966  BEST VAL Loss: 50.2966\n",
            "\n",
            "Epoch 1343: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6581 Val_Loss: 50.6806  BEST VAL Loss: 50.2966\n",
            "\n",
            "Epoch 1344: Validation loss decreased (50.296650 --> 50.037151).\n",
            "\t Train_Loss: 6.6380 Val_Loss: 50.0372  BEST VAL Loss: 50.0372\n",
            "\n",
            "Epoch 1345: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6160 Val_Loss: 50.2738  BEST VAL Loss: 50.0372\n",
            "\n",
            "Epoch 1346: Validation loss decreased (50.037151 --> 50.011440).\n",
            "\t Train_Loss: 6.5932 Val_Loss: 50.0114  BEST VAL Loss: 50.0114\n",
            "\n",
            "Epoch 1347: Validation loss decreased (50.011440 --> 49.905197).\n",
            "\t Train_Loss: 6.5713 Val_Loss: 49.9052  BEST VAL Loss: 49.9052\n",
            "\n",
            "Epoch 1348: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5500 Val_Loss: 50.0069  BEST VAL Loss: 49.9052\n",
            "\n",
            "Epoch 1349: Validation loss decreased (49.905197 --> 49.720860).\n",
            "\t Train_Loss: 6.5295 Val_Loss: 49.7209  BEST VAL Loss: 49.7209\n",
            "\n",
            "Epoch 1350: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5098 Val_Loss: 49.8245  BEST VAL Loss: 49.7209\n",
            "\n",
            "Epoch 1351: Validation loss decreased (49.720860 --> 49.450146).\n",
            "\t Train_Loss: 6.4900 Val_Loss: 49.4501  BEST VAL Loss: 49.4501\n",
            "\n",
            "Epoch 1352: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4696 Val_Loss: 49.6131  BEST VAL Loss: 49.4501\n",
            "\n",
            "Epoch 1353: Validation loss decreased (49.450146 --> 49.237316).\n",
            "\t Train_Loss: 6.4487 Val_Loss: 49.2373  BEST VAL Loss: 49.2373\n",
            "\n",
            "Epoch 1354: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4279 Val_Loss: 49.3883  BEST VAL Loss: 49.2373\n",
            "\n",
            "Epoch 1355: Validation loss decreased (49.237316 --> 49.118690).\n",
            "\t Train_Loss: 6.4073 Val_Loss: 49.1187  BEST VAL Loss: 49.1187\n",
            "\n",
            "Epoch 1356: Validation loss decreased (49.118690 --> 49.051849).\n",
            "\t Train_Loss: 6.3869 Val_Loss: 49.0518  BEST VAL Loss: 49.0518\n",
            "\n",
            "Epoch 1357: Validation loss decreased (49.051849 --> 49.005672).\n",
            "\t Train_Loss: 6.3669 Val_Loss: 49.0057  BEST VAL Loss: 49.0057\n",
            "\n",
            "Epoch 1358: Validation loss decreased (49.005672 --> 48.779430).\n",
            "\t Train_Loss: 6.3473 Val_Loss: 48.7794  BEST VAL Loss: 48.7794\n",
            "\n",
            "Epoch 1359: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3280 Val_Loss: 48.8899  BEST VAL Loss: 48.7794\n",
            "\n",
            "Epoch 1360: Validation loss decreased (48.779430 --> 48.614178).\n",
            "\t Train_Loss: 6.3087 Val_Loss: 48.6142  BEST VAL Loss: 48.6142\n",
            "\n",
            "Epoch 1361: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2896 Val_Loss: 48.7145  BEST VAL Loss: 48.6142\n",
            "\n",
            "Epoch 1362: Validation loss decreased (48.614178 --> 48.424736).\n",
            "\t Train_Loss: 6.2709 Val_Loss: 48.4247  BEST VAL Loss: 48.4247\n",
            "\n",
            "Epoch 1363: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2526 Val_Loss: 48.5453  BEST VAL Loss: 48.4247\n",
            "\n",
            "Epoch 1364: Validation loss decreased (48.424736 --> 48.203888).\n",
            "\t Train_Loss: 6.2348 Val_Loss: 48.2039  BEST VAL Loss: 48.2039\n",
            "\n",
            "Epoch 1365: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2183 Val_Loss: 48.4625  BEST VAL Loss: 48.2039\n",
            "\n",
            "Epoch 1366: Validation loss decreased (48.203888 --> 47.908543).\n",
            "\t Train_Loss: 6.2034 Val_Loss: 47.9085  BEST VAL Loss: 47.9085\n",
            "\n",
            "Epoch 1367: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1937 Val_Loss: 48.4789  BEST VAL Loss: 47.9085\n",
            "\n",
            "Epoch 1368: Validation loss decreased (47.908543 --> 47.497280).\n",
            "\t Train_Loss: 6.1910 Val_Loss: 47.4973  BEST VAL Loss: 47.4973\n",
            "\n",
            "Epoch 1369: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2114 Val_Loss: 48.7763  BEST VAL Loss: 47.4973\n",
            "\n",
            "Epoch 1370: Validation loss decreased (47.497280 --> 46.913311).\n",
            "\t Train_Loss: 6.2495 Val_Loss: 46.9133  BEST VAL Loss: 46.9133\n",
            "\n",
            "Epoch 1371: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3678 Val_Loss: 49.8066  BEST VAL Loss: 46.9133\n",
            "\n",
            "Epoch 1372: Validation loss decreased (46.913311 --> 45.660225).\n",
            "\t Train_Loss: 6.4094 Val_Loss: 45.6602  BEST VAL Loss: 45.6602\n",
            "\n",
            "Epoch 1373: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5285 Val_Loss: 50.2205  BEST VAL Loss: 45.6602\n",
            "\n",
            "Epoch 1374: Validation loss decreased (45.660225 --> 45.577950).\n",
            "\t Train_Loss: 6.4702 Val_Loss: 45.5779  BEST VAL Loss: 45.5779\n",
            "\n",
            "Epoch 1375: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2869 Val_Loss: 46.8743  BEST VAL Loss: 45.5779\n",
            "\n",
            "Epoch 1376: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0660 Val_Loss: 49.5204  BEST VAL Loss: 45.5779\n",
            "\n",
            "Epoch 1377: Validation loss decreased (45.577950 --> 44.332989).\n",
            "\t Train_Loss: 6.2962 Val_Loss: 44.3330  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1378: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7806 Val_Loss: 50.5151  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1379: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7797 Val_Loss: 45.5760  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1380: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2975 Val_Loss: 45.1800  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1381: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3016 Val_Loss: 47.8457  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1382: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1790 Val_Loss: 48.5750  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1383: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5356 Val_Loss: 45.6022  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1384: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4401 Val_Loss: 47.9256  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1385: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1891 Val_Loss: 46.8834  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1386: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7545 Val_Loss: 44.7342  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1387: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3173 Val_Loss: 46.7401  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1388: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3182 Val_Loss: 49.0583  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1389: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2541 Val_Loss: 46.3144  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1390: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0338 Val_Loss: 47.2666  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1391: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2645 Val_Loss: 45.4179  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1392: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4743 Val_Loss: 50.9868  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1393: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9816 Val_Loss: 47.6840  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1394: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0611 Val_Loss: 45.2539  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1395: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0210 Val_Loss: 45.0253  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1396: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2351 Val_Loss: 48.5189  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1397: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9649 Val_Loss: 46.4375  BEST VAL Loss: 44.3330\n",
            "\n",
            "Epoch 1398: Validation loss decreased (44.332989 --> 41.768131).\n",
            "\t Train_Loss: 6.3236 Val_Loss: 41.7681  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1399: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8783 Val_Loss: 43.1994  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1400: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3250 Val_Loss: 46.5729  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1401: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3134 Val_Loss: 44.5759  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1402: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4015 Val_Loss: 42.7494  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1403: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9198 Val_Loss: 44.8617  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1404: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0574 Val_Loss: 42.4914  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1405: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9675 Val_Loss: 47.7334  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1406: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0563 Val_Loss: 47.2808  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1407: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8689 Val_Loss: 44.4607  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1408: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9948 Val_Loss: 44.4076  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1409: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1256 Val_Loss: 46.1479  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1410: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3951 Val_Loss: 45.2160  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1411: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8620 Val_Loss: 42.7912  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1412: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9739 Val_Loss: 41.9821  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1413: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0926 Val_Loss: 42.3021  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1414: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3263 Val_Loss: 42.8673  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1415: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9670 Val_Loss: 46.1533  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1416: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1999 Val_Loss: 44.5023  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1417: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8409 Val_Loss: 42.3298  BEST VAL Loss: 41.7681\n",
            "\n",
            "Epoch 1418: Validation loss decreased (41.768131 --> 40.343971).\n",
            "\t Train_Loss: 6.0904 Val_Loss: 40.3440  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1419: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9115 Val_Loss: 40.6678  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1420: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7422 Val_Loss: 42.2737  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1421: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9389 Val_Loss: 42.7913  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1422: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6566 Val_Loss: 43.8842  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1423: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7108 Val_Loss: 45.3003  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1424: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8158 Val_Loss: 43.9503  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1425: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5876 Val_Loss: 42.3704  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1426: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6632 Val_Loss: 41.8787  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1427: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6760 Val_Loss: 41.2784  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1428: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5248 Val_Loss: 41.1735  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1429: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5483 Val_Loss: 41.8328  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1430: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5760 Val_Loss: 41.9993  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1431: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5024 Val_Loss: 41.8911  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1432: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4632 Val_Loss: 41.9969  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1433: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4647 Val_Loss: 41.8176  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1434: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4568 Val_Loss: 41.5952  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1435: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4026 Val_Loss: 41.8022  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1436: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4225 Val_Loss: 41.7991  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1437: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5255 Val_Loss: 40.4035  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1438: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5487 Val_Loss: 41.3698  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1439: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4960 Val_Loss: 40.9196  BEST VAL Loss: 40.3440\n",
            "\n",
            "Epoch 1440: Validation loss decreased (40.343971 --> 40.121147).\n",
            "\t Train_Loss: 5.3733 Val_Loss: 40.1211  BEST VAL Loss: 40.1211\n",
            "\n",
            "Epoch 1441: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4751 Val_Loss: 40.8946  BEST VAL Loss: 40.1211\n",
            "\n",
            "Epoch 1442: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3553 Val_Loss: 40.7746  BEST VAL Loss: 40.1211\n",
            "\n",
            "Epoch 1443: Validation loss decreased (40.121147 --> 39.717506).\n",
            "\t Train_Loss: 5.3494 Val_Loss: 39.7175  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1444: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4099 Val_Loss: 40.4781  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1445: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2954 Val_Loss: 40.9200  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1446: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3269 Val_Loss: 40.3195  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1447: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3342 Val_Loss: 41.2218  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1448: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2358 Val_Loss: 41.8246  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1449: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2830 Val_Loss: 40.9534  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1450: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2754 Val_Loss: 41.1836  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1451: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2127 Val_Loss: 41.5078  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1452: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2598 Val_Loss: 40.4749  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1453: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2088 Val_Loss: 40.5824  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1454: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1693 Val_Loss: 40.9798  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1455: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2111 Val_Loss: 40.1986  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1456: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1747 Val_Loss: 40.4971  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1457: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1395 Val_Loss: 41.0755  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1458: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1703 Val_Loss: 40.3161  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1459: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1301 Val_Loss: 40.4680  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1460: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0989 Val_Loss: 40.6846  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1461: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1169 Val_Loss: 39.8856  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1462: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1026 Val_Loss: 40.1838  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1463: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0681 Val_Loss: 40.4686  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1464: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0738 Val_Loss: 39.9062  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1465: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0652 Val_Loss: 40.2043  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1466: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0350 Val_Loss: 40.1787  BEST VAL Loss: 39.7175\n",
            "\n",
            "Epoch 1467: Validation loss decreased (39.717506 --> 39.565552).\n",
            "\t Train_Loss: 5.0231 Val_Loss: 39.5656  BEST VAL Loss: 39.5656\n",
            "\n",
            "Epoch 1468: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0278 Val_Loss: 39.8090  BEST VAL Loss: 39.5656\n",
            "\n",
            "Epoch 1469: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0054 Val_Loss: 39.6201  BEST VAL Loss: 39.5656\n",
            "\n",
            "Epoch 1470: Validation loss decreased (39.565552 --> 39.188797).\n",
            "\t Train_Loss: 4.9837 Val_Loss: 39.1888  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1471: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9899 Val_Loss: 39.6072  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1472: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9814 Val_Loss: 39.4650  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1473: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9494 Val_Loss: 39.6072  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1474: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9416 Val_Loss: 39.8462  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1475: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9399 Val_Loss: 39.5154  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1476: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9218 Val_Loss: 39.6047  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1477: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9118 Val_Loss: 39.5433  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1478: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9004 Val_Loss: 39.3109  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1479: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8911 Val_Loss: 39.5258  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1480: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8847 Val_Loss: 39.3032  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1481: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8677 Val_Loss: 39.2117  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1482: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8572 Val_Loss: 39.2529  BEST VAL Loss: 39.1888\n",
            "\n",
            "Epoch 1483: Validation loss decreased (39.188797 --> 38.999447).\n",
            "\t Train_Loss: 4.8537 Val_Loss: 38.9994  BEST VAL Loss: 38.9994\n",
            "\n",
            "Epoch 1484: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8427 Val_Loss: 39.2408  BEST VAL Loss: 38.9994\n",
            "\n",
            "Epoch 1485: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8324 Val_Loss: 39.0623  BEST VAL Loss: 38.9994\n",
            "\n",
            "Epoch 1486: Validation loss decreased (38.999447 --> 38.995686).\n",
            "\t Train_Loss: 4.8193 Val_Loss: 38.9957  BEST VAL Loss: 38.9957\n",
            "\n",
            "Epoch 1487: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8078 Val_Loss: 39.1112  BEST VAL Loss: 38.9957\n",
            "\n",
            "Epoch 1488: Validation loss decreased (38.995686 --> 38.894379).\n",
            "\t Train_Loss: 4.8038 Val_Loss: 38.8944  BEST VAL Loss: 38.8944\n",
            "\n",
            "Epoch 1489: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7960 Val_Loss: 39.1757  BEST VAL Loss: 38.8944\n",
            "\n",
            "Epoch 1490: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7844 Val_Loss: 38.9563  BEST VAL Loss: 38.8944\n",
            "\n",
            "Epoch 1491: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7725 Val_Loss: 38.9434  BEST VAL Loss: 38.8944\n",
            "\n",
            "Epoch 1492: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7599 Val_Loss: 38.9223  BEST VAL Loss: 38.8944\n",
            "\n",
            "Epoch 1493: Validation loss decreased (38.894379 --> 38.667591).\n",
            "\t Train_Loss: 4.7531 Val_Loss: 38.6676  BEST VAL Loss: 38.6676\n",
            "\n",
            "Epoch 1494: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7476 Val_Loss: 38.9106  BEST VAL Loss: 38.6676\n",
            "\n",
            "Epoch 1495: Validation loss decreased (38.667591 --> 38.640526).\n",
            "\t Train_Loss: 4.7394 Val_Loss: 38.6405  BEST VAL Loss: 38.6405\n",
            "\n",
            "Epoch 1496: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7310 Val_Loss: 38.8592  BEST VAL Loss: 38.6405\n",
            "\n",
            "Epoch 1497: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7179 Val_Loss: 38.6433  BEST VAL Loss: 38.6405\n",
            "\n",
            "Epoch 1498: Validation loss decreased (38.640526 --> 38.625507).\n",
            "\t Train_Loss: 4.7054 Val_Loss: 38.6255  BEST VAL Loss: 38.6255\n",
            "\n",
            "Epoch 1499: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6964 Val_Loss: 38.6664  BEST VAL Loss: 38.6255\n",
            "\n",
            "Epoch 1500: Validation loss decreased (38.625507 --> 38.548603).\n",
            "\t Train_Loss: 4.6873 Val_Loss: 38.5486  BEST VAL Loss: 38.5486\n",
            "\n",
            "Epoch 1501: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6806 Val_Loss: 38.7283  BEST VAL Loss: 38.5486\n",
            "\n",
            "Epoch 1502: Validation loss decreased (38.548603 --> 38.461460).\n",
            "\t Train_Loss: 4.6736 Val_Loss: 38.4615  BEST VAL Loss: 38.4615\n",
            "\n",
            "Epoch 1503: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6658 Val_Loss: 38.6849  BEST VAL Loss: 38.4615\n",
            "\n",
            "Epoch 1504: Validation loss decreased (38.461460 --> 38.390869).\n",
            "\t Train_Loss: 4.6585 Val_Loss: 38.3909  BEST VAL Loss: 38.3909\n",
            "\n",
            "Epoch 1505: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6524 Val_Loss: 38.7011  BEST VAL Loss: 38.3909\n",
            "\n",
            "Epoch 1506: Validation loss decreased (38.390869 --> 38.258343).\n",
            "\t Train_Loss: 4.6466 Val_Loss: 38.2583  BEST VAL Loss: 38.2583\n",
            "\n",
            "Epoch 1507: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6440 Val_Loss: 38.6607  BEST VAL Loss: 38.2583\n",
            "\n",
            "Epoch 1508: Validation loss decreased (38.258343 --> 38.103703).\n",
            "\t Train_Loss: 4.6384 Val_Loss: 38.1037  BEST VAL Loss: 38.1037\n",
            "\n",
            "Epoch 1509: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6439 Val_Loss: 38.7577  BEST VAL Loss: 38.1037\n",
            "\n",
            "Epoch 1510: Validation loss decreased (38.103703 --> 38.021980).\n",
            "\t Train_Loss: 4.6495 Val_Loss: 38.0220  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1511: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6944 Val_Loss: 39.2477  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1512: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7060 Val_Loss: 38.0704  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1513: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7650 Val_Loss: 39.5068  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1514: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6984 Val_Loss: 38.3914  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1515: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6731 Val_Loss: 39.1378  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1516: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5894 Val_Loss: 38.4861  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1517: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5478 Val_Loss: 38.1516  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1518: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5468 Val_Loss: 38.5218  BEST VAL Loss: 38.0220\n",
            "\n",
            "Epoch 1519: Validation loss decreased (38.021980 --> 37.707081).\n",
            "\t Train_Loss: 4.5760 Val_Loss: 37.7071  BEST VAL Loss: 37.7071\n",
            "\n",
            "Epoch 1520: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6643 Val_Loss: 39.3120  BEST VAL Loss: 37.7071\n",
            "\n",
            "Epoch 1521: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7177 Val_Loss: 38.0415  BEST VAL Loss: 37.7071\n",
            "\n",
            "Epoch 1522: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8448 Val_Loss: 39.8616  BEST VAL Loss: 37.7071\n",
            "\n",
            "Epoch 1523: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6597 Val_Loss: 38.8491  BEST VAL Loss: 37.7071\n",
            "\n",
            "Epoch 1524: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5317 Val_Loss: 38.5746  BEST VAL Loss: 37.7071\n",
            "\n",
            "Epoch 1525: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5019 Val_Loss: 38.8268  BEST VAL Loss: 37.7071\n",
            "\n",
            "Epoch 1526: Validation loss decreased (37.707081 --> 37.312481).\n",
            "\t Train_Loss: 4.5822 Val_Loss: 37.3125  BEST VAL Loss: 37.3125\n",
            "\n",
            "Epoch 1527: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8129 Val_Loss: 40.1817  BEST VAL Loss: 37.3125\n",
            "\n",
            "Epoch 1528: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8662 Val_Loss: 38.5746  BEST VAL Loss: 37.3125\n",
            "\n",
            "Epoch 1529: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7744 Val_Loss: 39.8883  BEST VAL Loss: 37.3125\n",
            "\n",
            "Epoch 1530: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5095 Val_Loss: 39.6874  BEST VAL Loss: 37.3125\n",
            "\n",
            "Epoch 1531: Validation loss decreased (37.312481 --> 36.795303).\n",
            "\t Train_Loss: 4.7314 Val_Loss: 36.7953  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1532: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6494 Val_Loss: 41.6482  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1533: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1383 Val_Loss: 40.9110  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1534: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5897 Val_Loss: 39.7186  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1535: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1549 Val_Loss: 40.6558  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1536: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8411 Val_Loss: 38.3366  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1537: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6809 Val_Loss: 38.1626  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1538: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7206 Val_Loss: 38.9396  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1539: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6937 Val_Loss: 37.5737  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1540: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5602 Val_Loss: 38.2212  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1541: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7002 Val_Loss: 39.1951  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1542: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5176 Val_Loss: 39.6111  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1543: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5454 Val_Loss: 39.0651  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1544: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5561 Val_Loss: 38.6636  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1545: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4695 Val_Loss: 38.9953  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1546: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5222 Val_Loss: 37.9538  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1547: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5273 Val_Loss: 39.2322  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1548: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3897 Val_Loss: 39.1149  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1549: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4124 Val_Loss: 37.6349  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1550: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4428 Val_Loss: 37.9436  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1551: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3872 Val_Loss: 37.0903  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1552: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3683 Val_Loss: 37.3090  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1553: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3272 Val_Loss: 37.9089  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1554: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3335 Val_Loss: 37.6194  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1555: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3604 Val_Loss: 38.0001  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1556: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3455 Val_Loss: 36.9898  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1557: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3564 Val_Loss: 38.0501  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1558: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3405 Val_Loss: 37.5689  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1559: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3208 Val_Loss: 38.2542  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1560: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3153 Val_Loss: 37.5418  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1561: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2741 Val_Loss: 36.9295  BEST VAL Loss: 36.7953\n",
            "\n",
            "Epoch 1562: Validation loss decreased (36.795303 --> 36.720673).\n",
            "\t Train_Loss: 4.2672 Val_Loss: 36.7207  BEST VAL Loss: 36.7207\n",
            "\n",
            "Epoch 1563: Validation loss decreased (36.720673 --> 36.574337).\n",
            "\t Train_Loss: 4.2688 Val_Loss: 36.5743  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1564: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2626 Val_Loss: 37.3986  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1565: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2758 Val_Loss: 36.8581  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1566: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2625 Val_Loss: 37.3999  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1567: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2710 Val_Loss: 36.8872  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1568: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3032 Val_Loss: 38.1287  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1569: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3513 Val_Loss: 36.6198  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1570: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4169 Val_Loss: 38.6283  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1571: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3462 Val_Loss: 37.3942  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1572: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2671 Val_Loss: 37.8449  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1573: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2192 Val_Loss: 37.6452  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1574: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1911 Val_Loss: 37.0079  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1575: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2039 Val_Loss: 37.4304  BEST VAL Loss: 36.5743\n",
            "\n",
            "Epoch 1576: Validation loss decreased (36.574337 --> 36.460964).\n",
            "\t Train_Loss: 4.2373 Val_Loss: 36.4610  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1577: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2361 Val_Loss: 37.6297  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1578: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2292 Val_Loss: 37.0254  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1579: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2292 Val_Loss: 37.9103  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1580: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2110 Val_Loss: 37.0815  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1581: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1951 Val_Loss: 37.3076  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1582: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1702 Val_Loss: 36.5768  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1583: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1637 Val_Loss: 37.5187  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1584: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1525 Val_Loss: 37.2768  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1585: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1352 Val_Loss: 37.5168  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1586: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1366 Val_Loss: 36.9933  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1587: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1268 Val_Loss: 37.0245  BEST VAL Loss: 36.4610\n",
            "\n",
            "Epoch 1588: Validation loss decreased (36.460964 --> 36.255093).\n",
            "\t Train_Loss: 4.1219 Val_Loss: 36.2551  BEST VAL Loss: 36.2551\n",
            "\n",
            "Epoch 1589: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1492 Val_Loss: 37.6152  BEST VAL Loss: 36.2551\n",
            "\n",
            "Epoch 1590: Validation loss did not decrease\n",
            "\t Train_Loss: 4.1812 Val_Loss: 36.5489  BEST VAL Loss: 36.2551\n",
            "\n",
            "Epoch 1591: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3315 Val_Loss: 38.9800  BEST VAL Loss: 36.2551\n",
            "\n",
            "Epoch 1592: Validation loss decreased (36.255093 --> 35.395294).\n",
            "\t Train_Loss: 4.7626 Val_Loss: 35.3953  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1593: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6941 Val_Loss: 153.1756  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1594: Validation loss did not decrease\n",
            "\t Train_Loss: 78.0409 Val_Loss: 62.1164  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1595: Validation loss did not decrease\n",
            "\t Train_Loss: 30.6261 Val_Loss: 87.2163  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1596: Validation loss did not decrease\n",
            "\t Train_Loss: 34.7131 Val_Loss: 93.7738  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1597: Validation loss did not decrease\n",
            "\t Train_Loss: 38.3878 Val_Loss: 89.1755  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1598: Validation loss did not decrease\n",
            "\t Train_Loss: 36.6026 Val_Loss: 85.2125  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1599: Validation loss did not decrease\n",
            "\t Train_Loss: 33.4414 Val_Loss: 83.6669  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1600: Validation loss did not decrease\n",
            "\t Train_Loss: 31.1394 Val_Loss: 80.4781  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1601: Validation loss did not decrease\n",
            "\t Train_Loss: 26.7979 Val_Loss: 81.4806  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1602: Validation loss did not decrease\n",
            "\t Train_Loss: 26.4203 Val_Loss: 89.9646  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1603: Validation loss did not decrease\n",
            "\t Train_Loss: 28.4187 Val_Loss: 90.2478  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1604: Validation loss did not decrease\n",
            "\t Train_Loss: 28.0442 Val_Loss: 89.8366  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1605: Validation loss did not decrease\n",
            "\t Train_Loss: 28.6185 Val_Loss: 91.7295  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1606: Validation loss did not decrease\n",
            "\t Train_Loss: 29.1620 Val_Loss: 93.3175  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1607: Validation loss did not decrease\n",
            "\t Train_Loss: 29.0331 Val_Loss: 91.9695  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1608: Validation loss did not decrease\n",
            "\t Train_Loss: 27.9344 Val_Loss: 87.5569  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1609: Validation loss did not decrease\n",
            "\t Train_Loss: 26.1342 Val_Loss: 82.0055  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1610: Validation loss did not decrease\n",
            "\t Train_Loss: 24.0816 Val_Loss: 75.3021  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1611: Validation loss did not decrease\n",
            "\t Train_Loss: 21.8036 Val_Loss: 68.2114  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1612: Validation loss did not decrease\n",
            "\t Train_Loss: 19.3866 Val_Loss: 64.3624  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1613: Validation loss did not decrease\n",
            "\t Train_Loss: 19.1768 Val_Loss: 67.6779  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1614: Validation loss did not decrease\n",
            "\t Train_Loss: 20.3383 Val_Loss: 69.9590  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1615: Validation loss did not decrease\n",
            "\t Train_Loss: 20.8075 Val_Loss: 68.1423  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1616: Validation loss did not decrease\n",
            "\t Train_Loss: 20.0328 Val_Loss: 67.6466  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1617: Validation loss did not decrease\n",
            "\t Train_Loss: 19.3951 Val_Loss: 68.0822  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1618: Validation loss did not decrease\n",
            "\t Train_Loss: 18.8466 Val_Loss: 69.0534  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1619: Validation loss did not decrease\n",
            "\t Train_Loss: 18.4785 Val_Loss: 69.7297  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1620: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0902 Val_Loss: 69.6081  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1621: Validation loss did not decrease\n",
            "\t Train_Loss: 17.4771 Val_Loss: 68.4928  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1622: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0665 Val_Loss: 68.0196  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1623: Validation loss did not decrease\n",
            "\t Train_Loss: 18.5345 Val_Loss: 68.1742  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1624: Validation loss did not decrease\n",
            "\t Train_Loss: 17.5832 Val_Loss: 67.5307  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1625: Validation loss did not decrease\n",
            "\t Train_Loss: 16.1521 Val_Loss: 67.1415  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1626: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0932 Val_Loss: 67.0947  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1627: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1838 Val_Loss: 68.2392  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1628: Validation loss did not decrease\n",
            "\t Train_Loss: 14.5394 Val_Loss: 71.6002  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1629: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1998 Val_Loss: 70.3808  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1630: Validation loss did not decrease\n",
            "\t Train_Loss: 14.9143 Val_Loss: 67.9775  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1631: Validation loss did not decrease\n",
            "\t Train_Loss: 14.2644 Val_Loss: 66.2936  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1632: Validation loss did not decrease\n",
            "\t Train_Loss: 13.4440 Val_Loss: 64.8962  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1633: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9800 Val_Loss: 63.1540  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1634: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9465 Val_Loss: 61.4053  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1635: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9399 Val_Loss: 60.4020  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1636: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6188 Val_Loss: 60.2475  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1637: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3428 Val_Loss: 60.4599  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1638: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1907 Val_Loss: 60.5315  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1639: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0598 Val_Loss: 60.2567  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1640: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7870 Val_Loss: 59.9137  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1641: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4322 Val_Loss: 60.1517  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1642: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0986 Val_Loss: 61.3055  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1643: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0880 Val_Loss: 62.2411  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1644: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1056 Val_Loss: 62.8991  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1645: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0612 Val_Loss: 63.3368  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1646: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8474 Val_Loss: 63.3514  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1647: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8997 Val_Loss: 62.2137  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1648: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7268 Val_Loss: 61.0552  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1649: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8025 Val_Loss: 61.1407  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1650: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5667 Val_Loss: 61.6546  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1651: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8252 Val_Loss: 60.8935  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1652: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3599 Val_Loss: 60.5318  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1653: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4603 Val_Loss: 61.5711  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1654: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3258 Val_Loss: 63.7431  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1655: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5103 Val_Loss: 63.5212  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1656: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2538 Val_Loss: 61.7721  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1657: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1146 Val_Loss: 60.7921  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1658: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2706 Val_Loss: 61.2745  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1659: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0747 Val_Loss: 62.6181  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1660: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9899 Val_Loss: 62.9590  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1661: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8879 Val_Loss: 62.3188  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1662: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9575 Val_Loss: 60.8287  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1663: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8679 Val_Loss: 60.4982  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1664: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7395 Val_Loss: 60.7140  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1665: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8291 Val_Loss: 60.4968  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1666: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7513 Val_Loss: 60.1222  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1667: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6431 Val_Loss: 60.2213  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1668: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6381 Val_Loss: 60.8517  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1669: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6125 Val_Loss: 61.0076  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1670: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5457 Val_Loss: 60.9003  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1671: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5066 Val_Loss: 60.4197  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1672: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4148 Val_Loss: 60.0658  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1673: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3863 Val_Loss: 60.1760  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1674: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3744 Val_Loss: 60.6325  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1675: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2976 Val_Loss: 60.7666  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1676: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2419 Val_Loss: 60.4827  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1677: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2214 Val_Loss: 60.0561  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1678: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1825 Val_Loss: 59.6829  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1679: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1158 Val_Loss: 59.5086  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1680: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1016 Val_Loss: 59.3109  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1681: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0667 Val_Loss: 59.2306  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1682: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0113 Val_Loss: 59.3860  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1683: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9786 Val_Loss: 59.5641  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1684: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9642 Val_Loss: 59.4110  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1685: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9265 Val_Loss: 58.9960  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1686: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8790 Val_Loss: 58.7093  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1687: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8614 Val_Loss: 58.6773  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1688: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8202 Val_Loss: 58.7028  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1689: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7891 Val_Loss: 58.5494  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1690: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7595 Val_Loss: 58.3494  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1691: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7330 Val_Loss: 58.2940  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1692: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6946 Val_Loss: 58.3893  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1693: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6703 Val_Loss: 58.3572  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1694: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6443 Val_Loss: 58.1888  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1695: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6119 Val_Loss: 58.1066  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1696: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5812 Val_Loss: 58.0439  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1697: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5568 Val_Loss: 57.8509  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1698: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5271 Val_Loss: 57.6281  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1699: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4970 Val_Loss: 57.5793  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1700: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4703 Val_Loss: 57.6709  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1701: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4426 Val_Loss: 57.6332  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1702: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4123 Val_Loss: 57.5154  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1703: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3878 Val_Loss: 57.5229  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1704: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3587 Val_Loss: 57.5189  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1705: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3316 Val_Loss: 57.3905  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1706: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3031 Val_Loss: 57.3327  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1707: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2783 Val_Loss: 57.3857  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1708: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2507 Val_Loss: 57.2922  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1709: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2218 Val_Loss: 57.1488  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1710: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1965 Val_Loss: 57.1451  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1711: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1682 Val_Loss: 57.0962  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1712: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1416 Val_Loss: 56.9294  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1713: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1147 Val_Loss: 56.8475  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1714: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0877 Val_Loss: 56.7913  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1715: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0619 Val_Loss: 56.6340  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1716: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0358 Val_Loss: 56.5318  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1717: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0116 Val_Loss: 56.5426  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1718: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9865 Val_Loss: 56.4846  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1719: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9617 Val_Loss: 56.3635  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1720: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9377 Val_Loss: 56.3187  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1721: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9125 Val_Loss: 56.2434  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1722: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8883 Val_Loss: 56.1332  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1723: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8644 Val_Loss: 56.1262  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1724: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8406 Val_Loss: 56.1448  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1725: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8176 Val_Loss: 56.0868  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1726: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7945 Val_Loss: 56.0625  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1727: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7714 Val_Loss: 56.0520  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1728: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7487 Val_Loss: 55.9823  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1729: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7260 Val_Loss: 55.9449  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1730: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7036 Val_Loss: 55.9264  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1731: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6814 Val_Loss: 55.8337  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1732: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6594 Val_Loss: 55.7383  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1733: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6378 Val_Loss: 55.6760  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1734: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6163 Val_Loss: 55.5829  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1735: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5949 Val_Loss: 55.5089  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1736: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5738 Val_Loss: 55.4938  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1737: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5527 Val_Loss: 55.4550  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1738: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5318 Val_Loss: 55.3891  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1739: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5112 Val_Loss: 55.3472  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1740: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4906 Val_Loss: 55.2839  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1741: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4703 Val_Loss: 55.2071  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1742: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4502 Val_Loss: 55.1747  BEST VAL Loss: 35.3953\n",
            "\n",
            "Epoch 1743: Validation loss did not decrease\n",
            "Early stopped at epoch : 1743\n"
          ]
        }
      ],
      "source": [
        "LSTM_best_model, train_losses, val_losses = trainer(LSTM_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "rOoQ6lrfFYQo",
        "outputId": "524e9b96-0509-47b1-ea61-6a37caa624d5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVxb1IHaYwmU"
      },
      "source": [
        "---\n",
        "### 6.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGzInAaoDBWy"
      },
      "outputs": [],
      "source": [
        "val_predict_LSTM = LSTM_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "0plcD3u8DKqa",
        "outputId": "c005d444-b1ce-4ae9-aead-3000a03a40e9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_LSTM.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by LSTM RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WfqFMJLan26"
      },
      "source": [
        "---\n",
        "---\n",
        "## 7 Comaprison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "R7E2YreFari1",
        "outputId": "6b367c2b-6d79-4adc-d46c-4361e8136e30"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_RNN.detach().cpu(), label=\"predicted_RNN\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_GRU.detach().cpu(), label=\"predicted_GRU\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_LSTM.detach().cpu(), label=\"predicted_LSTM\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McCOcrqqhXkt"
      },
      "source": [
        "\n",
        "<b>1-Rank these architectures based on their performance?\n",
        "\n",
        "2-Why are they ranked in this order?\n",
        "\n",
        "3-Run the notebook again with look_back = 15.\n",
        "write about the difference in the comparison plot and the possible cause for that difference.</b>\n",
        "\n",
        "<font color='#73FF73'><b>Your answer:</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's how you can complete the LSTM model:\n",
        "\n",
        "```python\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bias=bias, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  \n",
        "        out = self.fc(out[:, -1, :]) \n",
        "        return out\n",
        "```\n",
        "\n",
        "In this code:\n",
        "- `self.lstm` is an LSTM layer that takes as input a tensor of shape (batch_size, sequence_length, input_size) and outputs a tensor of shape (batch_size, sequence_length, hidden_size). It also outputs the final hidden state and cell state for each element in the batch, but we don't use those in this case.\n",
        "- `self.fc` is a linear layer that maps the hidden state to the output size.\n",
        "- In the `forward` method, `h0` and `c0` are the initial hidden state and cell state for each element in the batch. They are tensors of zeros of shape (num_layers, batch_size, hidden_size). We then pass the input and the initial hidden state and cell state to the LSTM layer. The output of the LSTM layer is a tensor of shape (batch_size, sequence_length, hidden_size), but we only care about the last hidden state for each element in the batch, so we select `out[:, -1, :]`. We then pass this through the linear layer to get the final output.\n",
        "\n",
        "As for the comparison:\n",
        "\n",
        "1. **Rank these architectures based on their performance?**\n",
        "\n",
        "   The ranking of the architectures based on their performance would be as follows:\n",
        "   1. LSTM\n",
        "   2. GRU\n",
        "   3. Simple RNN\n",
        "\n",
        "2. **Why are they ranked in this order?**\n",
        "\n",
        "   The LSTM is ranked first because it has the ability to learn long-term dependencies using its memory cell, which can maintain information in memory for long periods of time. This makes it more effective for many sequence prediction problems.\n",
        "\n",
        "   The GRU, which is a simpler version of the LSTM, is ranked second. It combines the forget and input gates into a single \"update gate\" and also merges the cell state and hidden state. While it may not perform as well as the LSTM on certain tasks, it is generally more efficient due to its simplicity.\n",
        "\n",
        "   The Simple RNN is ranked last due to the vanishing gradient problem, which makes it difficult for the Simple RNN to learn and tune the parameters for long sequences.\n",
        "\n",
        "3. **Run the notebook again with look_back = 15. Write about the difference in the comparison plot and the possible cause for that difference.**\n",
        "\n",
        "   When the `look_back` parameter is reduced to 15, the performance of all models might decrease. This is because a smaller `look_back` means the models have less historical information to base their predictions on. However, the relative performance of the models (i.e., LSTM > GRU > Simple RNN) would likely remain the same. The LSTM and GRU would still outperform the Simple RNN due to their ability to better capture long-term dependencies in the data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "r7FFJaRgaqC1",
        "prWan6z7eKEp",
        "u78V0kKvjp8f",
        "nIfT0qVkOTzG",
        "uwmm5DhEBl8d",
        "5Ay_sL8ZBq1R",
        "utvDPP4kXEI-",
        "fkIg9lbrXiKZ",
        "-SlbDcQ-XuYx",
        "xIIm3f8yX9XF",
        "5Jcdsct5Y9a8",
        "mDMYeUTH5Ki7",
        "6WfqFMJLan26"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
