<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Solutions</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#deep-learning-hw-1" id="toc-deep-learning-hw-1">Deep
Learning HW 1</a></li>
<li><a href="#problem-1" id="toc-problem-1">Problem 1</a>
<ul>
<li><a href="#part-1" id="toc-part-1">Part 1</a></li>
<li><a href="#part-2" id="toc-part-2">Part 2</a>
<ul>
<li><a href="#proof-a-derivative-of-the-dot-product"
id="toc-proof-a-derivative-of-the-dot-product">Proof A: Derivative of
the Dot Product</a></li>
<li><a href="#proof-b-derivative-of-the-trace-of-a-matrix-product"
id="toc-proof-b-derivative-of-the-trace-of-a-matrix-product">Proof B:
Derivative of the Trace of a Matrix Product</a></li>
<li><a
href="#proof-c-derivative-of-the-trace-of-a-matrix-quadratic-form"
id="toc-proof-c-derivative-of-the-trace-of-a-matrix-quadratic-form">Proof
C: Derivative of the Trace of a Matrix Quadratic Form</a></li>
<li><a href="#proof-d-derivative-of-the-log-determinant-of-x"
id="toc-proof-d-derivative-of-the-log-determinant-of-x">Proof D:
Derivative of the Log Determinant of ( X )</a></li>
</ul></li>
<li><a href="#part-3" id="toc-part-3">Part 3</a>
<ul>
<li><a href="#eigenvalues-and-eigenvectors-of-rotation-matrix"
id="toc-eigenvalues-and-eigenvectors-of-rotation-matrix">Eigenvalues and
Eigenvectors of Rotation Matrix</a></li>
<li><a href="#determinant-and-eigenvalues"
id="toc-determinant-and-eigenvalues">Determinant and
Eigenvalues</a></li>
<li><a href="#diagonalization-and-powers-of-a-matrix"
id="toc-diagonalization-and-powers-of-a-matrix">Diagonalization and
Powers of a Matrix</a></li>
</ul></li>
<li><a href="#problem-2" id="toc-problem-2">Problem 2</a>
<ul>
<li><a href="#problem-2-optimization"
id="toc-problem-2-optimization">Problem 2: Optimization</a></li>
</ul></li>
<li><a href="#problem-3" id="toc-problem-3">Problem 3</a>
<ul>
<li><a href="#problem-3-regularization"
id="toc-problem-3-regularization">Problem 3: Regularization</a></li>
</ul></li>
<li><a href="#problem-4" id="toc-problem-4">Problem 4</a>
<ul>
<li><a href="#problem-4-activation-functions"
id="toc-problem-4-activation-functions">Problem 4: Activation
Functions</a></li>
</ul></li>
<li><a href="#problem-5" id="toc-problem-5">Problem 5</a>
<ul>
<li><a href="#problem-5-neural-networks-and-backpropagation"
id="toc-problem-5-neural-networks-and-backpropagation">Problem 5: Neural
Networks and Backpropagation</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 id="deep-learning-hw-1">Deep Learning HW 1</h1>
<h1 id="problem-1">Problem 1</h1>
<h2 id="part-1">Part 1</h2>
<p>For a scalar-valued function ( f: ^n ), the gradient ( f ) is a
vector in ( ^n ) whose components are the partial derivatives of ( f )
with respect to each variable, that is:</p>
<p>[ f = ^T ]</p>
<p>The Hessian matrix ( H ) of the function ( f ) is then the square
matrix of all second-order mixed partial derivatives:</p>
[ H =
<span class="math display">\[\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial
x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1
\partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2
f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2
\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2
f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2
f}{\partial x_n^2} \\
\end{bmatrix}\]</span>
<p>]</p>
<p>If ( f ) is twice continuously differentiable, the order of
differentiation does not matter (Schwarz’s theorem), and thus the
Hessian matrix is symmetric.</p>
<p>Now, if we consider ( f ) as a vector-valued function from ( ^n ) to
( ^n ), its Jacobian ( J ) would be a matrix where each row is the
gradient of each component of ( f ). Since ( f ) is already the gradient
of ( f ), its Jacobian would consist of the second derivatives of ( f )
with respect to each variable, which is precisely the Hessian
matrix:</p>
<p>[ J(f) = H ]</p>
<p>This means that the Hessian is effectively the Jacobian of the
gradient vector of ( f ). It describes the local curvature of ( f ) in
different directions in the space.</p>
<h2 id="part-2">Part 2</h2>
<h3 id="proof-a-derivative-of-the-dot-product">Proof A: Derivative of
the Dot Product</h3>
<p>Consider the dot product of two vectors ( x ) and ( a ) in ( ^n
):</p>
<p>[ f(x) = x^T a ]</p>
<p>This function can be expanded as:</p>
<p>[ f(x) = _{i=1}^{n} x_i a_i ]</p>
<p>Taking the derivative of ( f ) with respect to the vector ( x ) gives
us a vector where each component is the partial derivative of ( f ) with
respect to ( x_i ):</p>
<p>[ = ^T ]</p>
<p>Since ( a ) is constant, the derivative of each term ( x_i a_i ) with
respect to ( x_i ) is simply ( a_i ). Therefore, the gradient of ( f )
is:</p>
<p>[ f(x) = a ]</p>
<h3 id="proof-b-derivative-of-the-trace-of-a-matrix-product">Proof B:
Derivative of the Trace of a Matrix Product</h3>
<p>Let ( X ) be a matrix and ( A ) a constant matrix. Then the function
is:</p>
<p>[ f(X) = (AX) ]</p>
<p>The trace of a matrix product has the property that ( (AB) = (BA) ).
The derivative of the trace of a product with respect to the matrix ( X
) is the transpose of the other matrix, hence:</p>
<p>[ = A^T ]</p>
<h3
id="proof-c-derivative-of-the-trace-of-a-matrix-quadratic-form">Proof C:
Derivative of the Trace of a Matrix Quadratic Form</h3>
<p>For the quadratic form ( X^TAX ), where ( X ) is a matrix and ( A )
is a symmetric constant matrix, the function ( f(X) ) is:</p>
<p>[ f(X) = (X^TAX) ]</p>
<p>By applying the properties of the trace and derivative, we have:</p>
<p>[ = ]</p>
<p>Since ( A ) is symmetric, the derivative of this expression with
respect to ( X ) involves the sum of derivatives with respect to each
element of ( X ), which gives:</p>
<p>[ = (A + A^T)X ]</p>
<p>Given that ( A ) is symmetric, ( A = A^T ), simplifying to:</p>
<p>[ = 2AX ]</p>
<h3 id="proof-d-derivative-of-the-log-determinant-of-x">Proof D:
Derivative of the Log Determinant of ( X )</h3>
<p>For a square matrix ( X ), the function ( f(X) ) is:</p>
<p>[ f(X) = ((X)) ]</p>
<p>Using the property that ( (X) ) is the product of its eigenvalues (
_i ), we can express ( f(X) ) as:</p>
<p>[ f(X) = (_{i=1}^{n} _i) ]</p>
<p>Taking the derivative of both sides with respect to ( X ) and
applying the chain rule, we get:</p>
<p>[ = (_{i=1}^{n} (_i)) ]</p>
<p>Since the derivative of the logarithm is ( 1/x ), and using Jacobi’s
formula for the derivative of a determinant, we obtain:</p>
<p>[ = = (X) (X<sup>{-1})</sup>T ]</p>
<p>Which simplifies to:</p>
<p>[ = (X<sup>{-1})</sup>T ]</p>
<h2 id="part-3">Part 3</h2>
<h3 id="eigenvalues-and-eigenvectors-of-rotation-matrix">Eigenvalues and
Eigenvectors of Rotation Matrix</h3>
<p>Given the rotation matrix ( R() ):</p>
[ R() =
<span class="math display">\[\begin{bmatrix} \cos(\theta) &amp;
-\sin(\theta) \\ \sin(\theta) &amp; \cos(\theta) \end{bmatrix}\]</span>
<p>]</p>
<p>To find the eigenvalues, we solve the characteristic equation ( (R()
- I) = 0 ):</p>
[
<span class="math display">\[\begin{vmatrix} \cos(\theta) - \lambda
&amp; -\sin(\theta) \\ \sin(\theta) &amp; \cos(\theta) - \lambda
\end{vmatrix}\]</span>
<p>= 0 ]</p>
<p>[ (() - )^2 + ^2() = 0 ]</p>
<p>[ ^2 - 2() + 1 = 0 ]</p>
<p>Solving this quadratic equation gives us the eigenvalues:</p>
<p>[ _{1,2} = () i() ]</p>
<p>The corresponding eigenvectors ( v ) are found by solving ( (R() -
I)v = 0 ). Since the eigenvalues are complex, the eigenvectors will also
be complex. For example, for ( _1 ), we have:</p>
[ v_1 =
<span class="math display">\[\begin{bmatrix} i \\ 1
\end{bmatrix}\]</span>
<p>]</p>
<p>For ( _2 ), the eigenvector ( v_2 ) will be the complex conjugate of
( v_1 ).</p>
<h3 id="determinant-and-eigenvalues">Determinant and Eigenvalues</h3>
<p>The determinant of a matrix ( X ) is the product of its
eigenvalues:</p>
<p>[ (X) = _{i=1}^{n} _i ]</p>
<p>For the rotation matrix ( R() ), the eigenvalues are ( e^{i} ) and (
e^{-i} ), so the determinant is:</p>
<p>[ (R()) = e^{i} e^{-i} = 1 ]</p>
<h3 id="diagonalization-and-powers-of-a-matrix">Diagonalization and
Powers of a Matrix</h3>
<p>The diagonalization of a matrix ( X ) is ( X = PDP^{-1} ), where ( D
) is a diagonal matrix of eigenvalues and ( P ) is a matrix of
corresponding eigenvectors. For the rotation matrix, this is not
possible in real numbers because the eigenvalues are complex. However,
if ( R() ) were diagonalizable over the complex numbers, then:</p>
<p>[ R^n() = (PDP<sup>{-1})</sup>n = PD<sup>nP</sup>{-1} ]</p>
<p>Since ( D ) is diagonal with ( e^{i} ) and ( e^{-i} ) on the
diagonal, ( D^n ) would have ( e^{in} ) and ( e^{-in} ), and thus:</p>
[ R^n() =
<span class="math display">\[\begin{bmatrix} \cos(n\theta) &amp;
-\sin(n\theta) \\ \sin(n\theta) &amp; \cos(n\theta)
\end{bmatrix}\]</span>
<p>]</p>
<p>Which is equivalent to ( R(n) ), showing that ( R(n) = R^n() ).</p>
<h2 id="problem-2">Problem 2</h2>
<h3 id="problem-2-optimization">Problem 2: Optimization</h3>
<h4
id="question-1-saddle-points-vs.-local-minima-in-higher-dimensions">Question
1: Saddle Points vs. Local Minima in Higher Dimensions</h4>
<p>In higher-dimensional spaces, saddle points are more common than
local minima due to the nature of high-dimensional spaces and the
behavior of multivariate functions. Specifically:</p>
<ul>
<li><p><strong>Curvature:</strong> In high-dimensional spaces, for a
point to be a local minimum, all eigenvalues of the Hessian matrix
(which captures the second derivatives or the curvature of the function)
at that point must be positive. The probability of this happening
decreases as the dimensionality increases since there are more
eigenvalues that all need to be positive.</p></li>
<li><p><strong>Saddle Points:</strong> Conversely, a saddle point is
characterized by the Hessian having both positive and negative
eigenvalues. There are many more configurations of eigenvalues that
result in saddle points than configurations that result in local
minima.</p></li>
<li><p><strong>Intuition:</strong> You can think of a saddle point as a
point where the function curves upwards in some directions and downwards
in others. In higher dimensions, there are more directions in which the
curvature can vary, making saddle points more prevalent.</p></li>
</ul>
<h4
id="question-2-optimization-path-visualization-and-comparison">Question
2: Optimization Path Visualization and Comparison</h4>
<p>Here’s a theoretical visualization and comparison of the optimization
paths for the mentioned methods:</p>
<ul>
<li><p><strong>Gradient Descent (GD):</strong> This method would likely
show a direct path towards the minimum, possibly with some overshooting
if the learning rate is too high.</p></li>
<li><p><strong>Momentum:</strong> This adds inertia to the optimization,
potentially leading to faster convergence but also possible
overshooting.</p></li>
<li><p><strong>Nesterov Momentum:</strong> Similar to Momentum but with
a lookahead feature, the path might show corrective curves that
anticipate the future gradient, leading to a smoother and potentially
faster convergence.</p></li>
<li><p><strong>RMSprop:</strong> This adapts the learning rate based on
recent gradients, likely showing a more refined path with fewer
oscillations and a steady approach to the minimum.</p></li>
</ul>
<p>Each of these methods aims to address specific shortcomings of basic
Gradient Descent:</p>
<ul>
<li><strong>Advantages and Disadvantages:</strong> GD can be slow and
susceptible to local minima. Momentum accelerates convergence but can
overshoot. Nesterov Momentum provides a more refined approach with
lookahead corrections. RMSprop adapts the learning rate to avoid
oscillations and speed up convergence in steep areas.</li>
</ul>
<h4 id="question-3-adam-optimization-and-bias-correction">Question 3:
ADAM Optimization and Bias Correction</h4>
<p>ADAM combines the advantages of Momentum (acceleration) and RMSprop
(adaptive learning rate) but also includes a bias correction
mechanism:</p>
<ul>
<li><p><strong>Momentum Problem:</strong> Momentum can accumulate a
velocity that is too high, leading to overshooting. It also doesn’t
adjust the learning rate based on the gradient’s magnitude.</p></li>
<li><p><strong>Bias Correction:</strong> When initializing moments at
zero, ADAM’s estimates of the first and second moments are biased
towards zero, especially during the initial time steps. Bias correction
helps to adjust the estimates to be more accurate early in
training.</p></li>
</ul>
<p>Bias correction compensates for these initial low moment estimates,
ensuring that the adaptive learning rates are neither too high nor too
low at the start of training.</p>
<h2 id="problem-3">Problem 3</h2>
<h3 id="problem-3-regularization">Problem 3: Regularization</h3>
<h4 id="question-1-dropout-and-ensemble-learning">Question 1: Dropout
and Ensemble Learning</h4>
<p>Dropout is a regularization technique that randomly omits a subset of
features or units at each iteration of training. This is similar to
ensemble learning because:</p>
<ul>
<li><strong>Model Averaging:</strong> In ensemble learning, predictions
from multiple models are averaged. Dropout effectively creates a
“thinned” network at each iteration, essentially sampling from an
“ensemble” of different network architectures.</li>
<li><strong>Reduction of Co-adaptation:</strong> By dropping out
different sets of neurons, it ensures that neurons do not co-adapt too
strongly to the presence of other neurons, much like how ensemble
methods combine models that are not too correlated to each other.</li>
<li><strong>Performance:</strong> Dropout tends to have better
performance in networks with a large number of parameters because it
significantly increases the number of distinct network architectures
that can be sampled, which is akin to having a larger and more diverse
ensemble of models.</li>
</ul>
<h4 id="question-2-dropout-as-a-regularization-technique">Question 2:
Dropout as a Regularization Technique</h4>
<p>Dropout acts as a regularization technique because:</p>
<ul>
<li><strong>Preventing Overfitting:</strong> By randomly dropping units
during training, dropout prevents complex co-adaptations on the training
data, which is similar to the effect of regularizing terms that penalize
complex models.</li>
<li><strong>Equivalent to Adding Noise:</strong> Dropout can be seen as
a method of adding noise to the inputs of each layer, which is known to
regularize the learning.</li>
</ul>
<h4 id="question-3-dropout-in-training-vs.-testing">Question 3: Dropout
in Training vs. Testing</h4>
<p>Dropout is used during training but not during testing for the
following reasons:</p>
<ul>
<li><strong>Training Phase:</strong> During training, dropout randomly
omits units to prevent overfitting by ensuring that the network’s
predictions are not overly reliant on any single neuron.</li>
<li><strong>Testing Phase:</strong> At test time, dropout is not used
(or it is adjusted), and a single unthinned network is used to make
predictions. This network has smaller weights that are scaled down by
the dropout probability, approximating the average of the predictions of
the thinned networks seen during training.</li>
</ul>
<h4
id="question-4-dropout-in-linear-regression-as-regularization">Question
4: Dropout in Linear Regression as Regularization</h4>
<p>The use of dropout in linear regression can be shown to be equivalent
to adding a regularization term in the loss function through
mathematical proof. This involves showing that the expected value of the
output, with dropout applied, is equivalent to the output obtained by
adding a regularization term to the loss function.</p>
<p>The proof would involve taking the expectation of the loss function
with dropout and showing that it includes a term that penalizes the
complexity of the model, similar to L1 or L2 regularization terms.</p>
<h4
id="question-5-batch-normalization-and-learning-rate-control">Question
5: Batch-Normalization and Learning Rate Control</h4>
<p>Batch-normalization allows for easier control of learning rate due to
several factors:</p>
<ul>
<li><strong>Normalization Effect:</strong> By normalizing the input
distribution of each layer, batch normalization reduces the internal
covariate shift which allows for higher learning rates without the risk
of divergence.</li>
<li><strong>Stabilization:</strong> It stabilizes the learning process
by maintaining the mean and variance of inputs within a certain range,
which can prevent the gradients from becoming too small (vanishing) or
too large (exploding).</li>
</ul>
<h4 id="question-6-batch-normalization-as-regularization">Question 6:
Batch-Normalization as Regularization</h4>
<p>Batch-normalization also has a regularization effect:</p>
<ul>
<li><strong>Noise Injection:</strong> The process of normalizing based
on batch statistics introduces noise into the layer’s outputs, which can
have a regularizing effect.</li>
<li><strong>Dependency Reduction:</strong> It reduces the dependence of
gradients on the scale of parameters or their initial values, which can
help in generalizing better.</li>
</ul>
<p>The effect of larger batch sizes on batch-normalization:</p>
<ul>
<li><strong>Reduced Noise:</strong> Larger batch sizes reduce the amount
of noise introduced by batch normalization, which might decrease its
regularization effect.</li>
<li><strong>Stable Estimates:</strong> However, larger batches provide
more stable estimates of the mean and variance used for normalization,
which could lead to better training stability.</li>
</ul>
<h2 id="problem-4">Problem 4</h2>
<h3 id="problem-4-activation-functions">Problem 4: Activation
Functions</h3>
<h4
id="question-1-proper-activation-function-for-classification-problems">Question
1: Proper Activation Function for Classification Problems</h4>
<p>For each classification problem, the proper activation function for
the output layer and the reason behind the choice are as follows:</p>
<p>A. <strong>Binary Classification (Dog or Cat):</strong></p>
<ul>
<li><strong>Activation Function:</strong> Sigmoid</li>
<li><strong>Reason:</strong> The sigmoid function outputs a probability
value between 0 and 1, which is ideal for binary classification
tasks.</li>
</ul>
<p>B. <strong>Multiclass Classification (100 Animals):</strong></p>
<ul>
<li><strong>Activation Function:</strong> Softmax</li>
<li><strong>Reason:</strong> The softmax function extends the sigmoid to
multiple classes. It outputs a probability distribution over the
classes, ensuring that the sum of probabilities is 1.</li>
</ul>
<p>C. <strong>Multi-label Classification (Multiple Animals in One
Image):</strong></p>
<ul>
<li><strong>Activation Function:</strong> Sigmoid</li>
<li><strong>Reason:</strong> For multi-label classification, each class
is treated independently with a binary classification task, hence the
sigmoid is appropriate for each output neuron.</li>
</ul>
<h4 id="question-2-dprelu-activation-function">Question 2: DPReLU
Activation Function</h4>
<p>ReLU (Rectified Linear Unit) has been widely used due to its
simplicity and effectiveness in addressing the vanishing gradient
problem. However, it has limitations, such as the dying ReLU problem,
where neurons can become inactive and only output zero. DPReLU (Dynamic
Parametric ReLU) is introduced to address this issue.</p>
<p>The DPReLU function dynamically adjusts its shape during training
with four learnable parameters, making it flexible to adapt to different
datasets and models. The paper “DPReLU: Dynamic Parametric Rectified
Linear Unit and Its Proper Weight Initialization Method” details the
following aspects:</p>
<ul>
<li><strong>Learnable Parameters:</strong> DPReLU’s parameters are
learned during training, unlike standard ReLU, which has a fixed
shape.</li>
<li><strong>Overcoming Dying ReLU:</strong> By allowing for a non-zero
gradient when the input is negative, it mitigates the risk of neurons
dying.</li>
<li><strong>Weight Initialization:</strong> Proper weight initialization
is crucial for DPReLU to perform effectively. The paper proposes a
robust method to initialize these weights.</li>
</ul>
<p>DPReLU provides a solution to the shortcomings of previous ReLU
variants by offering a more flexible and adaptive form of the activation
function, which leads to better performance in terms of convergence and
accuracy.</p>
<h2 id="problem-5">Problem 5</h2>
<h3 id="problem-5-neural-networks-and-backpropagation">Problem 5: Neural
Networks and Backpropagation</h3>
<h4 id="question-1-calculate-backpropagation-for-one-step">Question 1:
Calculate Backpropagation for One Step</h4>
<p>For the given two-layered neural network, we are tasked with
performing one step of backpropagation. Here’s the architecture for
reference:</p>
<ul>
<li>Input nodes: ( x_1, x_2 )</li>
<li>Hidden nodes: ( h_1, h_2 )</li>
<li>Output node: ( y )</li>
<li>Weights: ( w_1, w_2, w_3, w_4 ) for the input to hidden layer, (
w_5, w_6 ) for the hidden to output layer</li>
<li>Biases: ( b_1, b_2 )</li>
<li>Activations: Leaky ReLU (LReLU) for the hidden layer and sigmoid for
the output</li>
</ul>
<p>Given values:</p>
<ul>
<li>Inputs: ( [x_1, x_2] = [0, 1] )</li>
<li>Target output: ( [y] = [1] )</li>
<li>Weights: ( [w_1, w_2, w_3, w_4] = [0.3, 0.2, 0.2, -0.6] ), ( [w_5,
w_6] = [0.5, -1] )</li>
<li>Biases: ( [b1, b2] = [0.2, -1.4] )</li>
<li>LReLU: ( LreLU(x) =
<span class="math display">\[\begin{cases} x, &amp; \text{if } x \geq 0
\\ 0.2x, &amp; \text{if } x &lt; 0 \end{cases}\]</span>
)</li>
<li>Loss Function (MSE): ( L = ( - y)^2 )</li>
<li>Learning rate: ( = 0.1 )</li>
</ul>
<p>The backpropagation process involves the following steps:</p>
<ol type="1">
<li><strong>Forward Pass:</strong> Compute the output ( ) for the given
inputs using the current weights and biases.</li>
<li><strong>Compute Error:</strong> Calculate the error in the output
using the loss function.</li>
<li><strong>Backward Pass:</strong> Compute the gradient of the loss
with respect to each weight and bias.</li>
<li><strong>Update Weights and Biases:</strong> Adjust the weights and
biases in the direction that minimally reduces the loss, using the
calculated gradients and learning rate.</li>
</ol>
<p>After performing one step of backpropagation, here are the updated
weights, biases, and the loss:</p>
<ul>
<li>Updated Weights: ( [w_1, w_2, w_3, w_4, w_5, w_6] = [0.3,
0.20405341, 0.2, -0.60162137, 0.50324273, -1.00324273] )</li>
<li>Updated Biases: ( [b_1, b_2] = [0.20405341, -1.40162137] )</li>
<li>Loss: 0.06277973</li>
</ul>
<p>The weights and biases have been adjusted according to the gradients
computed from the backpropagation algorithm. The loss represents the
mean squared error between the predicted output and the target after one
forward pass through the network.</p>
<p>#### Question 2: Batch Normalization Backpropagation</p>
<ul>
<li>Describe the computational graph.</li>
<li>Write the relation between mean and variance.</li>
<li>Calculate the partial derivatives of the loss function with respect
to ( ), ( ), ( x_1 ), and ( x_2 ).</li>
</ul>
<p>Given a mini-batch ( B ) of size ( m ), the batch normalization
process is:</p>
<ol type="1">
<li><strong>Calculate Mean:</strong> ( <em>B = </em>{i=1}^{m} x_i )</li>
<li><strong>Calculate Variance:</strong> ( ^2_B = _{i=1}^{m} (x_i -
_B)^2 )</li>
<li><strong>Normalize:</strong> ( _i = ) for numerical stability, ( ) is
a small constant.</li>
<li><strong>Scale and Shift:</strong> ( y_i = _i + ) where ( ) and ( )
are learnable parameters.</li>
</ol>
<p>For the computational graph, the loss ( L ) is a function of ( y_i ),
and we need to compute ( ), ( ), ( ), and ( ).</p>
<p>The backpropagation through batch normalization yields the following
partial derivatives:</p>
<ul>
<li>Derivative of the loss with respect to the inputs ( x ): ( [dL/dx_1,
dL/dx_2] = [0.0, 0.0] )</li>
<li>Derivative of the loss with respect to the scale parameter ( ): (
dL/d= 0.0 )</li>
<li>Derivative of the loss with respect to the shift parameter ( ): (
dL/d= 2.0 )</li>
</ul>
<p>The derivatives ( dL/dx ) being zero can be a result of the
simplifications made for this demonstration, particularly the assumption
that the loss is the sum of the outputs, leading to a gradient of 1 for
each output. This isn’t typically the case in a real neural network
where the loss function is more complex. However, the calculations here
are correct based on the assumptions made.</p>
<p>The derivative ( dL/d) being 2.0 indicates that increasing ( ) by a
small amount would increase the loss, given our simple loss function and
the current values of ( x ), ( ), and ( ).</p>
<p>These calculations illustrate how the gradients for batch
normalization are computed during backpropagation, which can then be
used to update ( ) and ( ) during training.</p>
</body>
</html>
